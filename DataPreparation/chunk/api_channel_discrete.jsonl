"# Discrete\n\nThis module provides layers and functions that implement channel\nmodels with discrete input/output alphabets.\n\nAll channel models support binary inputs $x \\in \\{0, 1\\}$ and <cite>bipolar</cite>\ninputs $x \\in \\{-1, 1\\}$, respectively. In the later case, it is assumed\nthat each <cite>0</cite> is mapped to <cite>-1</cite>.\n\nThe channels can either return discrete values or log-likelihood ratios (LLRs).\nThese LLRs describe the channel transition probabilities\n$L(y|X=1)=L(X=1|y)+L_a(X=1)$ where $L_a(X=1)=\\operatorname{log} \\frac{P(X=1)}{P(X=0)}$ depends only on the <cite>a priori</cite> probability of $X=1$. These LLRs equal the <cite>a posteriori</cite> probability if $P(X=1)=P(X=0)=0.5$.\n\nFurther, the channel reliability parameter $p_b$ can be either a scalar\nvalue or a tensor of any shape that can be broadcasted to the input. This\nallows for the efficient implementation of\nchannels with non-uniform error probabilities.\n\nThe channel models are based on the <cite>Gumble-softmax trick</cite> [[GumbleSoftmax]](https://nvlabs.github.io/sionna/api/channel.discrete.html#gumblesoftmax) to\nensure differentiability of the channel w.r.t. to the channel reliability\nparameter. Please see [[LearningShaping]](https://nvlabs.github.io/sionna/api/channel.discrete.html#learningshaping) for further details.\n\nSetting-up:\n```python\n>>> bsc = BinarySymmetricChannel(return_llrs=False, bipolar_input=False)\n```\n\n\nRunning:\n```python\n>>> x = tf.zeros((128,)) # x is the channel input\n>>> pb = 0.1 # pb is the bit flipping probability\n>>> y = bsc((x, pb))\n```"
"## BinaryMemorylessChannel\n\n`class` `sionna.channel.``BinaryMemorylessChannel`(*`return_llrs``=``False`*, *`bipolar_input``=``False`*, *`llr_max``=``100.`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/channel/discrete_channel.html#BinaryMemorylessChannel)\n\nDiscrete binary memory less channel with (possibly) asymmetric bit flipping\nprobabilities.\n\nInputs bits are flipped with probability $p_\\text{b,0}$ and\n$p_\\text{b,1}$, respectively.\n\nThis layer supports binary inputs ($x \\in \\{0, 1\\}$) and <cite>bipolar</cite>\ninputs ($x \\in \\{-1, 1\\}$).\n\nIf activated, the channel directly returns log-likelihood ratios (LLRs)\ndefined as\n\n$$\n\\begin{split}\\ell =\n\\begin{cases}\n    \\operatorname{log} \\frac{p_{b,1}}{1-p_{b,0}}, \\qquad \\text{if} \\, y=0 \\\\\n    \\operatorname{log} \\frac{1-p_{b,1}}{p_{b,0}}, \\qquad \\text{if} \\, y=1 \\\\\n\\end{cases}\\end{split}\n$$\n\nThe error probability $p_\\text{b}$ can be either scalar or a\ntensor (broadcastable to the shape of the input). This allows\ndifferent erasure probabilities per bit position. In any case, its last\ndimension must be of length 2 and is interpreted as $p_\\text{b,0}$ and\n$p_\\text{b,1}$.\n\nThis class inherits from the Keras <cite>Layer</cite> class and can be used as layer in\na Keras model.\nParameters\n\n- **return_llrs** (*bool*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the layer returns log-likelihood ratios\ninstead of binary values based on `pb`.\n- **bipolar_input** (*bool**, **False*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the expected input is given as\n$\\{-1,1\\}$ instead of $\\{0,1\\}$.\n- **llr_max** (*tf.float*)  Defaults to 100. Defines the clipping value of the LLRs.\n- **dtype** (*tf.DType*)  Defines the datatype for internal calculations and the output\ndtype. Defaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **(x, pb)**  Tuple:\n- **x** (*[,n], tf.float32*)  Input sequence to the channel consisting of binary values $\\{0,1\\}\n${-1,1}`, respectively.\n- **pb** (*[,2], tf.float32*)  Error probability. Can be a tuple of two scalars or of any\nshape that can be broadcasted to the shape of `x`. It has an\nadditional last dimension which is interpreted as $p_\\text{b,0}$\nand $p_\\text{b,1}$.\n\n\nOutput\n\n*[,n], tf.float32*  Output sequence of same length as the input `x`. If\n`return_llrs` is <cite>False</cite>, the output is ternary where a <cite>-1</cite> and\n<cite>0</cite> indicate an erasure for the binary and bipolar input,\nrespectively.\n\n\n`property` `llr_max`\n\nMaximum value used for LLR calculations.\n\n\n`property` `temperature`\n\nTemperature for Gumble-softmax trick."
"## BinarySymmetricChannel\n\n`class` `sionna.channel.``BinarySymmetricChannel`(*`return_llrs``=``False`*, *`bipolar_input``=``False`*, *`llr_max``=``100.`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/channel/discrete_channel.html#BinarySymmetricChannel)\n\nDiscrete binary symmetric channel which randomly flips bits with probability\n$p_\\text{b}$.\n\nThis layer supports binary inputs ($x \\in \\{0, 1\\}$) and <cite>bipolar</cite>\ninputs ($x \\in \\{-1, 1\\}$).\n\nIf activated, the channel directly returns log-likelihood ratios (LLRs)\ndefined as\n\n$$\n\\begin{split}\\ell =\n\\begin{cases}\n    \\operatorname{log} \\frac{p_{b}}{1-p_{b}}, \\qquad \\text{if}\\, y=0 \\\\\n    \\operatorname{log} \\frac{1-p_{b}}{p_{b}}, \\qquad \\text{if}\\, y=1 \\\\\n\\end{cases}\\end{split}\n$$\n\nwhere $y$ denotes the binary output of the channel.\n\nThe bit flipping probability $p_\\text{b}$ can be either a scalar or  a\ntensor (broadcastable to the shape of the input). This allows\ndifferent bit flipping probabilities per bit position.\n\nThis class inherits from the Keras <cite>Layer</cite> class and can be used as layer in\na Keras model.\nParameters\n\n- **return_llrs** (*bool*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the layer returns log-likelihood ratios\ninstead of binary values based on `pb`.\n- **bipolar_input** (*bool**, **False*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the expected input is given as {-1,1}\ninstead of {0,1}.\n- **llr_max** (*tf.float*)  Defaults to 100. Defines the clipping value of the LLRs.\n- **dtype** (*tf.DType*)  Defines the datatype for internal calculations and the output\ndtype. Defaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **(x, pb)**  Tuple:\n- **x** (*[,n], tf.float32*)  Input sequence to the channel.\n- **pb** (*tf.float32*)  Bit flipping probability. Can be a scalar or of any shape that\ncan be broadcasted to the shape of `x`.\n\n\nOutput\n\n*[,n], tf.float32*  Output sequence of same length as the input `x`. If\n`return_llrs` is <cite>False</cite>, the output is binary and otherwise\nsoft-values are returned."
"## BinaryErasureChannel\n\n`class` `sionna.channel.``BinaryErasureChannel`(*`return_llrs``=``False`*, *`bipolar_input``=``False`*, *`llr_max``=``100.`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/channel/discrete_channel.html#BinaryErasureChannel)\n\nBinary erasure channel (BEC) where a bit is either correctly received\nor erased.\n\nIn the binary erasure channel, bits are always correctly received or erased\nwith erasure probability $p_\\text{b}$.\n\nThis layer supports binary inputs ($x \\in \\{0, 1\\}$) and <cite>bipolar</cite>\ninputs ($x \\in \\{-1, 1\\}$).\n\nIf activated, the channel directly returns log-likelihood ratios (LLRs)\ndefined as\n\n$$\n\\begin{split}\\ell =\n\\begin{cases}\n    -\\infty, \\qquad \\text{if} \\, y=0 \\\\\n    0, \\qquad \\quad \\,\\, \\text{if} \\, y=? \\\\\n    \\infty, \\qquad \\quad \\text{if} \\, y=1 \\\\\n\\end{cases}\\end{split}\n$$\n\nThe erasure probability $p_\\text{b}$ can be either a scalar or a\ntensor (broadcastable to the shape of the input). This allows\ndifferent erasure probabilities per bit position.\n\nPlease note that the output of the BEC is ternary. Hereby, <cite>-1</cite> indicates an\nerasure for the binary configuration and <cite>0</cite> for the bipolar mode,\nrespectively.\n\nThis class inherits from the Keras <cite>Layer</cite> class and can be used as layer in\na Keras model.\nParameters\n\n- **return_llrs** (*bool*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the layer returns log-likelihood ratios\ninstead of binary values based on `pb`.\n- **bipolar_input** (*bool**, **False*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the expected input is given as {-1,1}\ninstead of {0,1}.\n- **llr_max** (*tf.float*)  Defaults to 100. Defines the clipping value of the LLRs.\n- **dtype** (*tf.DType*)  Defines the datatype for internal calculations and the output\ndtype. Defaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **(x, pb)**  Tuple:\n- **x** (*[,n], tf.float32*)  Input sequence to the channel.\n- **pb** (*tf.float32*)  Erasure probability. Can be a scalar or of any shape that can be\nbroadcasted to the shape of `x`.\n\n\nOutput\n\n*[,n], tf.float32*  Output sequence of same length as the input `x`. If\n`return_llrs` is <cite>False</cite>, the output is ternary where each <cite>-1</cite>\nand each <cite>0</cite> indicate an erasure for the binary and bipolar input,\nrespectively."
"## BinaryZChannel\n\n`class` `sionna.channel.``BinaryZChannel`(*`return_llrs``=``False`*, *`bipolar_input``=``False`*, *`llr_max``=``100.`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/channel/discrete_channel.html#BinaryZChannel)\n\nLayer that implements the binary Z-channel.\n\nIn the Z-channel, transmission errors only occur for the transmission of\nsecond input element (i.e., if a <cite>1</cite> is transmitted) with error probability\nprobability $p_\\text{b}$ but the first element is always correctly\nreceived.\n\nThis layer supports binary inputs ($x \\in \\{0, 1\\}$) and <cite>bipolar</cite>\ninputs ($x \\in \\{-1, 1\\}$).\n\nIf activated, the channel directly returns log-likelihood ratios (LLRs)\ndefined as\n\n$$\n\\begin{split}\\ell =\n\\begin{cases}\n    \\operatorname{log} \\left( p_b \\right), \\qquad \\text{if} \\, y=0 \\\\\n    \\infty, \\qquad \\qquad \\text{if} \\, y=1 \\\\\n\\end{cases}\\end{split}\n$$\n\nassuming equal probable inputs $P(X=0) = P(X=1) = 0.5$.\n\nThe error probability $p_\\text{b}$ can be either a scalar or a\ntensor (broadcastable to the shape of the input). This allows\ndifferent error probabilities per bit position.\n\nThis class inherits from the Keras <cite>Layer</cite> class and can be used as layer in\na Keras model.\nParameters\n\n- **return_llrs** (*bool*)  Defaults to <cite>False</cite>. If <cite>True</cite>, the layer returns log-likelihood ratios\ninstead of binary values based on `pb`.\n- **bipolar_input** (*bool**, **False*)  Defaults to <cite>False</cite>. If True, the expected input is given as {-1,1}\ninstead of {0,1}.\n- **llr_max** (*tf.float*)  Defaults to 100. Defines the clipping value of the LLRs.\n- **dtype** (*tf.DType*)  Defines the datatype for internal calculations and the output\ndtype. Defaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **(x, pb)**  Tuple:\n- **x** (*[,n], tf.float32*)  Input sequence to the channel.\n- **pb** (*tf.float32*)  Error probability. Can be a scalar or of any shape that can be\nbroadcasted to the shape of `x`.\n\n\nOutput\n\n*[,n], tf.float32*  Output sequence of same length as the input `x`. If\n`return_llrs` is <cite>False</cite>, the output is binary and otherwise\nsoft-values are returned.\n\n\nReferences:\n[GumbleSoftmax](https://nvlabs.github.io/sionna/api/channel.discrete.html#id1)\n<ol class=\"upperalpha simple\" start=\"5\">\n- Jang, G. Shixiang, and Ben Poole. <cite>Categorical reparameterization with gumbel-softmax,</cite> arXiv preprint arXiv:1611.01144 (2016).\n</ol>\n\n[LearningShaping](https://nvlabs.github.io/sionna/api/channel.discrete.html#id2)\n<ol class=\"upperalpha simple\" start=\"13\">\n- Stark, F. Ait Aoudia, and J. Hoydis. <cite>Joint learning of geometric and probabilistic constellation shaping,</cite> 2019 IEEE Globecom Workshops (GC Wkshps). IEEE, 2019.\n</ol>"

"# Part 2: Differentiable Communication Systems\n\nThis tutorial will guide you through Sionna, from its basic principles to the implementation of a point-to-point link with a 5G NR compliant code and a 3GPP channel model. You will also learn how to write custom trainable layers by implementing a state of the art neural receiver, and how to train and evaluate end-to-end communication systems.\n\nThe tutorial is structured in four notebooks:\n\n- Part I: Getting started with Sionna\n- **Part II: Differentiable Communication Systems**\n- Part III: Advanced Link-level Simulations\n- Part IV: Toward Learned Receivers\n\n\nThe [official documentation](https://nvlabs.github.io/sionna) provides key material on how to use Sionna and how its components are implemented."
"## Imports\n\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Import Sionna\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna as sn\n# Import TensorFlow and NumPy\nimport tensorflow as tf\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\nimport numpy as np\n# For plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\n# For saving complex Python data structures efficiently\nimport pickle\n# For the implementation of the neural receiver\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Layer\n```"
"## Gradient Computation Through End-to-end Systems\n\nLets start by setting up a simple communication system that transmit bits modulated as QAM symbols over an AWGN channel.\n\nHowever, compared to what we have previously done, we now make the constellation *trainable*. With Sionna, achieving this requires only setting a boolean parameter to `True` when instantiating the `Constellation` object.\n\n\n```python\n# Binary source to generate uniform i.i.d. bits\nbinary_source = sn.utils.BinarySource()\n# 256-QAM constellation\nNUM_BITS_PER_SYMBOL = 6\nconstellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL, trainable=True) # The constellation is set to be trainable\n# Mapper and demapper\nmapper = sn.mapping.Mapper(constellation=constellation)\ndemapper = sn.mapping.Demapper(\"app\", constellation=constellation)\n# AWGN channel\nawgn_channel = sn.channel.AWGN()\n```\n\n\nAs we have already seen, we can now easily simulate forward passes through the system we have just setup\n\n\n```python\nBATCH_SIZE = 128 # How many examples are processed by Sionna in parallel\nEBN0_DB = 17.0 # Eb/N0 in dB\nno = sn.utils.ebnodb2no(ebno_db=EBN0_DB,\n                        num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n                        coderate=1.0) # Coderate set to 1 as we do uncoded transmission here\nbits = binary_source([BATCH_SIZE,\n                        1200]) # Blocklength\nx = mapper(bits)\ny = awgn_channel([x, no])\nllr = demapper([y,no])\n```\n\n\nJust for fun, lets visualize the channel inputs and outputs\n\n\n```python\nplt.figure(figsize=(8,8))\nplt.axes().set_aspect(1.0)\nplt.grid(True)\nplt.scatter(tf.math.real(y), tf.math.imag(y), label='Output')\nplt.scatter(tf.math.real(x), tf.math.imag(x), label='Input')\nplt.legend(fontsize=20);\n```\n\n\nLets now *optimize* the constellation through *stochastic gradient descent* (SGD). As we will see, this is made very easy by Sionna.\n\nWe need to define a *loss function* that we will aim to minimize.\n\nWe can see the task of the receiver as jointly solving, for each received symbol, `NUM_BITS_PER_SYMBOL` binary classification problems in order to reconstruct the transmitted bits. Therefore, a natural choice for the loss function is the *binary cross-entropy* (BCE) applied to each bit and to each received symbol.\n\n*Remark:* The LLRs computed by the demapper are *logits* on the transmitted bits, and can therefore be used as-is to compute the BCE without any additional processing. *Remark 2:* The BCE is closely related to an achieveable information rate for bit-interleaved coded modulation systems [1,2]\n\n[1] Georg Bcherer, Principles of Coded Modulation, [available online](http://www.georg-boecherer.de/bocherer2018principles.pdf)\n\n[2] F. Ait Aoudia and J. Hoydis, End-to-End Learning for OFDM: From Neural Receivers to Pilotless Communication, in IEEE Transactions on Wireless Communications, vol.21, no. 2, pp.1049-1063, Feb.2022, doi: 10.1109/TWC.2021.3101364."
"```python\nbce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nprint(f\"BCE: {bce(bits, llr)}\")\n```\n\n\n```python\nBCE: 0.0001015052548609674\n```\n\n\nOne iteration of SGD consists in three steps: 1. Perform a forward pass through the end-to-end system and compute the loss function 2. Compute the gradient of the loss function with respect to the trainable weights 3. Apply the gradient to the weights\n\nTo enable gradient computation, we need to perform the forward pass (step 1) within a `GradientTape`\n\n\n```python\nwith tf.GradientTape() as tape:\n    bits = binary_source([BATCH_SIZE,\n                            1200]) # Blocklength\n    x = mapper(bits)\n    y = awgn_channel([x, no])\n    llr = demapper([y,no])\n    loss = bce(bits, llr)\n```\n\n\nUsing the `GradientTape`, computing the gradient is done as follows\n\n\n```python\ngradient = tape.gradient(loss, tape.watched_variables())\n```\n\n\n`gradient` is a list of tensor, each tensor corresponding to a trainable variable of our model.\n\nFor this model, we only have a single trainable tensor: The constellation of shape [`2`, `2^NUM_BITS_PER_SYMBOL`], the first dimension corresponding to the real and imaginary components of the constellation points.\n\n*Remark:* It is important to notice that the gradient computation was performed *through the demapper and channel*, which are conventional non-trainable algorithms implemented as *differentiable* Keras layers. This key feature of Sionna enables the training of end-to-end communication systems that combine both trainable and conventional and/or non-trainable signal processing algorithms.\n\n\n```python\nfor g in gradient:\n    print(g.shape)\n```\n\n\n```python\n(2, 64)\n```\n\n\nApplying the gradient (third step) is performed using an *optimizer*. [Many optimizers are available as part of TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), and we use in this notebook `Adam`.\n\n\n```python\noptimizer = tf.keras.optimizers.Adam(1e-2)\n```\n\n\nUsing the optimizer, the gradients can be applied to the trainable weights to update them\n\n\n```python\noptimizer.apply_gradients(zip(gradient, tape.watched_variables()));\n```\n\n\nLet compare the constellation before and after the gradient application"
"```python\nfig = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL).show()\nfig.axes[0].scatter(tf.math.real(constellation.points), tf.math.imag(constellation.points), label='After SGD')\nfig.axes[0].legend();\n```\n\n\nThe SGD step has led to slight change in the position of the constellation points. Training of a communication system using SGD consists in looping over such SGD steps until a stop criterion is met."
"## Creating Custom Layers\n\nCustom trainable (or not trainable) algorithms should be implemented as [Keras layers](https://keras.io/api/layers/). All Sionna components, such as the mapper, demapper, channel are implemented as Keras layers.\n\nTo illustrate how this can be done, the next cell implements a simple neural network-based demapper which consists of three dense layers.\n\n\n```python\nclass NeuralDemapper(Layer): # Inherits from Keras Layer\n    def __init__(self):\n        super().__init__()\n        # The three dense layers that form the custom trainable neural network-based demapper\n        self.dense_1 = Dense(64, 'relu')\n        self.dense_2 = Dense(64, 'relu')\n        self.dense_3 = Dense(NUM_BITS_PER_SYMBOL, None) # The last layer has no activation and therefore outputs logits, i.e., LLRs\n    def call(self, y):\n        # y : complex-valued with shape [batch size, block length]\n        # y is first mapped to a real-valued tensor with shape\n        #  [batch size, block length, 2]\n        # where the last dimension consists of the real and imaginary components\n        # The dense layers operate on the last dimension, and treat the inner dimensions as batch dimensions, i.e.,\n        # all the received symbols are independently processed.\n        nn_input = tf.stack([tf.math.real(y), tf.math.imag(y)], axis=-1)\n        z = self.dense_1(nn_input)\n        z = self.dense_2(z)\n        z = self.dense_3(z) # [batch size, number of symbols per block, number of bits per symbol]\n        llr = tf.reshape(z, [tf.shape(y)[0], -1]) # [batch size, number of bits per block]\n        return llr\n```\n\n\nA custom Keras layer is used as any other Sionna layer, and therefore integration to a Sionna-based communication is straightforward.\n\nThe following model uses the neural demapper instead of the conventional demapper. It takes at initialization a parameter that indicates if the model is intantiated to be trained or evaluated. When instantiated to be trained, the loss function is returned. Otherwise, the transmitted bits and LLRs are returned.\n\n\n```python\nclass End2EndSystem(Model): # Inherits from Keras Model\n    def __init__(self, training):\n        super().__init__() # Must call the Keras model initializer\n        self.constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL, trainable=True) # Constellation is trainable\n        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n        self.demapper = NeuralDemapper() # Intantiate the NeuralDemapper custom layer as any other\n        self.binary_source = sn.utils.BinarySource()\n        self.awgn_channel = sn.channel.AWGN()\n        self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True) # Loss function\n        self.training = training\n    @tf.function(jit_compile=True) # Enable graph execution to speed things up\n    def __call__(self, batch_size, ebno_db):\n        # no channel coding used; we set coderate=1.0\n        no = sn.utils.ebnodb2no(ebno_db,\n                                num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n                                coderate=1.0)\n        bits = self.binary_source([batch_size, 1200]) # Blocklength set to 1200 bits\n        x = self.mapper(bits)\n        y = self.awgn_channel([x, no])\n        llr = self.demapper(y)  # Call the NeuralDemapper custom layer as any other\n        if self.training:\n            loss = self.bce(bits, llr)\n            return loss\n        else:\n            return bits, llr\n```"
"When a model that includes a neural network is created, the neural network weights are randomly initialized typically leading to very poor performance.\n\nTo see this, the following cell benchmarks the previously defined untrained model against a conventional baseline.\n\n\n```python\nEBN0_DB_MIN = 10.0\nEBN0_DB_MAX = 20.0\n\n###############################\n# Baseline\n###############################\nclass Baseline(Model): # Inherits from Keras Model\n    def __init__(self):\n        super().__init__() # Must call the Keras model initializer\n        self.constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n        self.binary_source = sn.utils.BinarySource()\n        self.awgn_channel = sn.channel.AWGN()\n    @tf.function # Enable graph execution to speed things up\n    def __call__(self, batch_size, ebno_db):\n        # no channel coding used; we set coderate=1.0\n        no = sn.utils.ebnodb2no(ebno_db,\n                                num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n                                coderate=1.0)\n        bits = self.binary_source([batch_size, 1200]) # Blocklength set to 1200 bits\n        x = self.mapper(bits)\n        y = self.awgn_channel([x, no])\n        llr = self.demapper([y,no])\n        return bits, llr\n###############################\n# Benchmarking\n###############################\nbaseline = Baseline()\nmodel = End2EndSystem(False)\nber_plots = sn.utils.PlotBER(\"Neural Demapper\")\nber_plots.simulate(baseline,\n                  ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20),\n                  batch_size=BATCH_SIZE,\n                  num_target_block_errors=100, # simulate until 100 block errors occured\n                  legend=\"Baseline\",\n                  soft_estimates=True,\n                  max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n                  show_fig=False);\nber_plots.simulate(model,\n                  ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20),\n                  batch_size=BATCH_SIZE,\n                  num_target_block_errors=100, # simulate until 100 block errors occured\n                  legend=\"Untrained model\",\n                  soft_estimates=True,\n                  max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n                  show_fig=True);\n```"
"```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     10.0 | 2.6927e-02 | 1.0000e+00 |        4136 |      153600 |          128 |         128 |         0.7 |reached target block errors\n   10.526 | 2.1426e-02 | 1.0000e+00 |        3291 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.053 | 1.6100e-02 | 1.0000e+00 |        2473 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.579 | 1.2051e-02 | 1.0000e+00 |        1851 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.105 | 9.1927e-03 | 1.0000e+00 |        1412 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.632 | 6.5234e-03 | 1.0000e+00 |        1002 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   13.158 | 4.4792e-03 | 9.8438e-01 |         688 |      153600 |          126 |         128 |         0.0 |reached target block errors\n   13.684 | 2.7474e-03 | 9.6875e-01 |         422 |      153600 |          124 |         128 |         0.0 |reached target block errors\n   14.211 | 1.6146e-03 | 8.8281e-01 |         248 |      153600 |          113 |         128 |         0.0 |reached target block errors\n   14.737 | 9.9609e-04 | 7.0312e-01 |         306 |      307200 |          180 |         256 |         0.0 |reached target block errors\n   15.263 | 5.2083e-04 | 4.7266e-01 |         160 |      307200 |          121 |         256 |         0.0 |reached target block errors\n   15.789 | 3.4071e-04 | 3.3333e-01 |         157 |      460800 |          128 |         384 |         0.0 |reached target block errors\n   16.316 | 1.4193e-04 | 1.5781e-01 |         109 |      768000 |          101 |         640 |         0.0 |reached target block errors\n   16.842 | 6.0961e-05 | 7.1023e-02 |         103 |     1689600 |          100 |        1408 |         0.1 |reached target block errors\n   17.368 | 2.4113e-05 | 2.8935e-02 |         100 |     4147200 |          100 |        3456 |         0.2 |reached target block errors\n   17.895 | 7.6593e-06 | 9.1912e-03 |         100 |    13056000 |          100 |       10880 |         0.5 |reached target block errors\n   18.421 | 2.7995e-06 | 3.3594e-03 |          43 |    15360000 |           43 |       12800 |         0.6 |reached max iter\n   18.947 | 6.5104e-07 | 7.8125e-04 |          10 |    15360000 |           10 |       12800 |         0.6 |reached max iter\n   19.474 | 6.5104e-08 | 7.8125e-05 |           1 |    15360000 |            1 |       12800 |         0.5 |reached max iter\n     20.0 | 0.0000e+00 | 0.0000e+00 |           0 |    15360000 |            0 |       12800 |         0.5 |reached max iter\nSimulation stopped as no error occurred @ EbNo = 20.0 dB.\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     10.0 | 4.7460e-01 | 1.0000e+00 |       72899 |      153600 |          128 |         128 |         1.3 |reached target block errors\n   10.526 | 4.7907e-01 | 1.0000e+00 |       73585 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.053 | 4.7525e-01 | 1.0000e+00 |       72999 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.579 | 4.7865e-01 | 1.0000e+00 |       73521 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.105 | 4.7684e-01 | 1.0000e+00 |       73242 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.632 | 4.7469e-01 | 1.0000e+00 |       72913 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   13.158 | 4.7614e-01 | 1.0000e+00 |       73135 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   13.684 | 4.7701e-01 | 1.0000e+00 |       73268 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   14.211 | 4.7544e-01 | 1.0000e+00 |       73027 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   14.737 | 4.7319e-01 | 1.0000e+00 |       72682 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   15.263 | 4.7740e-01 | 1.0000e+00 |       73329 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   15.789 | 4.7385e-01 | 1.0000e+00 |       72783 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   16.316 | 4.7344e-01 | 1.0000e+00 |       72721 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   16.842 | 4.7303e-01 | 1.0000e+00 |       72658 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   17.368 | 4.7378e-01 | 1.0000e+00 |       72773 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   17.895 | 4.7257e-01 | 1.0000e+00 |       72586 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   18.421 | 4.7377e-01 | 1.0000e+00 |       72771 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   18.947 | 4.7315e-01 | 1.0000e+00 |       72676 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   19.474 | 4.7217e-01 | 1.0000e+00 |       72525 |      153600 |          128 |         128 |         0.0 |reached target block errors\n     20.0 | 4.7120e-01 | 1.0000e+00 |       72376 |      153600 |          128 |         128 |         0.0 |reached target block errors\n```"
"## Setting up Training Loops\n\nTraining of end-to-end communication systems consists in iterating over SGD steps.\n\nThe next cell implements a training loop of `NUM_TRAINING_ITERATIONS` iterations. The training SNR is set to $E_b/N_0 = 15$ dB.\n\nAt each iteration: - A forward pass through the end-to-end system is performed within a gradient tape - The gradients are computed using the gradient tape, and applied using the Adam optimizer - The estimated loss is periodically printed to follow the progress of training\n\n\n```python\n# Number of iterations used for training\nNUM_TRAINING_ITERATIONS = 30000\n# Set a seed for reproducibility\ntf.random.set_seed(1)\n# Instantiating the end-to-end model for training\nmodel_train = End2EndSystem(training=True)\n# Adam optimizer (SGD variant)\noptimizer = tf.keras.optimizers.Adam()\n# Training loop\nfor i in range(NUM_TRAINING_ITERATIONS):\n    # Forward pass\n    with tf.GradientTape() as tape:\n        loss = model_train(BATCH_SIZE, 15.0) # The model is assumed to return the BMD rate\n    # Computing and applying gradients\n    grads = tape.gradient(loss, model_train.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model_train.trainable_weights))\n    # Print progress\n    if i % 100 == 0:\n        print(f\"{i}/{NUM_TRAINING_ITERATIONS}  Loss: {loss:.2E}\", end=\"\\r\")\n```\n\n\n```python\n29900/30000  Loss: 2.02E-03\n```\n\n\nThe weights of the trained model are saved using [pickle](https://docs.python.org/3/library/pickle.html).\n\n\n```python\n# Save the weightsin a file\nweights = model_train.get_weights()\nwith open('weights-neural-demapper', 'wb') as f:\n    pickle.dump(weights, f)\n```\n\n\nFinally, we evaluate the trained model and benchmark it against the previously introduced baseline.\n\nWe first instantiate the model for evaluation and load the saved weights.\n\n\n```python\n# Instantiating the end-to-end model for evaluation\nmodel = End2EndSystem(training=False)\n# Run one inference to build the layers and loading the weights\nmodel(tf.constant(1, tf.int32), tf.constant(10.0, tf.float32))\nwith open('weights-neural-demapper', 'rb') as f:\n    weights = pickle.load(f)\n    model.set_weights(weights)\n```"
"The trained model is then evaluated.\n\n\n```python\n# Computing and plotting BER\nber_plots.simulate(model,\n                  ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20),\n                  batch_size=BATCH_SIZE,\n                  num_target_block_errors=100,\n                  legend=\"Trained model\",\n                  soft_estimates=True,\n                  max_mc_iter=100,\n                  show_fig=True);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     10.0 | 2.6094e-02 | 1.0000e+00 |        4008 |      153600 |          128 |         128 |         0.4 |reached target block errors\n   10.526 | 2.0768e-02 | 1.0000e+00 |        3190 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.053 | 1.5729e-02 | 1.0000e+00 |        2416 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   11.579 | 1.1667e-02 | 1.0000e+00 |        1792 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.105 | 8.3789e-03 | 1.0000e+00 |        1287 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   12.632 | 6.1458e-03 | 1.0000e+00 |         944 |      153600 |          128 |         128 |         0.0 |reached target block errors\n   13.158 | 3.8411e-03 | 9.7656e-01 |         590 |      153600 |          125 |         128 |         0.0 |reached target block errors\n   13.684 | 2.8971e-03 | 9.7656e-01 |         445 |      153600 |          125 |         128 |         0.0 |reached target block errors\n   14.211 | 1.6602e-03 | 8.4375e-01 |         255 |      153600 |          108 |         128 |         0.0 |reached target block errors\n   14.737 | 9.8958e-04 | 6.7578e-01 |         304 |      307200 |          173 |         256 |         0.0 |reached target block errors\n   15.263 | 5.0130e-04 | 4.7656e-01 |         154 |      307200 |          122 |         256 |         0.0 |reached target block errors\n   15.789 | 2.5228e-04 | 2.6367e-01 |         155 |      614400 |          135 |         512 |         0.0 |reached target block errors\n   16.316 | 1.4453e-04 | 1.6250e-01 |         111 |      768000 |          104 |         640 |         0.0 |reached target block errors\n   16.842 | 5.2548e-05 | 5.8594e-02 |         113 |     2150400 |          105 |        1792 |         0.1 |reached target block errors\n   17.368 | 2.7083e-05 | 3.1875e-02 |         104 |     3840000 |          102 |        3200 |         0.1 |reached target block errors\n   17.895 | 8.6520e-06 | 1.0382e-02 |         101 |    11673600 |          101 |        9728 |         0.3 |reached target block errors\n   18.421 | 2.7344e-06 | 3.2812e-03 |          42 |    15360000 |           42 |       12800 |         0.4 |reached max iter\n   18.947 | 8.4635e-07 | 1.0156e-03 |          13 |    15360000 |           13 |       12800 |         0.4 |reached max iter\n   19.474 | 1.3021e-07 | 1.5625e-04 |           2 |    15360000 |            2 |       12800 |         0.4 |reached max iter\n     20.0 | 0.0000e+00 | 0.0000e+00 |           0 |    15360000 |            0 |       12800 |         0.4 |reached max iter\nSimulation stopped as no error occurred @ EbNo = 20.0 dB.\n\n```"

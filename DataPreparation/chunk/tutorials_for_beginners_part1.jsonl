"# Part 1: Getting Started with Sionna\n\nThis tutorial will guide you through Sionna, from its basic principles to the implementation of a point-to-point link with a 5G NR compliant code and a 3GPP channel model. You will also learn how to write custom trainable layers by implementing a state of the art neural receiver, and how to train and evaluate end-to-end communication systems.\n\nThe tutorial is structured in four notebooks:\n\n- **Part I: Getting started with Sionna**\n- Part II: Differentiable Communication Systems\n- Part III: Advanced Link-level Simulations\n- Part IV: Toward Learned Receivers\n\n\nThe [official documentation](https://nvlabs.github.io/sionna) provides key material on how to use Sionna and how its components are implemented."
"## Imports & Basics\n\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Import Sionna\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna as sn\n# Import TensorFlow and NumPy\nimport tensorflow as tf\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\nimport numpy as np\n# For plotting\n%matplotlib inline\n# also try %matplotlib widget\nimport matplotlib.pyplot as plt\n# for performance measurements\nimport time\n# For the implementation of the Keras models\nfrom tensorflow.keras import Model\n```\n\n\nWe can now access Sionna functions within the `sn` namespace.\n\n**Hint**: In Jupyter notebooks, you can run bash commands with `!`.\n\n\n```python\n!nvidia-smi\n```\n\n\n```python\nTue Mar 15 14:47:45 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| 30%   51C    P8    23W / 350W |     53MiB / 24265MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce ...  Off  | 00000000:4C:00.0 Off |                  N/A |\n|  0%   33C    P8    24W / 350W |      8MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n```"
"## Sionna Data-flow and Design Paradigms\n\nSionna inherently parallelizes simulations via *batching*, i.e., each element in the batch dimension is simulated independently.\n\nThis means the first tensor dimension is always used for *inter-frame* parallelization similar to an outer *for-loop* in Matlab/NumPy simulations, but operations can be operated in parallel.\n\nTo keep the dataflow efficient, Sionna follows a few simple design principles:\n\n- Signal-processing components are implemented as an individual [Keras layer](https://keras.io/api/layers/).\n- `tf.float32` is used as preferred datatype and `tf.complex64` for complex-valued datatypes, respectively.\nThis allows simpler re-use of components (e.g., the same scrambling layer can be used for binary inputs and LLR-values).\n- `tf.float64`/`tf.complex128` are available when high precision is needed.\n- Models can be developed in *eager mode* allowing simple (and fast) modification of system parameters.\n- Number crunching simulations can be executed in the faster *graph mode* or even *XLA* acceleration (experimental) is available for most components.\n- Whenever possible, components are automatically differentiable via [auto-grad](https://www.tensorflow.org/guide/autodiff) to simplify the deep learning design-flow.\n- Code is structured into sub-packages for different tasks such as channel coding, mapping, (see [API documentation](http://nvlabs.github.io/sionna/api/sionna.html) for details).\n\n\nThese paradigms simplify the re-useability and reliability of our components for a wide range of communications related applications."
"## Hello, Sionna!\n\nLets start with a very simple simulation: Transmitting QAM symbols over an AWGN channel. We will implement the system shown in the figure below.\n\n\nWe will use upper case for naming simulation parameters that are used throughout this notebook\n\nEvery layer needs to be initialized once before it can be used.\n\n**Tip**: Use the [API documentation](http://nvlabs.github.io/sionna/api/sionna.html) to find an overview of all existing components. You can directly access the signature and the docstring within jupyter via `Shift+TAB`.\n\n*Remark*: Most layers are defined to be complex-valued.\n\nWe first need to create a QAM constellation.\n\n\n```python\nNUM_BITS_PER_SYMBOL = 2 # QPSK\nconstellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\nconstellation.show();\n```\n\n\n**Task:** Try to change the modulation order, e.g., to 16-QAM.\n\nWe then need to setup a mapper to map bits into constellation points. The mapper takes as parameter the constellation.\n\nWe also need to setup a corresponding demapper to compute log-likelihood ratios (LLRs) from received noisy samples.\n\n\n```python\nmapper = sn.mapping.Mapper(constellation=constellation)\n# The demapper uses the same constellation object as the mapper\ndemapper = sn.mapping.Demapper(\"app\", constellation=constellation)\n```\n\n\n**Tip**: You can access the signature+docstring via `?` command and print the complete class definition via `??` operator.\n\nObviously, you can also access the source code via [https://github.com/nvlabs/sionna/](https://github.com/nvlabs/sionna/).\n\n\n```python\n# print class definition of the Constellation class\nsn.mapping.Mapper??\n```\n\n\n```python\nInit signature: sn.mapping.Mapper(*args, **kwargs)\nSource:\nclass Mapper(Layer):\n    # pylint: disable=line-too-long\n    r&#34;&#34;&#34;\n    Mapper(constellation_type=None, num_bits_per_symbol=None, constellation=None, dtype=tf.complex64, **kwargs)\n    Maps binary tensors to points of a constellation.\n    This class defines a layer that maps a tensor of binary values\n    to a tensor of points from a provided constellation.\n    Parameters\n    ----------\n    constellation_type : One of [&#34;qam&#34;, &#34;pam&#34;, &#34;custom&#34;], str\n        For &#34;custom&#34;, an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [&#34;qam&#34;, &#34;pam&#34;].\n    constellation :  Constellation\n        An instance of :class:`~sionna.mapping.Constellation` or\n        `None`. In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n    dtype : One of [tf.complex64, tf.complex128], tf.DType\n        The output dtype. Defaults to tf.complex64.\n    Input\n    -----\n    : [..., n], tf.float or tf.int\n        Tensor with with binary entries.\n    Output\n    ------\n    : [...,n/Constellation.num_bits_per_symbol], tf.complex\n        The mapped constellation symbols.\n    Note\n    ----\n    The last input dimension must be an integer multiple of the\n    number of bits per constellation symbol.\n    &#34;&#34;&#34;\n    def __init__(self,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 dtype=tf.complex64,\n                 **kwargs\n                ):\n        super().__init__(dtype=dtype, **kwargs)\n        assert dtype in [tf.complex64, tf.complex128],\\\n            &#34;dtype must be tf.complex64 or tf.complex128&#34;\n        #self._dtype = dtype\n        #print(dtype, self._dtype)\n        if constellation is not None:\n            assert constellation_type in [None, &#34;custom&#34;], \\\n                &#34;&#34;&#34;`constellation_type` must be &#34;custom&#34;.&#34;&#34;&#34;\n            assert num_bits_per_symbol in \\\n                     [None, constellation.num_bits_per_symbol], \\\n                &#34;&#34;&#34;`Wrong value of `num_bits_per_symbol.`&#34;&#34;&#34;\n            self._constellation = constellation\n        else:\n            assert constellation_type in [&#34;qam&#34;, &#34;pam&#34;], \\\n                &#34;Wrong constellation type.&#34;\n            assert num_bits_per_symbol is not None, \\\n                &#34;`num_bits_per_symbol` must be provided.&#34;\n            self._constellation = Constellation(constellation_type,\n                                                num_bits_per_symbol,\n                                                dtype=self._dtype)\n        self._binary_base = 2**tf.constant(\n                        range(self.constellation.num_bits_per_symbol-1,-1,-1))\n    @property\n    def constellation(self):\n        &#34;&#34;&#34;The Constellation used by the Mapper.&#34;&#34;&#34;\n        return self._constellation\n    def call(self, inputs):\n        tf.debugging.assert_greater_equal(tf.rank(inputs), 2,\n            message=&#34;The input must have at least rank 2&#34;)\n        # Reshape inputs to the desired format\n        new_shape = [-1] + inputs.shape[1:-1].as_list() + \\\n           [int(inputs.shape[-1] / self.constellation.num_bits_per_symbol),\n            self.constellation.num_bits_per_symbol]\n        inputs_reshaped = tf.cast(tf.reshape(inputs, new_shape), tf.int32)\n        # Convert the last dimension to an integer\n        int_rep = tf.reduce_sum(inputs_reshaped * self._binary_base, axis=-1)\n        # Map integers to constellation symbols\n        x = tf.gather(self.constellation.points, int_rep, axis=0)\n        return x\nFile:           ~/.local/lib/python3.8/site-packages/sionna/mapping.py\nType:           type\nSubclasses:\n```"
"As can be seen, the `Mapper` class inherits from `Layer`, i.e., implements a Keras layer.\n\nThis allows to simply built complex systems by using the [Keras functional API](https://keras.io/guides/functional_api/) to stack layers.\n\nSionna provides as utility a binary source to sample uniform i.i.d. bits.\n\n\n```python\nbinary_source = sn.utils.BinarySource()\n```\n\n\nFinally, we need the AWGN channel.\n\n\n```python\nawgn_channel = sn.channel.AWGN()\n```\n\n\nSionna provides a utility function to compute the noise power spectral density ratio $N_0$ from the energy per bit to noise power spectral density ratio $E_b/N_0$ in dB and a variety of parameters such as the coderate and the nunber of bits per symbol.\n\n\n```python\nno = sn.utils.ebnodb2no(ebno_db=10.0,\n                        num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n                        coderate=1.0) # Coderate set to 1 as we do uncoded transmission here\n```\n\n\nWe now have all the components we need to transmit QAM symbols over an AWGN channel.\n\nSionna natively supports multi-dimensional tensors.\n\nMost layers operate at the last dimension and can have arbitrary input shapes (preserved at output).\n\n\n```python\nBATCH_SIZE = 64 # How many examples are processed by Sionna in parallel\nbits = binary_source([BATCH_SIZE,\n                      1024]) # Blocklength\nprint(\"Shape of bits: \", bits.shape)\nx = mapper(bits)\nprint(\"Shape of x: \", x.shape)\ny = awgn_channel([x, no])\nprint(\"Shape of y: \", y.shape)\nllr = demapper([y, no])\nprint(\"Shape of llr: \", llr.shape)\n```\n\n\n```python\nShape of bits:  (64, 1024)\nShape of x:  (64, 512)\nShape of y:  (64, 512)\nShape of llr:  (64, 1024)\n```\n\n\nIn *Eager* mode, we can directly access the values of each tensor. This simplify debugging.\n\n\n```python\nnum_samples = 8 # how many samples shall be printed\nnum_symbols = int(num_samples/NUM_BITS_PER_SYMBOL)\nprint(f\"First {num_samples} transmitted bits: {bits[0,:num_samples]}\")\nprint(f\"First {num_symbols} transmitted symbols: {np.round(x[0,:num_symbols], 2)}\")\nprint(f\"First {num_symbols} received symbols: {np.round(y[0,:num_symbols], 2)}\")\nprint(f\"First {num_samples} demapped llrs: {np.round(llr[0,:num_samples], 2)}\")\n```"
"```python\nFirst 8 transmitted bits: [0. 1. 1. 1. 1. 0. 1. 0.]\nFirst 4 transmitted symbols: [ 0.71-0.71j -0.71-0.71j -0.71+0.71j -0.71+0.71j]\nFirst 4 received symbols: [ 0.65-0.61j -0.62-0.69j -0.6 +0.72j -0.86+0.63j]\nFirst 8 demapped llrs: [-36.65  34.36  35.04  38.83  33.96 -40.5   48.47 -35.65]\n```\n\n\nLets visualize the received noisy samples.\n\n\n```python\nplt.figure(figsize=(8,8))\nplt.axes().set_aspect(1)\nplt.grid(True)\nplt.title('Channel output')\nplt.xlabel('Real Part')\nplt.ylabel('Imaginary Part')\nplt.scatter(tf.math.real(y), tf.math.imag(y))\nplt.tight_layout()\n```\n\n\n**Task:** One can play with the SNR to visualize the impact on the received samples.\n\n**Advanced Task:** Compare the LLR distribution for app demapping with maxlog demapping. The [Bit-Interleaved Coded Modulation](https://nvlabs.github.io/sionna/examples/Bit_Interleaved_Coded_Modulation.html) example notebook can be helpful for this task."
"## Communication Systems as Keras Models\n\nIt is typically more convenient to wrap a Sionna-based communication system into a [Keras models](https://keras.io/api/models/model/).\n\nThese models can be simply built by using the [Keras functional API](https://keras.io/guides/functional_api/) to stack layers.\n\nThe following cell implements the previous system as a Keras model.\n\nThe key functions that need to be defined are `__init__()`, which instantiates the required components, and `__call()__`, which performs forward pass through the end-to-end system.\n\n\n```python\nclass UncodedSystemAWGN(Model): # Inherits from Keras Model\n    def __init__(self, num_bits_per_symbol, block_length):\n        \"\"\"\n        A keras model of an uncoded transmission over the AWGN channel.\n        Parameters\n        ----------\n        num_bits_per_symbol: int\n            The number of bits per constellation symbol, e.g., 4 for QAM16.\n        block_length: int\n            The number of bits per transmitted message block (will be the codeword length later).\n        Input\n        -----\n        batch_size: int\n            The batch_size of the Monte-Carlo simulation.\n        ebno_db: float\n            The `Eb/No` value (=rate-adjusted SNR) in dB.\n        Output\n        ------\n        (bits, llr):\n            Tuple:\n        bits: tf.float32\n            A tensor of shape `[batch_size, block_length] of 0s and 1s\n            containing the transmitted information bits.\n        llr: tf.float32\n            A tensor of shape `[batch_size, block_length] containing the\n            received log-likelihood-ratio (LLR) values.\n        \"\"\"\n        super().__init__() # Must call the Keras model initializer\n        self.num_bits_per_symbol = num_bits_per_symbol\n        self.block_length = block_length\n        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n        self.binary_source = sn.utils.BinarySource()\n        self.awgn_channel = sn.channel.AWGN()\n    # @tf.function # Enable graph execution to speed things up\n    def __call__(self, batch_size, ebno_db):\n        # no channel coding used; we set coderate=1.0\n        no = sn.utils.ebnodb2no(ebno_db,\n                                num_bits_per_symbol=self.num_bits_per_symbol,\n                                coderate=1.0)\n        bits = self.binary_source([batch_size, self.block_length]) # Blocklength set to 1024 bits\n        x = self.mapper(bits)\n        y = self.awgn_channel([x, no])\n        llr = self.demapper([y,no])\n        return bits, llr\n```"
"We need first to instantiate the model.\n\n\n```python\nmodel_uncoded_awgn = UncodedSystemAWGN(num_bits_per_symbol=NUM_BITS_PER_SYMBOL, block_length=1024)\n```\n\n\nSionna provides a utility to easily compute and plot the bit error rate (BER).\n\n\n```python\nEBN0_DB_MIN = -3.0 # Minimum value of Eb/N0 [dB] for simulations\nEBN0_DB_MAX = 5.0 # Maximum value of Eb/N0 [dB] for simulations\nBATCH_SIZE = 2000 # How many examples are processed by Sionna in parallel\nber_plots = sn.utils.PlotBER(\"AWGN\")\nber_plots.simulate(model_uncoded_awgn,\n                  ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20),\n                  batch_size=BATCH_SIZE,\n                  num_target_block_errors=100, # simulate until 100 block errors occured\n                  legend=\"Uncoded\",\n                  soft_estimates=True,\n                  max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n                  show_fig=True);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     -3.0 | 1.5825e-01 | 1.0000e+00 |      324099 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -2.579 | 1.4687e-01 | 1.0000e+00 |      300799 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -2.158 | 1.3528e-01 | 1.0000e+00 |      277061 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -1.737 | 1.2323e-01 | 1.0000e+00 |      252373 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -1.316 | 1.1246e-01 | 1.0000e+00 |      230320 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -0.895 | 1.0107e-01 | 1.0000e+00 |      206992 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -0.474 | 9.0021e-02 | 1.0000e+00 |      184362 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n   -0.053 | 8.0165e-02 | 1.0000e+00 |      164177 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    0.368 | 6.9933e-02 | 1.0000e+00 |      143222 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    0.789 | 6.0897e-02 | 1.0000e+00 |      124717 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    1.211 | 5.2020e-02 | 1.0000e+00 |      106537 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    1.632 | 4.3859e-02 | 1.0000e+00 |       89823 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    2.053 | 3.6686e-02 | 1.0000e+00 |       75132 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    2.474 | 3.0071e-02 | 1.0000e+00 |       61586 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    2.895 | 2.4304e-02 | 1.0000e+00 |       49775 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    3.316 | 1.9330e-02 | 1.0000e+00 |       39588 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    3.737 | 1.4924e-02 | 1.0000e+00 |       30565 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    4.158 | 1.1227e-02 | 1.0000e+00 |       22992 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n    4.579 | 8.2632e-03 | 1.0000e+00 |       16923 |     2048000 |         2000 |        2000 |         0.0 |reached target block errors\n      5.0 | 5.9722e-03 | 9.9850e-01 |       12231 |     2048000 |         1997 |        2000 |         0.0 |reached target block errors\n```"
"The `sn.utils.PlotBER` object stores the results and allows to add additional simulations to the previous curves.\n\n*Remark*: In Sionna, a block error is defined to happen if for two tensors at least one position in the last dimension differs (i.e., at least one bit wrongly received per codeword). The bit error rate the total number of erroneous positions divided by the total number of transmitted bits."
"## Forward Error Correction (FEC)\n\nWe now add channel coding to our transceiver to make it more robust against transmission errors. For this, we will use [5G compliant low-density parity-check (LDPC) codes and Polar codes](https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3214). You can find more detailed information in the notebooks [Bit-Interleaved Coded Modulation (BICM)](https://nvlabs.github.io/sionna/examples/Bit_Interleaved_Coded_Modulation.html) and <a class=\"reference external\" href=\"https://nvlabs.github.io/sionna/examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html\">5G Channel Coding\nand Rate-Matching: Polar vs.LDPC Codes</a>.\n\n\n```python\nk = 12\nn = 20\nencoder = sn.fec.ldpc.LDPC5GEncoder(k, n)\ndecoder = sn.fec.ldpc.LDPC5GDecoder(encoder, hard_out=True)\n```\n\n\nLet us encode some random input bits.\n\n\n```python\nBATCH_SIZE = 1 # one codeword in parallel\nu = binary_source([BATCH_SIZE, k])\nprint(\"Input bits are: \\n\", u.numpy())\nc = encoder(u)\nprint(\"Encoded bits are: \\n\", c.numpy())\n```\n\n\n```python\nInput bits are:\n [[1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.]]\nEncoded bits are:\n [[1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]]\n```\n\n\nOne of the fundamental paradigms of Sionna is batch-processing. Thus, the example above could be executed with for arbitrary batch-sizes to simulate `batch_size` codewords in parallel.\n\nHowever, Sionna can do more - it supports *N*-dimensional input tensors and, thereby, allows the processing of multiple samples of multiple users and several antennas in a single command line. Lets say we want to encoded `batch_size` codewords of length `n` for each of the `num_users` connected to each of the `num_basestations`. This means in total we transmit `batch_size` * `n` * `num_users` * `num_basestations` bits."
"```python\nBATCH_SIZE = 10 # samples per scenario\nnum_basestations = 4\nnum_users = 5 # users per basestation\nn = 1000 # codeword length per transmitted codeword\ncoderate = 0.5 # coderate\nk = int(coderate * n) # number of info bits per codeword\n# instantiate a new encoder for codewords of length n\nencoder = sn.fec.ldpc.LDPC5GEncoder(k, n)\n# the decoder must be linked to the encoder (to know the exact code parameters used for encoding)\ndecoder = sn.fec.ldpc.LDPC5GDecoder(encoder,\n                                    hard_out=True, # binary output or provide soft-estimates\n                                    return_infobits=True, # or also return (decoded) parity bits\n                                    num_iter=20, # number of decoding iterations\n                                    cn_type=\"boxplus-phi\") # also try \"minsum\" decoding\n# draw random bits to encode\nu = binary_source([BATCH_SIZE, num_basestations, num_users, k])\nprint(\"Shape of u: \", u.shape)\n# We can immediately encode u for all users, basetation and samples\n# This all happens with a single line of code\nc = encoder(u)\nprint(\"Shape of c: \", c.shape)\nprint(\"Total number of processed bits: \", np.prod(c.shape))\n```\n\n\n```python\nShape of u:  (10, 4, 5, 500)\nShape of c:  (10, 4, 5, 1000)\nTotal number of processed bits:  200000\n```\n\n\nThis works for arbitrary dimensions and allows a simple extension of the designed system to multi-user or multi-antenna scenarios.\n\nLet us now replace the LDPC code by a Polar code. The API remains similar.\n\n\n```python\nk = 64\nn = 128\nencoder = sn.fec.polar.Polar5GEncoder(k, n)\ndecoder = sn.fec.polar.Polar5GDecoder(encoder,\n                                      dec_type=\"SCL\") # you can also use \"SCL\"\n```\n\n\n*Advanced Remark:* The 5G Polar encoder/decoder class directly applies rate-matching and the additional CRC concatenation. This is all done internally and transparent to the user.\n\nIn case you want to access low-level features of the Polar codes, please use `sionna.fec.polar.PolarEncoder` and the desired decoder (`sionna.fec.polar.PolarSCDecoder`, `sionna.fec.polar.PolarSCLDecoder` or `sionna.fec.polar.PolarBPDecoder`).\n\nFurther details can be found in the tutorial notebook on [5G Channel Coding and Rate-Matching: Polar vs.LDPC Codes](https://nvlabs.github.io/sionna/examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html)."
"```python\nclass CodedSystemAWGN(Model): # Inherits from Keras Model\n    def __init__(self, num_bits_per_symbol, n, coderate):\n        super().__init__() # Must call the Keras model initializer\n        self.num_bits_per_symbol = num_bits_per_symbol\n        self.n = n\n        self.k = int(n*coderate)\n        self.coderate = coderate\n        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n        self.binary_source = sn.utils.BinarySource()\n        self.awgn_channel = sn.channel.AWGN()\n        self.encoder = sn.fec.ldpc.LDPC5GEncoder(self.k, self.n)\n        self.decoder = sn.fec.ldpc.LDPC5GDecoder(self.encoder, hard_out=True)\n    #@tf.function # activate graph execution to speed things up\n    def __call__(self, batch_size, ebno_db):\n        no = sn.utils.ebnodb2no(ebno_db, num_bits_per_symbol=self.num_bits_per_symbol, coderate=self.coderate)\n        bits = self.binary_source([batch_size, self.k])\n        codewords = self.encoder(bits)\n        x = self.mapper(codewords)\n        y = self.awgn_channel([x, no])\n        llr = self.demapper([y,no])\n        bits_hat = self.decoder(llr)\n        return bits, bits_hat\n```\n\n```python\nCODERATE = 0.5\nBATCH_SIZE = 2000\nmodel_coded_awgn = CodedSystemAWGN(num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n                                   n=2048,\n                                   coderate=CODERATE)\nber_plots.simulate(model_coded_awgn,\n                   ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 15),\n                   batch_size=BATCH_SIZE,\n                   num_target_block_errors=500,\n                   legend=\"Coded\",\n                   soft_estimates=False,\n                   max_mc_iter=15,\n                   show_fig=True,\n                   forward_keyboard_interrupt=False);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     -3.0 | 2.7896e-01 | 1.0000e+00 |      571312 |     2048000 |         2000 |        2000 |         1.8 |reached target block errors\n   -2.429 | 2.6327e-01 | 1.0000e+00 |      539174 |     2048000 |         2000 |        2000 |         1.7 |reached target block errors\n   -1.857 | 2.4783e-01 | 1.0000e+00 |      507546 |     2048000 |         2000 |        2000 |         1.7 |reached target block errors\n   -1.286 | 2.2687e-01 | 1.0000e+00 |      464636 |     2048000 |         2000 |        2000 |         1.7 |reached target block errors\n   -0.714 | 2.0312e-01 | 1.0000e+00 |      415988 |     2048000 |         2000 |        2000 |         1.7 |reached target block errors\n   -0.143 | 1.7154e-01 | 1.0000e+00 |      351316 |     2048000 |         2000 |        2000 |         1.7 |reached target block errors\n    0.429 | 1.1296e-01 | 9.9050e-01 |      231337 |     2048000 |         1981 |        2000 |         1.7 |reached target block errors\n      1.0 | 2.0114e-02 | 4.7150e-01 |       41193 |     2048000 |          943 |        2000 |         1.7 |reached target block errors\n    1.571 | 1.7617e-04 | 1.1633e-02 |        5412 |    30720000 |          349 |       30000 |        25.2 |reached max iter\n    2.143 | 0.0000e+00 | 0.0000e+00 |           0 |    30720000 |            0 |       30000 |        25.3 |reached max iter\nSimulation stopped as no error occurred @ EbNo = 2.1 dB.\n\n```"
"As can be seen, the `BerPlot` class uses multiple stopping conditions and stops the simulation after no error occured at a specifc SNR point.\n\n**Task**: Replace the coding scheme by a Polar encoder/decoder or a convolutional code with Viterbi decoding."
"## Eager vs Graph Mode\n\nSo far, we have executed the example in *eager* mode. This allows to run TensorFlow ops as if it was written NumPy and simplifies development and debugging.\n\nHowever, to unleash Sionnas full performance, we need to activate *graph* mode which can be enabled with the function decorator *@tf.function()*.\n\nWe refer to [TensorFlow Functions](https://www.tensorflow.org/guide/function) for further details.\n\n\n```python\n@tf.function() # enables graph-mode of the following function\ndef run_graph(batch_size, ebno_db):\n    # all code inside this function will be executed in graph mode, also calls of other functions\n    print(f\"Tracing run_graph for values batch_size={batch_size} and ebno_db={ebno_db}.\") # print whenever this function is traced\n    return model_coded_awgn(batch_size, ebno_db)\n```\n\n```python\nbatch_size = 10 # try also different batch sizes\nebno_db = 1.5\n# run twice - how does the output change?\nrun_graph(batch_size, ebno_db)\n```\n\n\n```python\nTracing run_graph for values batch_size=10 and ebno_db=1.5.\n```\n\n```python\n(<tf.Tensor: shape=(10, 1024), dtype=float32, numpy=\n array([[1., 1., 0., ..., 0., 1., 0.],\n        [1., 1., 1., ..., 0., 1., 1.],\n        [1., 1., 0., ..., 0., 1., 1.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 1., 0.],\n        [0., 1., 1., ..., 0., 1., 1.]], dtype=float32)>,\n <tf.Tensor: shape=(10, 1024), dtype=float32, numpy=\n array([[1., 1., 0., ..., 0., 1., 0.],\n        [1., 1., 1., ..., 0., 1., 1.],\n        [1., 1., 0., ..., 0., 1., 1.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 1., 0.],\n        [0., 1., 1., ..., 0., 1., 1.]], dtype=float32)>)\n```"
"In graph mode, Python code (i.e., *non-TensorFlow code*) is only executed whenever the function is *traced*. This happens whenever the input signature changes.\n\nAs can be seen above, the print statement was executed, i.e., the graph was traced again.\n\nTo avoid this re-tracing for different inputs, we now input tensors. You can see that the function is now traced once for input tensors of same dtype.\n\nSee [TensorFlow Rules of Tracing](https://www.tensorflow.org/guide/function#rules_of_tracing) for details.\n\n**Task:** change the code above such that tensors are used as input and execute the code with different input values. Understand when re-tracing happens.\n\n*Remark*: if the input to a function is a tensor its signature must change and not *just* its value. For example the input could have a different size or datatype. For efficient code execution, we usually want to avoid re-tracing of the code if not required.\n\n\n```python\n# You can print the cached signatures with\nprint(run_graph.pretty_printed_concrete_signatures())\n```\n\n\n```python\nrun_graph(batch_size=10, ebno_db=1.5)\n  Returns:\n    (<1>, <2>)\n      <1>: float32 Tensor, shape=(10, 1024)\n      <2>: float32 Tensor, shape=(10, 1024)\n```\n\n\nWe now compare the throughput of the different modes.\n\n\n```python\nrepetitions = 4 # average over multiple runs\nbatch_size = BATCH_SIZE # try also different batch sizes\nebno_db = 1.5\n# --- eager mode ---\nt_start = time.perf_counter()\nfor _ in range(repetitions):\n    bits, bits_hat = model_coded_awgn(tf.constant(batch_size, tf.int32),\n                                tf.constant(ebno_db, tf. float32))\nt_stop = time.perf_counter()\n# throughput in bit/s\nthroughput_eager = np.size(bits.numpy())*repetitions / (t_stop - t_start) / 1e6\nprint(f\"Throughput in Eager mode: {throughput_eager :.3f} Mbit/s\")\n# --- graph mode ---\n# run once to trace graph (ignored for throughput)\nrun_graph(tf.constant(batch_size, tf.int32),\n          tf.constant(ebno_db, tf. float32))\nt_start = time.perf_counter()\nfor _ in range(repetitions):\n    bits, bits_hat = run_graph(tf.constant(batch_size, tf.int32),\n                                tf.constant(ebno_db, tf. float32))\nt_stop = time.perf_counter()\n# throughput in bit/s\nthroughput_graph = np.size(bits.numpy())*repetitions / (t_stop - t_start) / 1e6\nprint(f\"Throughput in graph mode: {throughput_graph :.3f} Mbit/s\")\n\n```"
"```python\nThroughput in Eager mode: 1.212 Mbit/s\nTracing run_graph for values batch_size=Tensor(&#34;batch_size:0&#34;, shape=(), dtype=int32) and ebno_db=Tensor(&#34;ebno_db:0&#34;, shape=(), dtype=float32).\nThroughput in graph mode: 8.623 Mbit/s\n```\n\n\nLets run the same simulation as above in graph mode.\n\n\n```python\nber_plots.simulate(run_graph,\n                   ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 12),\n                   batch_size=BATCH_SIZE,\n                   num_target_block_errors=500,\n                   legend=\"Coded (Graph mode)\",\n                   soft_estimates=True,\n                   max_mc_iter=100,\n                   show_fig=True,\n                   forward_keyboard_interrupt=False);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     -3.0 | 2.7922e-01 | 1.0000e+00 |      571845 |     2048000 |         2000 |        2000 |         0.2 |reached target block errors\n   -2.273 | 2.5948e-01 | 1.0000e+00 |      531422 |     2048000 |         2000 |        2000 |         0.2 |reached target block errors\n   -1.545 | 2.3550e-01 | 1.0000e+00 |      482301 |     2048000 |         2000 |        2000 |         0.2 |reached target block errors\n   -0.818 | 2.0768e-01 | 1.0000e+00 |      425335 |     2048000 |         2000 |        2000 |         0.2 |reached target block errors\n   -0.091 | 1.6918e-01 | 1.0000e+00 |      346477 |     2048000 |         2000 |        2000 |         0.2 |reached target block errors\n    0.636 | 7.6115e-02 | 9.1650e-01 |      155883 |     2048000 |         1833 |        2000 |         0.2 |reached target block errors\n    1.364 | 1.7544e-03 | 7.2125e-02 |       14372 |     8192000 |          577 |        8000 |         1.0 |reached target block errors\n    2.091 | 7.8125e-08 | 2.0000e-05 |          16 |   204800000 |            4 |      200000 |        24.3 |reached max iter\n    2.818 | 0.0000e+00 | 0.0000e+00 |           0 |   204800000 |            0 |      200000 |        24.4 |reached max iter\nSimulation stopped as no error occurred @ EbNo = 2.8 dB.\n\n```"
"**Task:** TensorFlow allows to *compile* graphs with [XLA](https://www.tensorflow.org/xla). Try to further accelerate the code with XLA (`@tf.function(jit_compile=True)`).\n\n*Remark*: XLA is still an experimental feature and not all TensorFlow (and, thus, Sionna) functions support XLA.\n\n**Task 2:** Check the GPU load with `!nvidia-smi`. Find the best tradeoff between batch-size and throughput for your specific GPU architecture."
"## Exercise\n\nSimulate the coded bit error rate (BER) for a Polar coded and 64-QAM modulation. Assume a codeword length of n = 200 and coderate = 0.5.\n\n**Hint**: For Polar codes, successive cancellation list decoding (SCL) gives the best BER performance. However, successive cancellation (SC) decoding (without a list) is less complex.\n\n\n```python\nn = 200\ncoderate = 0.5\n# *You can implement your code here*\n\n```"

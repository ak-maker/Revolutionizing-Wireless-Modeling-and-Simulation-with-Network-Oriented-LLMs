"### BitErrorRate\n\n`class` `sionna.utils.``BitErrorRate`(*`name``=``'bit_error_rate'`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/metrics.html#BitErrorRate)\n\nComputes the average bit error rate (BER) between two binary tensors.\n\nThis class implements a Keras metric for the bit error rate\nbetween two tensors of bits.\nInput\n\n- **b** (*tf.float32*)  A tensor of arbitrary shape filled with ones and\nzeros.\n- **b_hat** (*tf.float32*)  A tensor of the same shape as `b` filled with\nones and zeros.\n\n\nOutput\n\n*tf.float32*  A scalar, the BER."
"### BitwiseMutualInformation\n\n`class` `sionna.utils.``BitwiseMutualInformation`(*`name``=``'bitwise_mutual_information'`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/metrics.html#BitwiseMutualInformation)\n\nComputes the bitwise mutual information between bits and LLRs.\n\nThis class implements a Keras metric for the bitwise mutual information\nbetween a tensor of bits and LLR (logits).\nInput\n\n- **bits** (*tf.float32*)  A tensor of arbitrary shape filled with ones and zeros.\n- **llr** (*tf.float32*)  A tensor of the same shape as `bits` containing logits.\n\n\nOutput\n\n*tf.float32*  A scalar, the bit-wise mutual information."
"### compute_ber\n\n`sionna.utils.``compute_ber`(*`b`*, *`b_hat`*)[`[source]`](../_modules/sionna/utils/metrics.html#compute_ber)\n\nComputes the bit error rate (BER) between two binary tensors.\nInput\n\n- **b** (*tf.float32*)  A tensor of arbitrary shape filled with ones and\nzeros.\n- **b_hat** (*tf.float32*)  A tensor of the same shape as `b` filled with\nones and zeros.\n\n\nOutput\n\n*tf.float64*  A scalar, the BER."
"### compute_bler\n\n`sionna.utils.``compute_bler`(*`b`*, *`b_hat`*)[`[source]`](../_modules/sionna/utils/metrics.html#compute_bler)\n\nComputes the block error rate (BLER) between two binary tensors.\n\nA block error happens if at least one element of `b` and `b_hat`\ndiffer in one block. The BLER is evaluated over the last dimension of\nthe input, i. e., all elements of the last dimension are considered to\ndefine a block.\n\nThis is also sometimes referred to as <cite>word error rate</cite> or <cite>frame error\nrate</cite>.\nInput\n\n- **b** (*tf.float32*)  A tensor of arbitrary shape filled with ones and\nzeros.\n- **b_hat** (*tf.float32*)  A tensor of the same shape as `b` filled with\nones and zeros.\n\n\nOutput\n\n*tf.float64*  A scalar, the BLER."
"### compute_ser\n\n`sionna.utils.``compute_ser`(*`s`*, *`s_hat`*)[`[source]`](../_modules/sionna/utils/metrics.html#compute_ser)\n\nComputes the symbol error rate (SER) between two integer tensors.\nInput\n\n- **s** (*tf.int*)  A tensor of arbitrary shape filled with integers indicating\nthe symbol indices.\n- **s_hat** (*tf.int*)  A tensor of the same shape as `s` filled with integers indicating\nthe estimated symbol indices.\n\n\nOutput\n\n*tf.float64*  A scalar, the SER."
"### count_errors\n\n`sionna.utils.``count_errors`(*`b`*, *`b_hat`*)[`[source]`](../_modules/sionna/utils/metrics.html#count_errors)\n\nCounts the number of bit errors between two binary tensors.\nInput\n\n- **b** (*tf.float32*)  A tensor of arbitrary shape filled with ones and\nzeros.\n- **b_hat** (*tf.float32*)  A tensor of the same shape as `b` filled with\nones and zeros.\n\n\nOutput\n\n*tf.int64*  A scalar, the number of bit errors."
"### count_block_errors\n\n`sionna.utils.``count_block_errors`(*`b`*, *`b_hat`*)[`[source]`](../_modules/sionna/utils/metrics.html#count_block_errors)\n\nCounts the number of block errors between two binary tensors.\n\nA block error happens if at least one element of `b` and `b_hat`\ndiffer in one block. The BLER is evaluated over the last dimension of\nthe input, i. e., all elements of the last dimension are considered to\ndefine a block.\n\nThis is also sometimes referred to as <cite>word error rate</cite> or <cite>frame error\nrate</cite>.\nInput\n\n- **b** (*tf.float32*)  A tensor of arbitrary shape filled with ones and\nzeros.\n- **b_hat** (*tf.float32*)  A tensor of the same shape as `b` filled with\nones and zeros.\n\n\nOutput\n\n*tf.int64*  A scalar, the number of block errors."
"### expand_to_rank\n\n`sionna.utils.``expand_to_rank`(*`tensor`*, *`target_rank`*, *`axis``=``-` `1`*)[`[source]`](../_modules/sionna/utils/tensors.html#expand_to_rank)\n\nInserts as many axes to a tensor as needed to achieve a desired rank.\n\nThis operation inserts additional dimensions to a `tensor` starting at\n`axis`, so that so that the rank of the resulting tensor has rank\n`target_rank`. The dimension index follows Python indexing rules, i.e.,\nzero-based, where a negative index is counted backward from the end.\nParameters\n\n- **tensor**  A tensor.\n- **target_rank** (*int*)  The rank of the output tensor.\nIf `target_rank` is smaller than the rank of `tensor`,\nthe function does nothing.\n- **axis** (*int*)  The dimension index at which to expand the\nshape of `tensor`. Given a `tensor` of <cite>D</cite> dimensions,\n`axis` must be within the range <cite>[-(D+1), D]</cite> (inclusive).\n\n\nReturns\n\nA tensor with the same data as `tensor`, with\n`target_rank`- rank(`tensor`) additional dimensions inserted at the\nindex specified by `axis`.\nIf `target_rank` <= rank(`tensor`), `tensor` is returned."
"### flatten_dims\n\n`sionna.utils.``flatten_dims`(*`tensor`*, *`num_dims`*, *`axis`*)[`[source]`](../_modules/sionna/utils/tensors.html#flatten_dims)\n\nFlattens a specified set of dimensions of a tensor.\n\nThis operation flattens `num_dims` dimensions of a `tensor`\nstarting at a given `axis`.\nParameters\n\n- **tensor**  A tensor.\n- **num_dims** (*int*)  The number of dimensions\nto combine. Must be larger than two and less or equal than the\nrank of `tensor`.\n- **axis** (*int*)  The index of the dimension from which to start.\n\n\nReturns\n\nA tensor of the same type as `tensor` with `num_dims`-1 lesser\ndimensions, but the same number of elements."
"### flatten_last_dims\n\n`sionna.utils.``flatten_last_dims`(*`tensor`*, *`num_dims``=``2`*)[`[source]`](../_modules/sionna/utils/tensors.html#flatten_last_dims)\n\nFlattens the last <cite>n</cite> dimensions of a tensor.\n\nThis operation flattens the last `num_dims` dimensions of a `tensor`.\nIt is a simplified version of the function `flatten_dims`.\nParameters\n\n- **tensor**  A tensor.\n- **num_dims** (*int*)  The number of dimensions\nto combine. Must be greater than or equal to two and less or equal\nthan the rank of `tensor`.\n\n\nReturns\n\nA tensor of the same type as `tensor` with `num_dims`-1 lesser\ndimensions, but the same number of elements."
"### insert_dims\n\n`sionna.utils.``insert_dims`(*`tensor`*, *`num_dims`*, *`axis``=``-` `1`*)[`[source]`](../_modules/sionna/utils/tensors.html#insert_dims)\n\nAdds multiple length-one dimensions to a tensor.\n\nThis operation is an extension to TensorFlow`s `expand_dims` function.\nIt inserts `num_dims` dimensions of length one starting from the\ndimension `axis` of a `tensor`. The dimension\nindex follows Python indexing rules, i.e., zero-based, where a negative\nindex is counted backward from the end.\nParameters\n\n- **tensor**  A tensor.\n- **num_dims** (*int*)  The number of dimensions to add.\n- **axis**  The dimension index at which to expand the\nshape of `tensor`. Given a `tensor` of <cite>D</cite> dimensions,\n`axis` must be within the range <cite>[-(D+1), D]</cite> (inclusive).\n\n\nReturns\n\nA tensor with the same data as `tensor`, with `num_dims` additional\ndimensions inserted at the index specified by `axis`."
"### split_dims\n\n`sionna.utils.``split_dim`(*`tensor`*, *`shape`*, *`axis`*)[`[source]`](../_modules/sionna/utils/tensors.html#split_dim)\n\nReshapes a dimension of a tensor into multiple dimensions.\n\nThis operation splits the dimension `axis` of a `tensor` into\nmultiple dimensions according to `shape`.\nParameters\n\n- **tensor**  A tensor.\n- **shape** (*list** or **TensorShape*)  The shape to which the dimension should\nbe reshaped.\n- **axis** (*int*)  The index of the axis to be reshaped.\n\n\nReturns\n\nA tensor of the same type as `tensor` with len(`shape`)-1\nadditional dimensions, but the same number of elements."
"### matrix_sqrt\n\n`sionna.utils.``matrix_sqrt`(*`tensor`*)[`[source]`](../_modules/sionna/utils/tensors.html#matrix_sqrt)\n\nComputes the square root of a matrix.\n\nGiven a batch of Hermitian positive semi-definite matrices\n$\\mathbf{A}$, returns matrices $\\mathbf{B}$,\nsuch that $\\mathbf{B}\\mathbf{B}^H = \\mathbf{A}$.\n\nThe two inner dimensions are assumed to correspond to the matrix rows\nand columns, respectively.\nParameters\n\n**tensor** (*[**...**, **M**, **M**]*)  A tensor of rank greater than or equal\nto two.\n\nReturns\n\nA tensor of the same shape and type as `tensor` containing\nthe matrix square root of its last two dimensions.\n\n\n**Note**\n\nIf you want to use this function in Graph mode with XLA, i.e., within\na function that is decorated with `@tf.function(jit_compile=True)`,\nyou must set `sionna.config.xla_compat=true`.\nSee `xla_compat`."
"### matrix_sqrt_inv\n\n`sionna.utils.``matrix_sqrt_inv`(*`tensor`*)[`[source]`](../_modules/sionna/utils/tensors.html#matrix_sqrt_inv)\n\nComputes the inverse square root of a Hermitian matrix.\n\nGiven a batch of Hermitian positive definite matrices\n$\\mathbf{A}$, with square root matrices $\\mathbf{B}$,\nsuch that $\\mathbf{B}\\mathbf{B}^H = \\mathbf{A}$, the function\nreturns $\\mathbf{B}^{-1}$, such that\n$\\mathbf{B}^{-1}\\mathbf{B}=\\mathbf{I}$.\n\nThe two inner dimensions are assumed to correspond to the matrix rows\nand columns, respectively.\nParameters\n\n**tensor** (*[**...**, **M**, **M**]*)  A tensor of rank greater than or equal\nto two.\n\nReturns\n\nA tensor of the same shape and type as `tensor` containing\nthe inverse matrix square root of its last two dimensions.\n\n\n**Note**\n\nIf you want to use this function in Graph mode with XLA, i.e., within\na function that is decorated with `@tf.function(jit_compile=True)`,\nyou must set `sionna.Config.xla_compat=true`.\nSee [`xla_compat`](config.html#sionna.Config.xla_compat)."
"### matrix_inv\n\n`sionna.utils.``matrix_inv`(*`tensor`*)[`[source]`](../_modules/sionna/utils/tensors.html#matrix_inv)\n\nComputes the inverse of a Hermitian matrix.\n\nGiven a batch of Hermitian positive definite matrices\n$\\mathbf{A}$, the function\nreturns $\\mathbf{A}^{-1}$, such that\n$\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}$.\n\nThe two inner dimensions are assumed to correspond to the matrix rows\nand columns, respectively.\nParameters\n\n**tensor** (*[**...**, **M**, **M**]*)  A tensor of rank greater than or equal\nto two.\n\nReturns\n\nA tensor of the same shape and type as `tensor`, containing\nthe inverse of its last two dimensions.\n\n\n**Note**\n\nIf you want to use this function in Graph mode with XLA, i.e., within\na function that is decorated with `@tf.function(jit_compile=True)`,\nyou must set `sionna.Config.xla_compat=true`.\nSee [`xla_compat`](config.html#sionna.Config.xla_compat)."
"### matrix_pinv\n\n`sionna.utils.``matrix_pinv`(*`tensor`*)[`[source]`](../_modules/sionna/utils/tensors.html#matrix_pinv)\n\nComputes the MoorePenrose (or pseudo) inverse of a matrix.\n\nGiven a batch of $M \\times K$ matrices $\\mathbf{A}$ with rank\n$K$ (i.e., linearly independent columns), the function returns\n$\\mathbf{A}^+$, such that\n$\\mathbf{A}^{+}\\mathbf{A}=\\mathbf{I}_K$.\n\nThe two inner dimensions are assumed to correspond to the matrix rows\nand columns, respectively.\nParameters\n\n**tensor** (*[**...**, **M**, **K**]*)  A tensor of rank greater than or equal\nto two.\n\nReturns\n\nA tensor of shape ([, K,K]) of the same type as `tensor`,\ncontaining the pseudo inverse of its last two dimensions.\n\n\n**Note**\n\nIf you want to use this function in Graph mode with XLA, i.e., within\na function that is decorated with `@tf.function(jit_compile=True)`,\nyou must set `sionna.config.xla_compat=true`.\nSee `xla_compat`."
"### BinarySource\n\n`class` `sionna.utils.``BinarySource`(*`dtype``=``tf.float32`*, *`seed``=``None`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/misc.html#BinarySource)\n\nLayer generating random binary tensors.\nParameters\n\n- **dtype** (*tf.DType*)  Defines the output datatype of the layer.\nDefaults to <cite>tf.float32</cite>.\n- **seed** (*int** or **None*)  Set the seed for the random generator used to generate the bits.\nSet to <cite>None</cite> for random initialization of the RNG.\n\n\nInput\n\n**shape** (*1D tensor/array/list, int*)  The desired shape of the output tensor.\n\nOutput\n\n`shape`, `dtype`  Tensor filled with random binary values."
"### SymbolSource\n\n`class` `sionna.utils.``SymbolSource`(*`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`return_indices``=``False`*, *`return_bits``=``False`*, *`seed``=``None`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/misc.html#SymbolSource)\n\nLayer generating a tensor of arbitrary shape filled with random constellation symbols.\nOptionally, the symbol indices and/or binary representations of the\nconstellation symbols can be returned.\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](mapping.html#sionna.mapping.Constellation) or\n<cite>None</cite>. In the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **return_indices** (*bool*)  If enabled, the function also returns the symbol indices.\nDefaults to <cite>False</cite>.\n- **return_bits** (*bool*)  If enabled, the function also returns the binary symbol\nrepresentations (i.e., bit labels).\nDefaults to <cite>False</cite>.\n- **seed** (*int** or **None*)  The seed for the random generator.\n<cite>None</cite> leads to a random initialization of the RNG.\nDefaults to <cite>None</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**]**, **tf.DType*)  The output dtype. Defaults to tf.complex64.\n\n\nInput\n\n**shape** (*1D tensor/array/list, int*)  The desired shape of the output tensor.\n\nOutput\n\n- **symbols** (`shape`, `dtype`)  Tensor filled with random symbols of the chosen `constellation_type`.\n- **symbol_indices** (`shape`, tf.int32)  Tensor filled with the symbol indices.\nOnly returned if `return_indices` is <cite>True</cite>.\n- **bits** ([`shape`, `num_bits_per_symbol`], tf.int32)  Tensor filled with the binary symbol representations (i.e., bit labels).\nOnly returned if `return_bits` is <cite>True</cite>."
"### QAMSource\n\n`class` `sionna.utils.``QAMSource`(*`num_bits_per_symbol``=``None`*, *`return_indices``=``False`*, *`return_bits``=``False`*, *`seed``=``None`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/misc.html#QAMSource)\n\nLayer generating a tensor of arbitrary shape filled with random QAM symbols.\nOptionally, the symbol indices and/or binary representations of the\nconstellation symbols can be returned.\nParameters\n\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **return_indices** (*bool*)  If enabled, the function also returns the symbol indices.\nDefaults to <cite>False</cite>.\n- **return_bits** (*bool*)  If enabled, the function also returns the binary symbol\nrepresentations (i.e., bit labels).\nDefaults to <cite>False</cite>.\n- **seed** (*int** or **None*)  The seed for the random generator.\n<cite>None</cite> leads to a random initialization of the RNG.\nDefaults to <cite>None</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**]**, **tf.DType*)  The output dtype. Defaults to tf.complex64.\n\n\nInput\n\n**shape** (*1D tensor/array/list, int*)  The desired shape of the output tensor.\n\nOutput\n\n- **symbols** (`shape`, `dtype`)  Tensor filled with random QAM symbols.\n- **symbol_indices** (`shape`, tf.int32)  Tensor filled with the symbol indices.\nOnly returned if `return_indices` is <cite>True</cite>.\n- **bits** ([`shape`, `num_bits_per_symbol`], tf.int32)  Tensor filled with the binary symbol representations (i.e., bit labels).\nOnly returned if `return_bits` is <cite>True</cite>."
"### PAMSource\n\n`class` `sionna.utils.``PAMSource`(*`num_bits_per_symbol``=``None`*, *`return_indices``=``False`*, *`return_bits``=``False`*, *`seed``=``None`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/utils/misc.html#PAMSource)\n\nLayer generating a tensor of arbitrary shape filled with random PAM symbols.\nOptionally, the symbol indices and/or binary representations of the\nconstellation symbols can be returned.\nParameters\n\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 1 for BPSK.\n- **return_indices** (*bool*)  If enabled, the function also returns the symbol indices.\nDefaults to <cite>False</cite>.\n- **return_bits** (*bool*)  If enabled, the function also returns the binary symbol\nrepresentations (i.e., bit labels).\nDefaults to <cite>False</cite>.\n- **seed** (*int** or **None*)  The seed for the random generator.\n<cite>None</cite> leads to a random initialization of the RNG.\nDefaults to <cite>None</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**]**, **tf.DType*)  The output dtype. Defaults to tf.complex64.\n\n\nInput\n\n**shape** (*1D tensor/array/list, int*)  The desired shape of the output tensor.\n\nOutput\n\n- **symbols** (`shape`, `dtype`)  Tensor filled with random PAM symbols.\n- **symbol_indices** (`shape`, tf.int32)  Tensor filled with the symbol indices.\nOnly returned if `return_indices` is <cite>True</cite>.\n- **bits** ([`shape`, `num_bits_per_symbol`], tf.int32)  Tensor filled with the binary symbol representations (i.e., bit labels).\nOnly returned if `return_bits` is <cite>True</cite>."
"### PlotBER\n\n`class` `sionna.utils.plotting.``PlotBER`(*`title``=``'Bit/Block` `Error` `Rate'`*)[`[source]`](../_modules/sionna/utils/plotting.html#PlotBER)\n\nProvides a plotting object to simulate and store BER/BLER curves.\nParameters\n\n**title** (*str*)  A string defining the title of the figure. Defaults to\n<cite>Bit/Block Error Rate</cite>.\n\nInput\n\n- **snr_db** (*float*)  Python array (or list of Python arrays) of additional SNR values to be\nplotted.\n- **ber** (*float*)  Python array (or list of Python arrays) of additional BERs\ncorresponding to `snr_db`.\n- **legend** (*str*)  String (or list of strings) of legends entries.\n- **is_bler** (*bool*)  A boolean (or list of booleans) defaults to False.\nIf True, `ber` will be interpreted as BLER.\n- **show_ber** (*bool*)  A boolean defaults to True. If True, BER curves will be plotted.\n- **show_bler** (*bool*)  A boolean defaults to True. If True, BLER curves will be plotted.\n- **xlim** (*tuple of floats*)  Defaults to None. A tuple of two floats defining x-axis limits.\n- **ylim** (*tuple of floats*)  Defaults to None. A tuple of two floats defining y-axis limits.\n- **save_fig** (*bool*)  A boolean defaults to False. If True, the figure\nis saved as file.\n- **path** (*str*)  A string defining where to save the figure (if `save_fig`\nis True).\n\n\n`add`(*`ebno_db`*, *`ber`*, *`is_bler``=``False`*, *`legend``=``''`*)[`[source]`](../_modules/sionna/utils/plotting.html#PlotBER.add)\n\nAdd static reference curves.\nInput\n\n- **ebno_db** (*float*)  Python array or list of floats defining the SNR points.\n- **ber** (*float*)  Python array or list of floats defining the BER corresponding\nto each SNR point.\n- **is_bler** (*bool*)  A boolean defaults to False. If True, `ber` is interpreted as\nBLER.\n- **legend** (*str*)  A string defining the text of the legend entry.\n\n\n`property` `ber`\n\nList containing all stored BER curves.\n\n\n`property` `is_bler`\n\nList of booleans indicating if ber shall be interpreted as BLER.\n\n\n`property` `legend`\n\nList containing all stored legend entries curves.\n\n\n`remove`(*`idx``=``-` `1`*)[`[source]`](../_modules/sionna/utils/plotting.html#PlotBER.remove)\n\nRemove curve with index `idx`.\nInput\n\n**idx** (*int*)  An integer defining the index of the dataset that should\nbe removed. Negative indexing is possible.\n\n\n`reset`()[`[source]`](../_modules/sionna/utils/plotting.html#PlotBER.reset)\n\nRemove all internal data.\n\n\n`simulate`(*`mc_fun`*, *`ebno_dbs`*, *`batch_size`*, *`max_mc_iter`*, *`legend``=``''`*, *`add_ber``=``True`*, *`add_bler``=``False`*, *`soft_estimates``=``False`*, *`num_target_bit_errors``=``None`*, *`num_target_block_errors``=``None`*, *`target_ber``=``None`*, *`target_bler``=``None`*, *`early_stop``=``True`*, *`graph_mode``=``None`*, *`distribute``=``None`*, *`add_results``=``True`*, *`forward_keyboard_interrupt``=``True`*, *`show_fig``=``True`*, *`verbose``=``True`*)[`[source]`](../_modules/sionna/utils/plotting.html#PlotBER.simulate)\n\nSimulate BER/BLER curves for given Keras model and saves the results.\n\nInternally calls [`sionna.utils.sim_ber`](https://nvlabs.github.io/sionna/api/utils.html#sionna.utils.sim_ber).\nInput\n\n- **mc_fun**  Callable that yields the transmitted bits <cite>b</cite> and the\nreceivers estimate <cite>b_hat</cite> for a given `batch_size` and\n`ebno_db`. If `soft_estimates` is True, b_hat is interpreted as\nlogit.\n- **ebno_dbs** (*ndarray of floats*)  SNR points to be evaluated.\n- **batch_size** (*tf.int32*)  Batch-size for evaluation.\n- **max_mc_iter** (*int*)  Max. number of Monte-Carlo iterations per SNR point.\n- **legend** (*str*)  Name to appear in legend.\n- **add_ber** (*bool*)  Defaults to True. Indicate if BER should be added to plot.\n- **add_bler** (*bool*)  Defaults to False. Indicate if BLER should be added\nto plot.\n- **soft_estimates** (*bool*)  A boolean, defaults to False. If True, `b_hat`\nis interpreted as logit and additional hard-decision is applied\ninternally.\n- **num_target_bit_errors** (*int*)  Target number of bit errors per SNR point until the simulation\nstops.\n- **num_target_block_errors** (*int*)  Target number of block errors per SNR point until the simulation\nstops.\n- **target_ber** (*tf.float32*)  Defaults to <cite>None</cite>. The simulation stops after the first SNR point\nwhich achieves a lower bit error rate as specified by\n`target_ber`. This requires `early_stop` to be <cite>True</cite>.\n- **target_bler** (*tf.float32*)  Defaults to <cite>None</cite>. The simulation stops after the first SNR point\nwhich achieves a lower block error rate as specified by\n`target_bler`.  This requires `early_stop` to be <cite>True</cite>.\n- **early_stop** (*bool*)  A boolean defaults to True. If True, the simulation stops after the\nfirst error-free SNR point (i.e., no error occurred after\n`max_mc_iter` Monte-Carlo iterations).\n- **graph_mode** (*One of [graph, xla], str*)  A string describing the execution mode of `mc_fun`.\nDefaults to <cite>None</cite>. In this case, `mc_fun` is executed as is.\n- **distribute** (<cite>None</cite> (default) | all | list of indices | <cite>tf.distribute.strategy</cite>)  Distributes simulation on multiple parallel devices. If <cite>None</cite>,\nmulti-device simulations are deactivated. If all, the workload\nwill be automatically distributed across all available GPUs via the\n<cite>tf.distribute.MirroredStrategy</cite>.\nIf an explicit list of indices is provided, only the GPUs with the\ngiven indices will be used. Alternatively, a custom\n<cite>tf.distribute.strategy</cite> can be provided. Note that the same\n<cite>batch_size</cite> will be used for all GPUs in parallel, but the number\nof Monte-Carlo iterations `max_mc_iter` will be scaled by the\nnumber of devices such that the same number of total samples is\nsimulated. However, all stopping conditions are still in-place\nwhich can cause slight differences in the total number of simulated\nsamples.\n- **add_results** (*bool*)  Defaults to True. If True, the simulation results will be appended\nto the internal list of results.\n- **show_fig** (*bool*)  Defaults to True. If True, a BER figure will be plotted.\n- **verbose** (*bool*)  A boolean defaults to True. If True, the current progress will be\nprinted.\n- **forward_keyboard_interrupt** (*bool*)  A boolean defaults to True. If False, <cite>KeyboardInterrupts</cite> will be\ncatched internally and not forwarded (e.g., will not stop outer\nloops). If False, the simulation ends and returns the intermediate\nsimulation results.\n\n\nOutput\n\n- **(ber, bler)**  Tuple:\n- **ber** (*float*)  The simulated bit-error rate.\n- **bler** (*float*)  The simulated block-error rate.\n\n\n`property` `snr`\n\nList containing all stored SNR curves.\n\n\n`property` `title`\n\nTitle of the plot."
"### sim_ber\n\n`sionna.utils.``sim_ber`(*`mc_fun`*, *`ebno_dbs`*, *`batch_size`*, *`max_mc_iter`*, *`soft_estimates``=``False`*, *`num_target_bit_errors``=``None`*, *`num_target_block_errors``=``None`*, *`target_ber``=``None`*, *`target_bler``=``None`*, *`early_stop``=``True`*, *`graph_mode``=``None`*, *`distribute``=``None`*, *`verbose``=``True`*, *`forward_keyboard_interrupt``=``True`*, *`callback``=``None`*, *`dtype``=``tf.complex64`*)[`[source]`](../_modules/sionna/utils/misc.html#sim_ber)\n\nSimulates until target number of errors is reached and returns BER/BLER.\n\nThe simulation continues with the next SNR point if either\n`num_target_bit_errors` bit errors or `num_target_block_errors` block\nerrors is achieved. Further, it continues with the next SNR point after\n`max_mc_iter` batches of size `batch_size` have been simulated.\nEarly stopping allows to stop the simulation after the first error-free SNR\npoint or after reaching a certain `target_ber` or `target_bler`.\nInput\n\n- **mc_fun** (*callable*)  Callable that yields the transmitted bits <cite>b</cite> and the\nreceivers estimate <cite>b_hat</cite> for a given `batch_size` and\n`ebno_db`. If `soft_estimates` is True, <cite>b_hat</cite> is interpreted as\nlogit.\n- **ebno_dbs** (*tf.float32*)  A tensor containing SNR points to be evaluated.\n- **batch_size** (*tf.int32*)  Batch-size for evaluation.\n- **max_mc_iter** (*tf.int32*)  Maximum number of Monte-Carlo iterations per SNR point.\n- **soft_estimates** (*bool*)  A boolean, defaults to <cite>False</cite>. If <cite>True</cite>, <cite>b_hat</cite>\nis interpreted as logit and an additional hard-decision is applied\ninternally.\n- **num_target_bit_errors** (*tf.int32*)  Defaults to <cite>None</cite>. Target number of bit errors per SNR point until\nthe simulation continues to next SNR point.\n- **num_target_block_errors** (*tf.int32*)  Defaults to <cite>None</cite>. Target number of block errors per SNR point\nuntil the simulation continues\n- **target_ber** (*tf.float32*)  Defaults to <cite>None</cite>. The simulation stops after the first SNR point\nwhich achieves a lower bit error rate as specified by `target_ber`.\nThis requires `early_stop` to be <cite>True</cite>.\n- **target_bler** (*tf.float32*)  Defaults to <cite>None</cite>. The simulation stops after the first SNR point\nwhich achieves a lower block error rate as specified by `target_bler`.\nThis requires `early_stop` to be <cite>True</cite>.\n- **early_stop** (*bool*)  A boolean defaults to <cite>True</cite>. If <cite>True</cite>, the simulation stops after the\nfirst error-free SNR point (i.e., no error occurred after\n`max_mc_iter` Monte-Carlo iterations).\n- **graph_mode** (*One of [graph, xla], str*)  A string describing the execution mode of `mc_fun`.\nDefaults to <cite>None</cite>. In this case, `mc_fun` is executed as is.\n- **distribute** (<cite>None</cite> (default) | all | list of indices | <cite>tf.distribute.strategy</cite>)  Distributes simulation on multiple parallel devices. If <cite>None</cite>,\nmulti-device simulations are deactivated. If all, the workload will\nbe automatically distributed across all available GPUs via the\n<cite>tf.distribute.MirroredStrategy</cite>.\nIf an explicit list of indices is provided, only the GPUs with the given\nindices will be used. Alternatively, a custom <cite>tf.distribute.strategy</cite>\ncan be provided. Note that the same <cite>batch_size</cite> will be\nused for all GPUs in parallel, but the number of Monte-Carlo iterations\n`max_mc_iter` will be scaled by the number of devices such that the\nsame number of total samples is simulated. However, all stopping\nconditions are still in-place which can cause slight differences in the\ntotal number of simulated samples.\n- **verbose** (*bool*)  A boolean defaults to <cite>True</cite>. If <cite>True</cite>, the current progress will be\nprinted.\n- **forward_keyboard_interrupt** (*bool*)  A boolean defaults to <cite>True</cite>. If <cite>False</cite>, KeyboardInterrupts will be\ncatched internally and not forwarded (e.g., will not stop outer loops).\nIf <cite>False</cite>, the simulation ends and returns the intermediate simulation\nresults.\n- **callback** (<cite>None</cite> (default) | callable)  If specified, `callback` will be called after each Monte-Carlo step.\nCan be used for logging or advanced early stopping. Input signature of\n`callback` must match <cite>callback(mc_iter, snr_idx, ebno_dbs,\nbit_errors, block_errors, nb_bits, nb_blocks)</cite> where `mc_iter`\ndenotes the number of processed batches for the current SNR point,\n`snr_idx` is the index of the current SNR point, `ebno_dbs` is the\nvector of all SNR points to be evaluated, `bit_errors` the vector of\nnumber of bit errors for each SNR point, `block_errors` the vector of\nnumber of block errors, `nb_bits` the vector of number of simulated\nbits, `nb_blocks` the vector of number of simulated blocks,\nrespectively. If `callable` returns <cite>sim_ber.CALLBACK_NEXT_SNR</cite>, early\nstopping is detected and the simulation will continue with the\nnext SNR point. If `callable` returns\n<cite>sim_ber.CALLBACK_STOP</cite>, the simulation is stopped\nimmediately. For <cite>sim_ber.CALLBACK_CONTINUE</cite> continues with\nthe simulation.\n- **dtype** (*tf.complex64*)  Datatype of the callable `mc_fun` to be used as input/output.\n\n\nOutput\n\n- **(ber, bler)**  Tuple:\n- **ber** (*tf.float32*)  The bit-error rate.\n- **bler** (*tf.float32*)  The block-error rate.\n\n\nRaises\n\n- **AssertionError**  If `soft_estimates` is not bool.\n- **AssertionError**  If `dtype` is not <cite>tf.complex</cite>.\n\n\n**Note**\n\nThis function is implemented based on tensors to allow\nfull compatibility with tf.function(). However, to run simulations\nin graph mode, the provided `mc_fun` must use the <cite>@tf.function()</cite>\ndecorator."
"### ebnodb2no\n\n`sionna.utils.``ebnodb2no`(*`ebno_db`*, *`num_bits_per_symbol`*, *`coderate`*, *`resource_grid``=``None`*)[`[source]`](../_modules/sionna/utils/misc.html#ebnodb2no)\n\nCompute the noise variance <cite>No</cite> for a given <cite>Eb/No</cite> in dB.\n\nThe function takes into account the number of coded bits per constellation\nsymbol, the coderate, as well as possible additional overheads related to\nOFDM transmissions, such as the cyclic prefix and pilots.\n\nThe value of <cite>No</cite> is computed according to the following expression\n\n$$\nN_o = \\left(\\frac{E_b}{N_o} \\frac{r M}{E_s}\\right)^{-1}\n$$\n\nwhere $2^M$ is the constellation size, i.e., $M$ is the\naverage number of coded bits per constellation symbol,\n$E_s=1$ is the average energy per constellation per symbol,\n$r\\in(0,1]$ is the coderate,\n$E_b$ is the energy per information bit,\nand $N_o$ is the noise power spectral density.\nFor OFDM transmissions, $E_s$ is scaled\naccording to the ratio between the total number of resource elements in\na resource grid with non-zero energy and the number\nof resource elements used for data transmission. Also the additionally\ntransmitted energy during the cyclic prefix is taken into account, as\nwell as the number of transmitted streams per transmitter.\nInput\n\n- **ebno_db** (*float*)  The <cite>Eb/No</cite> value in dB.\n- **num_bits_per_symbol** (*int*)  The number of bits per symbol.\n- **coderate** (*float*)  The coderate used.\n- **resource_grid** (*ResourceGrid*)  An (optional) instance of [`ResourceGrid`](ofdm.html#sionna.ofdm.ResourceGrid)\nfor OFDM transmissions.\n\n\nOutput\n\n*float*  The value of $N_o$ in linear scale."
"### hard_decisions\n\n`sionna.utils.``hard_decisions`(*`llr`*)[`[source]`](../_modules/sionna/utils/misc.html#hard_decisions)\n\nTransforms LLRs into hard decisions.\n\nPositive values are mapped to $1$.\nNonpositive values are mapped to $0$.\nInput\n\n**llr** (*any non-complex tf.DType*)  Tensor of LLRs.\n\nOutput\n\nSame shape and dtype as `llr`  The hard decisions."
"### plot_ber\n\n`sionna.utils.plotting.``plot_ber`(*`snr_db`*, *`ber`*, *`legend``=``''`*, *`ylabel``=``'BER'`*, *`title``=``'Bit` `Error` `Rate'`*, *`ebno``=``True`*, *`is_bler``=``None`*, *`xlim``=``None`*, *`ylim``=``None`*, *`save_fig``=``False`*, *`path``=``''`*)[`[source]`](../_modules/sionna/utils/plotting.html#plot_ber)\n\nPlot error-rates.\nInput\n\n- **snr_db** (*ndarray*)  Array of floats defining the simulated SNR points.\nCan be also a list of multiple arrays.\n- **ber** (*ndarray*)  Array of floats defining the BER/BLER per SNR point.\nCan be also a list of multiple arrays.\n- **legend** (*str*)  Defaults to . Defining the legend entries. Can be\neither a string or a list of strings.\n- **ylabel** (*str*)  Defaults to BER. Defining the y-label.\n- **title** (*str*)  Defaults to Bit Error Rate. Defining the title of the figure.\n- **ebno** (*bool*)  Defaults to True. If True, the x-label is set to\nEbNo [dB] instead of EsNo [dB].\n- **is_bler** (*bool*)  Defaults to False. If True, the corresponding curve is dashed.\n- **xlim** (*tuple of floats*)  Defaults to None. A tuple of two floats defining x-axis limits.\n- **ylim** (*tuple of floats*)  Defaults to None. A tuple of two floats defining y-axis limits.\n- **save_fig** (*bool*)  Defaults to False. If True, the figure is saved as <cite>.png</cite>.\n- **path** (*str*)  Defaults to . Defining the path to save the figure\n(iff `save_fig` is True).\n\n\nOutput\n\n- **(fig, ax)**  Tuple:\n- **fig** (*matplotlib.figure.Figure*)  A matplotlib figure handle.\n- **ax** (*matplotlib.axes.Axes*)  A matplotlib axes object."
"### complex_normal\n\n`sionna.utils.``complex_normal`(*`shape`*, *`var``=``1.0`*, *`dtype``=``tf.complex64`*)[`[source]`](../_modules/sionna/utils/misc.html#complex_normal)\n\nGenerates a tensor of complex normal random variables.\nInput\n\n- **shape** (*tf.shape, or list*)  The desired shape.\n- **var** (*float*)  The total variance., i.e., each complex dimension has\nvariance `var/2`.\n- **dtype** (*tf.complex*)  The desired dtype. Defaults to <cite>tf.complex64</cite>.\n\n\nOutput\n\n`shape`, `dtype`  Tensor of complex normal random variables."
"### log2\n\n`sionna.utils.``log2`(*`x`*)[`[source]`](../_modules/sionna/utils/misc.html#log2)\n\nTensorFlow implementation of NumPys <cite>log2</cite> function.\n\nSimple extension to <cite>tf.experimental.numpy.log2</cite>\nwhich casts the result to the <cite>dtype</cite> of the input.\nFor more details see the [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/log2) and [NumPy](https://numpy.org/doc/1.16/reference/generated/numpy.log2.html) documentation."
"### log10\n\n`sionna.utils.``log10`(*`x`*)[`[source]`](../_modules/sionna/utils/misc.html#log10)\n\nTensorFlow implementation of NumPys <cite>log10</cite> function.\n\nSimple extension to <cite>tf.experimental.numpy.log10</cite>\nwhich casts the result to the <cite>dtype</cite> of the input.\nFor more details see the [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/log10) and [NumPy](https://numpy.org/doc/1.16/reference/generated/numpy.log10.html) documentation.\nFor more details see the [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/log10) and [NumPy](https://numpy.org/doc/1.16/reference/generated/numpy.log10.html) documentation.\nFor more details see the [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/log10) and [NumPy](https://numpy.org/doc/1.16/reference/generated/numpy.log10.html) documentation."

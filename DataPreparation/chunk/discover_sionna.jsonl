"# Discover Sionna\n\nThis example notebook will guide you through the basic principles and illustrates the key features of [Sionna](https://nvlabs.github.io/sionna). With only a few commands, you can simulate the PHY-layer link-level performance for many 5G-compliant components, including easy visualization of the results."
"## Load Required Packages\n\nThe Sionna python package must be [installed](https://nvlabs.github.io/sionna/installation.html).\n\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Import Sionna\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\nimport numpy as np\nimport tensorflow as tf\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\n# IPython \"magic function\" for inline plots\n%matplotlib inline\nimport matplotlib.pyplot as plt\n```\n\n\n**Tip**: you can run bash commands in Jupyter via the `!` operator.\n\n\n```python\n!nvidia-smi\n```\n\n\n```python\nWed Mar 16 14:05:36 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| 51%   65C    P2   208W / 350W |   5207MiB / 24267MiB |     39%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce ...  Off  | 00000000:4C:00.0 Off |                  N/A |\n|  0%   28C    P8    13W / 350W |  17371MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n```"
"In case multiple GPUs are available, we restrict this notebook to single-GPU usage. You can ignore this command if only one GPU is available.\n\nFurther, we want to avoid that this notebook instantiates the whole GPU memory when initialized and set `memory_growth` as active.\n\n*Remark*: Sionna does not require a GPU. Everything can also run on your CPU - but you may need to wait a little longer.\n\n\n```python\n# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n# For more details, see https://www.tensorflow.org/guide/gpu\ngpus = tf.config.list_physical_devices('GPU')\nprint('Number of GPUs available :', len(gpus))\nif gpus:\n    gpu_num = 0 # Index of the GPU to be used\n    try:\n        #tf.config.set_visible_devices([], 'GPU')\n        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n        print('Only GPU number', gpu_num, 'used.')\n        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n    except RuntimeError as e:\n        print(e)\n```\n\n\n```python\nNumber of GPUs available : 2\nOnly GPU number 0 used.\n```"
"## Sionna Data-flow and Design Paradigms\n\nSionna inherently parallelizes simulations via *batching*, i.e., each element in the batch dimension is simulated independently.\n\nThis means the first tensor dimension is always used for *inter-frame* parallelization similar to an outer *for-loop* in Matlab/NumPy simulations.\n\nTo keep the dataflow efficient, Sionna follows a few simple design principles:\n\n- Signal-processing components are implemented as an individual [Keras layer](https://keras.io/api/layers/).\n- `tf.float32` is used as preferred datatype and `tf.complex64` for complex-valued datatypes, respectively.\nThis allows simpler re-use of components (e.g., the same scrambling layer can be used for binary inputs and LLR-values).\n- Models can be developed in *eager mode* allowing simple (and fast) modification of system parameters.\n- Number crunching simulations can be executed in the faster *graph mode* or even *XLA* acceleration is available for most components.\n- Whenever possible, components are automatically differentiable via [auto-grad](https://www.tensorflow.org/guide/autodiff) to simplify the deep learning design-flow.\n- Code is structured into sub-packages for different tasks such as channel coding, mapping, (see [API documentation](https://nvlabs.github.io/sionna/api/sionna.html) for details).\n\n\nThe division into individual blocks simplifies deployment and all layers and functions comes with unittests to ensure their correct behavior.\n\nThese paradigms simplify the re-useability and reliability of our components for a wide range of communications related applications."
"## Lets Get Started - The First Layers (*Eager Mode*)\n\nEvery layer needs to be initialized once before it can be used.\n\n**Tip**: use the [API documentation](https://nvlabs.github.io/sionna/api/sionna.html) to find an overview of all existing components.\n\nWe now want to transmit some symbols over an AWGN channel. First, we need to initialize the corresponding layer.\n\n\n```python\nchannel = sionna.channel.AWGN() # init AWGN channel layer\n```\n\n\nIn this first example, we want to add Gaussian noise to some given values of `x`.\n\nRemember - the first dimension is the *batch-dimension*.\n\nWe simulate 2 message frames each containing 4 symbols.\n\n*Remark*: the [AWGN channel](https://nvlabs.github.io/sionna/api/channel.html#awgn) is defined to be complex-valued.\n\n\n```python\n# define a (complex-valued) tensor to be transmitted\nx = tf.constant([[0., 1.5, 1., 0.],[-1., 0., -2, 3 ]], dtype=tf.complex64)\n# let's have look at the shape\nprint(\"Shape of x: \", x.shape)\nprint(\"Values of x: \", x)\n```\n\n\n```python\nShape of x:  (2, 4)\nValues of x:  tf.Tensor(\n[[ 0. +0.j  1.5+0.j  1. +0.j  0. +0.j]\n [-1. +0.j  0. +0.j -2. +0.j  3. +0.j]], shape=(2, 4), dtype=complex64)\n```\n\n\nWe want to simulate the channel at an SNR of 5 dB. For this, we can simply *call* the previously defined layer `channel`.\n\nIf you have never used [Keras](https://keras.io) you can think of a layer as of a function: it has an input and returns the processed output.\n\n*Remark*: Each time this cell is executed a new noise realization is drawn.\n\n\n```python\nebno_db = 5\n# calculate noise variance from given EbNo\nno = sionna.utils.ebnodb2no(ebno_db = ebno_db,\n                            num_bits_per_symbol=2, # QPSK\n                            coderate=1)\ny = channel([x, no])\nprint(\"Noisy symbols are: \", y)\n```"
"```python\nNoisy symbols are:  tf.Tensor(\n[[ 0.17642795-0.21076633j  1.540727  +0.2577709j   0.676615  -0.14763176j\n  -0.14807788-0.01961605j]\n [-0.9018068 -0.04732923j -0.55583185+0.41312575j -1.8852113 -0.23232108j\n   3.3803759 +0.2269492j ]], shape=(2, 4), dtype=complex64)\n```"
"## Batches and Multi-dimensional Tensors\n\nSionna natively supports multi-dimensional tensors.\n\nMost layers operate at the last dimension and can have arbitrary input shapes (preserved at output).\n\nLet us assume we want to add a CRC-24 check to 64 codewords of length 500 (e.g., different CRC per sub-carrier). Further, we want to parallelize the simulation over a batch of 100 samples.\n\n\n```python\nbatch_size = 100 # outer level of parallelism\nnum_codewords = 64 # codewords per batch sample\ninfo_bit_length = 500 # info bits PER codeword\nsource = sionna.utils.BinarySource() # yields random bits\nu = source([batch_size, num_codewords, info_bit_length]) # call the source layer\nprint(\"Shape of u: \", u.shape)\n# initialize an CRC encoder with the standard compliant \"CRC24A\" polynomial\nencoder_crc = sionna.fec.crc.CRCEncoder(\"CRC24A\")\ndecoder_crc = sionna.fec.crc.CRCDecoder(encoder_crc) # connect to encoder\n# add the CRC to the information bits u\nc = encoder_crc(u) # returns a list [c, crc_valid]\nprint(\"Shape of c: \", c.shape)\nprint(\"Processed bits: \", np.size(c.numpy()))\n# we can also verify the results\n# returns list of [info bits without CRC bits, indicator if CRC holds]\nu_hat, crc_valid = decoder_crc(c)\nprint(\"Shape of u_hat: \", u_hat.shape)\nprint(\"Shape of crc_valid: \", crc_valid.shape)\nprint(\"Valid CRC check of first codeword: \", crc_valid.numpy()[0,0,0])\n```\n\n\n```python\nShape of u:  (100, 64, 500)\nShape of c:  (100, 64, 524)\nProcessed bits:  3353600\nShape of u_hat:  (100, 64, 500)\nShape of crc_valid:  (100, 64, 1)\nValid CRC check of first codeword:  True\n```\n\n\nWe want to do another simulation but for 5 independent users.\n\nInstead of defining 5 different tensors, we can simply add another dimension.\n\n\n```python\nnum_users = 5\nu = source([batch_size, num_users, num_codewords, info_bit_length])\nprint(\"New shape of u: \", u.shape)\n# We can re-use the same encoder as before\nc = encoder_crc(u)\nprint(\"New shape of c: \", c.shape)\nprint(\"Processed bits: \", np.size(c.numpy()))\n```"
"```python\nNew shape of u:  (100, 5, 64, 500)\nNew shape of c:  (100, 5, 64, 524)\nProcessed bits:  16768000\n```\n\n\nOften a good visualization of results helps to get new research ideas. Thus, Sionna has built-in plotting functions.\n\nLets have look at a 16-QAM constellation.\n\n\n```python\nconstellation = sionna.mapping.Constellation(\"qam\", num_bits_per_symbol=4)\nconstellation.show();\n```"
"## First Link-level Simulation\n\nWe can already build powerful code with a few simple commands.\n\nAs mentioned earlier, Sionna aims at hiding system complexity into Keras layers. However, we still want to provide as much flexibility as possible. Thus, most layers have several choices of init parameters, but often the default choice is a good start.\n\n**Tip**: the [API documentation](https://nvlabs.github.io/sionna/api/sionna.html) provides many helpful references and implementation details.\n\n\n```python\n# system parameters\nn_ldpc = 500 # instruction_answer codeword length\nk_ldpc = 250 # number of info bits per instruction_answer codeword\ncoderate = k_ldpc / n_ldpc\nnum_bits_per_symbol = 4 # number of bits mapped to one symbol (cf. QAM)\n```\n\n\nOften, several different algorithms are implemented, e.g., the demapper supports *true app* demapping, but also *max-log* demapping.\n\nThe check-node (CN) update function of the LDPC BP decoder also supports multiple algorithms.\n\n\n```python\ndemapping_method = \"app\" # try \"max-log\"\nldpc_cn_type = \"boxplus\" # try also \"minsum\"\n```\n\n\nLet us initialize all required components for the given system parameters.\n\n\n```python\nbinary_source = sionna.utils.BinarySource()\nencoder = sionna.fec.ldpc.encoding.LDPC5GEncoder(k_ldpc, n_ldpc)\nconstellation = sionna.mapping.Constellation(\"qam\", num_bits_per_symbol)\nmapper = sionna.mapping.Mapper(constellation=constellation)\nchannel = sionna.channel.AWGN()\ndemapper = sionna.mapping.Demapper(demapping_method,\n                                   constellation=constellation)\ndecoder = sionna.fec.ldpc.decoding.LDPC5GDecoder(encoder,\n                                                 hard_out=True, cn_type=ldpc_cn_type,\n                                                 num_iter=20)\n```\n\n\nWe can now run the code in *eager mode*. This allows us to modify the structure at any time - you can try a different `batch_size` or a different SNR `ebno_db`.\n\n\n```python\n# simulation parameters\nbatch_size = 1000\nebno_db = 4\n# Generate a batch of random bit vectors\nb = binary_source([batch_size, k_ldpc])\n# Encode the bits using 5G instruction_answer code\nprint(\"Shape before encoding: \", b.shape)\nc = encoder(b)\nprint(\"Shape after encoding: \", c.shape)\n# Map bits to constellation symbols\nx = mapper(c)\nprint(\"Shape after mapping: \", x.shape)\n# Transmit over an AWGN channel at SNR 'ebno_db'\nno = sionna.utils.ebnodb2no(ebno_db, num_bits_per_symbol, coderate)\ny = channel([x, no])\nprint(\"Shape after channel: \", y.shape)\n# Demap to LLRs\nllr = demapper([y, no])\nprint(\"Shape after demapping: \", llr.shape)\n# instruction_answer decoding using 20 BP iterations\nb_hat = decoder(llr)\nprint(\"Shape after decoding: \", b_hat.shape)\n# calculate BERs\nc_hat = tf.cast(tf.less(0.0, llr), tf.float32) # hard-decided bits before dec.\nber_uncoded = sionna.utils.metrics.compute_ber(c, c_hat)\nber_coded = sionna.utils.metrics.compute_ber(b, b_hat)\nprint(\"BER uncoded = {:.3f} at EbNo = {:.1f} dB\".format(ber_uncoded, ebno_db))\nprint(\"BER after decoding = {:.3f} at EbNo = {:.1f} dB\".format(ber_coded, ebno_db))\nprint(\"In total {} bits were simulated\".format(np.size(b.numpy())))\n```"
"```python\nShape before encoding:  (1000, 250)\nShape after encoding:  (1000, 500)\nShape after mapping:  (1000, 125)\nShape after channel:  (1000, 125)\nShape after demapping:  (1000, 500)\nShape after decoding:  (1000, 250)\nBER uncoded = 0.119 at EbNo = 4.0 dB\nBER after decoding = 0.010 at EbNo = 4.0 dB\nIn total 250000 bits were simulated\n```\n\n\nJust to summarize: we have simulated the transmission of 250,000 bits including higher-order modulation and channel coding!\n\nBut we can go even faster with the *TF graph execution*!"
"## Setting up the End-to-end Model\n\nWe now define a *Keras model* that is more convenient for training and Monte-Carlo simulations.\n\nWe simulate the transmission over a time-varying multi-path channel (the *TDL-A* model from 3GPP TR38.901). For this, OFDM and a *conventional* bit-interleaved coded modulation (BICM) scheme with higher order modulation is used. The information bits are protected by a 5G-compliant LDPC code.\n\n*Remark*: Due to the large number of parameters, we define them as dictionary.\n\n\n```python\nclass e2e_model(tf.keras.Model): # inherits from keras.model\n    \"\"\"Example model for end-to-end link-level simulations.\n    Parameters\n    ----------\n    params: dict\n        A dictionary defining the system parameters.\n    Input\n    -----\n    batch_size: int or tf.int\n        The batch_sizeused for the simulation.\n    ebno_db: float or tf.float\n        A float defining the simulation SNR.\n    Output\n    ------\n    (b, b_hat):\n        Tuple:\n    b: tf.float32\n        A tensor of shape `[batch_size, k]` containing the transmitted\n        information bits.\n    b_hat: tf.float32\n        A tensor of shape `[batch_size, k]` containing the receiver's\n        estimate of the transmitted information bits.\n    \"\"\"\n    def __init__(self,\n                params):\n        super().__init__()\n\n        # Define an OFDM Resource Grid Object\n        self.rg = sionna.ofdm.ResourceGrid(\n                            num_ofdm_symbols=params[\"num_ofdm_symbols\"],\n                            fft_size=params[\"fft_size\"],\n                            subcarrier_spacing=params[\"subcarrier_spacing\"],\n                            num_tx=1,\n                            num_streams_per_tx=1,\n                            cyclic_prefix_length=params[\"cyclic_prefix_length\"],\n                            pilot_pattern=\"kronecker\",\n                            pilot_ofdm_symbol_indices=params[\"pilot_ofdm_symbol_indices\"])\n        # Create a Stream Management object\n        self.sm = sionna.mimo.StreamManagement(rx_tx_association=np.array([[1]]),\n                                               num_streams_per_tx=1)\n        self.coderate = params[\"coderate\"]\n        self.num_bits_per_symbol = params[\"num_bits_per_symbol\"]\n        self.n = int(self.rg.num_data_symbols*self.num_bits_per_symbol)\n        self.k = int(self.n*coderate)\n        # Init layers\n        self.binary_source = sionna.utils.BinarySource()\n        self.encoder = sionna.fec.ldpc.encoding.LDPC5GEncoder(self.k, self.n)\n        self.interleaver = sionna.fec.interleaving.RowColumnInterleaver(\n                                        row_depth=self.num_bits_per_symbol)\n        self.deinterleaver = sionna.fec.interleaving.Deinterleaver(self.interleaver)\n        self.mapper = sionna.mapping.Mapper(\"qam\", self.num_bits_per_symbol)\n        self.rg_mapper = sionna.ofdm.ResourceGridMapper(self.rg)\n        self.tdl = sionna.channel.tr38901.TDL(model=\"A\",\n                           delay_spread=params[\"delay_spread\"],\n                           carrier_frequency=params[\"carrier_frequency\"],\n                           min_speed=params[\"min_speed\"],\n                           max_speed=params[\"max_speed\"])\n        self.channel = sionna.channel.OFDMChannel(self.tdl, self.rg, add_awgn=True, normalize_channel=True)\n        self.ls_est = sionna.ofdm.LSChannelEstimator(self.rg, interpolation_type=\"nn\")\n        self.lmmse_equ = sionna.ofdm.LMMSEEqualizer(self.rg, self.sm)\n        self.demapper = sionna.mapping.Demapper(params[\"demapping_method\"],\n                                                \"qam\", self.num_bits_per_symbol)\n        self.decoder = sionna.fec.ldpc.decoding.LDPC5GDecoder(self.encoder,\n                                                    hard_out=True,\n                                                    cn_type=params[\"cn_type\"],\n                                                    num_iter=params[\"bp_iter\"])\n        print(\"Number of pilots: {}\".format(self.rg.num_pilot_symbols))\n        print(\"Number of data symbols: {}\".format(self.rg.num_data_symbols))\n        print(\"Number of resource elements: {}\".format(\n                                    self.rg.num_resource_elements))\n        print(\"Pilot overhead: {:.2f}%\".format(\n                                    self.rg.num_pilot_symbols /\n                                    self.rg.num_resource_elements*100))\n        print(\"Cyclic prefix overhead: {:.2f}%\".format(\n                                    params[\"cyclic_prefix_length\"] /\n                                    (params[\"cyclic_prefix_length\"]\n                                    +params[\"fft_size\"])*100))\n        print(\"Each frame contains {} information bits\".format(self.k))\n    def call(self, batch_size, ebno_db):\n        # Generate a batch of random bit vectors\n        # We need two dummy dimension representing the number of\n        # transmitters and streams per transmitter, respectively.\n        b = self.binary_source([batch_size, 1, 1, self.k])\n        # Encode the bits using the all-zero dummy encoder\n        c = self.encoder(b)\n        # Interleave the bits before mapping (BICM)\n        c_int = self.interleaver(c)\n        # Map bits to constellation symbols\n        s = self.mapper(c_int)\n        # Map symbols onto OFDM ressource grid\n        x_rg = self.rg_mapper(s)\n        # Transmit over noisy multi-path channel\n        no = sionna.utils.ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate, self.rg)\n        y = self.channel([x_rg, no])\n        # LS Channel estimation with nearest pilot interpolation\n        h_hat, err_var = self.ls_est ([y, no])\n        # LMMSE Equalization\n        x_hat, no_eff = self.lmmse_equ([y, h_hat, err_var, no])\n        # Demap to LLRs\n        llr = self.demapper([x_hat, no_eff])\n        # Deinterleave before decoding\n        llr_int = self.deinterleaver(llr)\n        # Decode\n        b_hat = self.decoder(llr_int)\n        # number of simulated bits\n        nb_bits = batch_size*self.k\n        # transmitted bits and the receiver's estimate after decoding\n        return b, b_hat\n```"
"Let us define the system parameters for our simulation as dictionary:\n\n\n```python\nsys_params = {\n    # Channel\n    \"carrier_frequency\" : 3.5e9,\n    \"delay_spread\" : 100e-9,\n    \"min_speed\" : 3,\n    \"max_speed\" : 3,\n    \"tdl_model\" : \"A\",\n    # OFDM\n    \"fft_size\" : 256,\n    \"subcarrier_spacing\" : 30e3,\n    \"num_ofdm_symbols\" : 14,\n    \"cyclic_prefix_length\" : 16,\n    \"pilot_ofdm_symbol_indices\" : [2, 11],\n    # Code & Modulation\n    \"coderate\" : 0.5,\n    \"num_bits_per_symbol\" : 4,\n    \"demapping_method\" : \"app\",\n    \"cn_type\" : \"boxplus\",\n    \"bp_iter\" : 20\n}\n```\n\n\nand initialize the model:\n\n\n```python\nmodel = e2e_model(sys_params)\n```\n\n\n```python\nNumber of pilots: 512\nNumber of data symbols: 3072\nNumber of resource elements: 3584\nPilot overhead: 14.29%\nCyclic prefix overhead: 5.88%\nEach frame contains 6144 information bits\n```\n\n\nAs before, we can simply *call* the model to simulate the BER for the given simulation parameters.\n\n\n```python\n#simulation parameters\nebno_db = 10\nbatch_size = 200\n# and call the model\nb, b_hat = model(batch_size, ebno_db)\nber = sionna.utils.metrics.compute_ber(b, b_hat)\nnb_bits = np.size(b.numpy())\nprint(\"BER: {:.4} at Eb/No of {} dB and {} simulated bits\".format(ber.numpy(), ebno_db, nb_bits))\n```\n\n\n```python\nBER: 0.006234 at Eb/No of 10 dB and 1228800 simulated bits\n```"
"## Run some Throughput Tests (Graph Mode)\n\nSionna is not just an easy-to-use library, but also incredibly fast. Let us measure the throughput of the model defined above.\n\nWe compare *eager* and *graph* execution modes (see [Tensorflow Doc](https://www.tensorflow.org/guide/intro_to_graphs) for details), as well as *eager with XLA* (see [https://www.tensorflow.org/xla#enable_xla_for_tensorflow_models](https://www.tensorflow.org/xla#enable_xla_for_tensorflow_models)). Note that we need to activate the [sionna.config.xla_compat](https://nvlabs.github.io/sionna/api/config.html#sionna.Config.xla_compat) feature for XLA to work.\n\n**Tip**: change the `batch_size` to see how the batch parallelism enhances the throughput. Depending on your machine, the `batch_size` may be too large.\n\n\n```python\nimport time # this block requires the timeit library\nbatch_size = 200\nebno_db = 5 # evalaute SNR point\nrepetitions = 4 # throughput is averaged over multiple runs\ndef get_throughput(batch_size, ebno_db, model, repetitions=1):\n    \"\"\" Simulate throughput in bit/s per ebno_db point.\n    The results are average over `repetition` trials.\n    Input\n    -----\n    batch_size: int or tf.int32\n        Batch-size for evaluation.\n    ebno_db: float or tf.float32\n        A tensor containing the SNR points be evaluated\n    model:\n        Function or model that yields the transmitted bits `u` and the\n        receiver's estimate `u_hat` for a given ``batch_size`` and\n        ``ebno_db``.\n    repetitions: int\n        An integer defining how many trails of the throughput\n        simulation are averaged.\n    \"\"\"\n\n    # call model once to be sure it is compile properly\n    # otherwise time to build graph is measured as well.\n    u, u_hat = model(tf.constant(batch_size, tf.int32),\n                     tf.constant(ebno_db, tf.float32))\n    t_start = time.perf_counter()\n    # average over multiple runs\n    for _ in range(repetitions):\n        u, u_hat = model(tf.constant(batch_size, tf.int32),\n                            tf.constant(ebno_db, tf. float32))\n    t_stop = time.perf_counter()\n    # throughput in bit/s\n    throughput = np.size(u.numpy())*repetitions / (t_stop - t_start)\n    return throughput\n# eager mode - just call the model\ndef run_eager(batch_size, ebno_db):\n    return model(batch_size, ebno_db)\ntime_eager = get_throughput(batch_size, ebno_db, run_eager, repetitions=4)\n# the decorator \"@tf.function\" enables the graph mode\n@tf.function\ndef run_graph(batch_size, ebno_db):\n    return model(batch_size, ebno_db)\ntime_graph = get_throughput(batch_size, ebno_db, run_graph, repetitions=4)\n# the decorator \"@tf.function(jit_compile=True)\" enables the graph mode with XLA\n# we need to activate the sionna.config.xla_compat feature for this to work\nsionna.config.xla_compat=True\n@tf.function(jit_compile=True)\ndef run_graph_xla(batch_size, ebno_db):\n    return model(batch_size, ebno_db)\ntime_graph_xla = get_throughput(batch_size, ebno_db, run_graph_xla, repetitions=4)\n# we deactivate the sionna.config.xla_compat so that the cell can be run mutiple times\nsionna.config.xla_compat=False\nprint(f\"Throughput in eager execution: {time_eager/1e6:.2f} Mb/s\")\nprint(f\"Throughput in graph execution: {time_graph/1e6:.2f} Mb/s\")\nprint(f\"Throughput in graph execution with XLA: {time_graph_xla/1e6:.2f} Mb/s\")\n```"
"```python\nWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\nInstructions for updating:\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\nThroughput in eager execution: 0.51 Mb/s\nThroughput in graph execution: 4.10 Mb/s\nThroughput in graph execution with XLA: 43.72 Mb/s\n```\n\n\nObviously, *graph* execution (with XLA) yields much higher throughputs (at least if a fast GPU is available). Thus, for exhaustive training and Monte-Carlo simulations the *graph* mode (with XLA and GPU acceleration) is the preferred choice."
"## Bit-Error Rate (BER) Monte-Carlo Simulations\n\nMonte-Carlo simulations are omnipresent in todays communications research and development. Due its performant implementation, Sionna can be directly used to simulate BER at a performance that competes with compiled languages  but still keeps the flexibility of a script language.\n\n\n```python\nebno_dbs = np.arange(0, 15, 1.)\nbatch_size = 200 # reduce in case you receive an out-of-memory (OOM) error\nmax_mc_iter = 1000 # max number of Monte-Carlo iterations before going to next SNR point\nnum_target_block_errors = 500 # continue with next SNR point after target number of block errors\n# we use the built-in ber simulator function from Sionna which uses and early stop after reaching num_target_errors\nsionna.config.xla_compat=True\nber_mc,_ = sionna.utils.sim_ber(run_graph_xla, # you can also evaluate the model directly\n                                ebno_dbs,\n                                batch_size=batch_size,\n                                num_target_block_errors=num_target_block_errors,\n                                max_mc_iter=max_mc_iter,\n                                verbose=True) # print status and summary\nsionna.config.xla_compat=False\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      0.0 | 3.4157e-01 | 1.0000e+00 |     1259148 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      1.0 | 3.1979e-01 | 1.0000e+00 |     1178870 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      2.0 | 2.9844e-01 | 1.0000e+00 |     1100177 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      3.0 | 2.7401e-01 | 1.0000e+00 |     1010102 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      4.0 | 2.4763e-01 | 1.0000e+00 |      912849 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      5.0 | 2.2038e-01 | 1.0000e+00 |      812407 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      6.0 | 1.8646e-01 | 1.0000e+00 |      687378 |     3686400 |          600 |         600 |         0.1 |reached target block errors\n      7.0 | 1.1909e-01 | 9.9000e-01 |      439008 |     3686400 |          594 |         600 |         0.1 |reached target block errors\n      8.0 | 4.1536e-02 | 4.7667e-01 |      306236 |     7372800 |          572 |        1200 |         0.2 |reached target block errors\n      9.0 | 1.2096e-02 | 1.4028e-01 |      267553 |    22118400 |          505 |        3600 |         0.6 |reached target block errors\n     10.0 | 3.2914e-03 | 3.5278e-02 |      291203 |    88473600 |          508 |       14400 |         2.5 |reached target block errors\n     11.0 | 9.5878e-04 | 9.8814e-03 |      298073 |   310886400 |          500 |       50600 |         8.5 |reached target block errors\n     12.0 | 2.6973e-04 | 2.7933e-03 |      296647 |  1099776000 |          500 |      179000 |        29.6 |reached target block errors\n     13.0 | 9.2277e-05 | 9.6000e-04 |      113390 |  1228800000 |          192 |      200000 |        32.9 |reached max iter\n     14.0 | 3.3341e-05 | 3.8000e-04 |       40970 |  1228800000 |           76 |      200000 |        32.6 |reached max iter\n```"
"Lets look at the results.\n\n\n```python\nsionna.utils.plotting.plot_ber(ebno_dbs,\n                               ber_mc,\n                               legend=\"E2E Model\",\n                               ylabel=\"Coded BER\");\n```"
"## Conclusion\n\nWe hope you are excited about Sionna - there is much more to be discovered:\n\n- TensorBoard debugging available\n- Scaling to multi-GPU simulation is simple\n- See the [available tutorials](https://nvlabs.github.io/sionna/tutorials.html) for more advanced examples.\n\n\nAnd if something is still missing - the project is [open-source](https://github.com/nvlabs/sionna/): you can modify, add, and extend any component at any time.And if something is still missing - the project is [open-source](https://github.com/nvlabs/sionna/): you can modify, add, and extend any component at any time.And if something is still missing - the project is [open-source](https://github.com/nvlabs/sionna/): you can modify, add, and extend any component at any time.And if something is still missing - the project is [open-source](https://github.com/nvlabs/sionna/): you can modify, add, and extend any component at any time."

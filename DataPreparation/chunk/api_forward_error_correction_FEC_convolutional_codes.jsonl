"# Convolutional Codes\n\nThis module supports encoding of convolutional codes and provides layers for Viterbi [[Viterbi]](https://nvlabs.github.io/sionna/api/fec.conv.html#viterbi) and BCJR [[BCJR]](https://nvlabs.github.io/sionna/api/fec.conv.html#bcjr) decoding.\n\nWhile the [`ViterbiDecoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ViterbiDecoder) decoding algorithm produces maximum likelihood *sequence* estimates, the [`BCJRDecoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.BCJRDecoder) produces the maximum a posterior (MAP) bit-estimates.\n\nThe following code snippet shows how to set up a rate-1/2, constraint-length-3 [`ConvEncoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ConvEncoder) in two alternate ways and a corresponding [`ViterbiDecoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ViterbiDecoder) or [`BCJRDecoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.BCJRDecoder). You can find further examples in the [Channel Coding Tutorial Notebook](../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html).\n\nSetting-up:\n```python\nencoder = ConvEncoder(rate=1/2, # rate of the desired code\n                      constraint_length=3) # constraint length of the code\n# or\nencoder = ConvEncoder(gen_poly=['101', '111']) # or polynomial can be used as input directly\n# --- Viterbi decoding ---\ndecoder = ViterbiDecoder(gen_poly=encoder.gen_poly) # polynomial used in encoder\n# or just reference to the encoder\ndecoder = ViterbiDecoder(encoder=encoder) # the code parameters are infered from the encoder\n# --- or BCJR decoding ---\ndecoder = BCJRDecoder(gen_poly=encoder.gen_poly, algorithm=\"map\") # polynomial used in encoder\n# or just reference to the encoder\ndecoder = BCJRDecoder(encoder=encoder, algorithm=\"map\") # the code parameters are infered from the encoder\n```\n\n\nRunning the encoder / decoder:"
"```python\n# --- encoder ---\n# u contains the information bits to be encoded and has shape [...,k].\n# c contains the convolutional encoded codewords and has shape [...,n].\nc = encoder(u)\n# --- decoder ---\n# y contains the de-mapped received codeword from channel and has shape [...,n].\n# u_hat contains the estimated information bits and has shape [...,k].\nu_hat = decoder(y)\n```"
"## Convolutional Encoding\n\n`class` `sionna.fec.conv.``ConvEncoder`(*`gen_poly``=``None`*, *`rate``=``1` `/` `2`*, *`constraint_length``=``3`*, *`rsc``=``False`*, *`terminate``=``False`*, *`output_dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/fec/conv/encoding.html#ConvEncoder)\n\nEncodes an information binary tensor to a convolutional codeword. Currently,\nonly generator polynomials for codes of rate=1/n for n=2,3,4, are allowed.\n\nThe class inherits from the Keras layer class and can be used as layer in a\nKeras model.\nParameters\n\n- **gen_poly** (*tuple*)  Sequence of strings with each string being a 0,1 sequence. If\n<cite>None</cite>, `rate` and `constraint_length` must be provided.\n- **rate** (*float*)  Valid values are 1/3 and 0.5. Only required if `gen_poly` is\n<cite>None</cite>.\n- **constraint_length** (*int*)  Valid values are between 3 and 8 inclusive. Only required if\n`gen_poly` is <cite>None</cite>.\n- **rsc** (*boolean*)  Boolean flag indicating whether the Trellis generated is recursive\nsystematic or not. If <cite>True</cite>, the encoder is recursive-systematic.\nIn this case first polynomial in `gen_poly` is used as the\nfeedback polynomial. Defaults to <cite>False</cite>.\n- **terminate** (*boolean*)  Encoder is terminated to all zero state if <cite>True</cite>.\nIf terminated, the <cite>true</cite> rate of the code is slightly lower than\n`rate`.\n- **output_dtype** (*tf.DType*)  Defaults to <cite>tf.float32</cite>. Defines the output datatype of the layer.\n\n\nInput\n\n**inputs** (*[,k], tf.float32*)  2+D tensor containing the information bits where <cite>k</cite> is the\ninformation length\n\nOutput\n\n*[,k/rate], tf.float32*  2+D tensor containing the encoded codeword for the given input\ninformation tensor where <cite>rate</cite> is\n$\\frac{1}{\\textrm{len}\\left(\\textrm{gen_poly}\\right)}$\n(if `gen_poly` is provided).\n\n\n**Note**\n\nThe generator polynomials from [[Moon]](https://nvlabs.github.io/sionna/api/fec.conv.html#moon) are available for various\nrate and constraint lengths. To select them, use the `rate` and\n`constraint_length` arguments.\n\nIn addition, polynomials for any non-recursive convolutional encoder\ncan be given as input via `gen_poly` argument. Currently, only\npolynomials with rate=1/n are supported. When the `gen_poly` argument\nis given, the `rate` and `constraint_length` arguments are ignored.\n\nVarious notations are used in the literature to represent the generator\npolynomials for convolutional codes. In [[Moon]](https://nvlabs.github.io/sionna/api/fec.conv.html#moon), the octal digits\nformat is primarily used. In the octal format, the generator polynomial\n<cite>10011</cite> corresponds to 46. Another widely used format\nis decimal notation with MSB. In this notation, polynomial <cite>10011</cite>\ncorresponds to 19. For simplicity, the\n[`ConvEncoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ConvEncoder) only accepts the bit\nformat i.e. <cite>10011</cite> as `gen_poly` argument.\n\nAlso note that `constraint_length` and `memory` are two different\nterms often used to denote the strength of a convolutional code. In this\nsub-package, we use `constraint_length`. For example, the\npolynomial <cite>10011</cite> has a `constraint_length` of 5, however its\n`memory` is only 4.\n\nWhen `terminate` is <cite>True</cite>, the true rate of the convolutional\ncode is slightly lower than `rate`. It equals\n$\\frac{r*k}{k+\\mu}$ where <cite>r</cite> denotes `rate` and\n$\\mu$ is `constraint_length` - 1. For example when\n`terminate` is <cite>True</cite>, `k=100`,\n$\\mu=4$ and `rate` =0.5, true rate equals\n$\\frac{0.5*100}{104}=0.481$.\n\n`property` `coderate`\n\nRate of the code used in the encoder\n\n\n`property` `gen_poly`\n\nGenerator polynomial used by the encoder\n\n\n`property` `k`\n\nNumber of information bits per codeword\n\n\n`property` `n`\n\nNumber of codeword bits\n\n\n`property` `terminate`\n\nIndicates if the convolutional encoder is terminated\n\n\n`property` `trellis`\n\nTrellis object used during encoding"
"## Viterbi Decoding\n\n`class` `sionna.fec.conv.``ViterbiDecoder`(*`encoder``=``None`*, *`gen_poly``=``None`*, *`rate``=``1` `/` `2`*, *`constraint_length``=``3`*, *`rsc``=``False`*, *`terminate``=``False`*, *`method``=``'soft_llr'`*, *`output_dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/fec/conv/decoding.html#ViterbiDecoder)\n\nImplements the Viterbi decoding algorithm [[Viterbi]](https://nvlabs.github.io/sionna/api/fec.conv.html#viterbi) that returns an\nestimate of the information bits for a noisy convolutional codeword.\nTakes as input either LLR values (<cite>method</cite> = <cite>soft_llr</cite>) or hard bit values\n(<cite>method</cite> = <cite>hard</cite>) and returns a hard decided estimation of the information\nbits.\n\nThe class inherits from the Keras layer class and can be used as layer in\na Keras model.\nParameters\n\n- **encoder** ([`ConvEncoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ConvEncoder))  If `encoder` is provided as input, the following input parameters\nare not required and will be ignored: `gen_poly`, `rate`,\n`constraint_length`, `rsc`, `terminate`. They will be inferred\nfrom the `encoder`  object itself. If `encoder` is <cite>None</cite>, the\nabove parameters must be provided explicitly.\n- **gen_poly** (*tuple*)  tuple of strings with each string being a 0, 1 sequence. If <cite>None</cite>,\n`rate` and `constraint_length` must be provided.\n- **rate** (*float*)  Valid values are 1/3 and 0.5. Only required if `gen_poly` is <cite>None</cite>.\n- **constraint_length** (*int*)  Valid values are between 3 and 8 inclusive. Only required if\n`gen_poly` is <cite>None</cite>.\n- **rsc** (*boolean*)  Boolean flag indicating whether the encoder is recursive-systematic for\ngiven generator polynomials.\n<cite>True</cite> indicates encoder is recursive-systematic.\n<cite>False</cite> indicates encoder is feed-forward non-systematic.\n- **terminate** (*boolean*)  Boolean flag indicating whether the codeword is terminated.\n<cite>True</cite> indicates codeword is terminated to all-zero state.\n<cite>False</cite> indicates codeword is not terminated.\n- **method** (*str*)  Valid values are <cite>soft_llr</cite> or <cite>hard</cite>. In computing path\nmetrics,\n<cite>soft_llr</cite> expects channel LLRs as input\n<cite>hard</cite> assumes a <cite>binary symmetric channel</cite> (BSC) with 0/1 values are\ninputs. In case of <cite>hard</cite>, <cite>inputs</cite> will be quantized to 0/1 values.\n- **output_dtype** (*tf.DType*)  Defaults to tf.float32. Defines the output datatype of the layer.\n\n\nInput\n\n**inputs** (*[,n], tf.float32*)  2+D tensor containing the (noisy) channel output symbols where <cite>n</cite>\ndenotes the codeword length\n\nOutput\n\n*[,rate*n], tf.float32*  2+D tensor containing the estimates of the information bit tensor\n\n\n**Note**\n\nA full implementation of the decoder rather than a windowed approach\nis used. For a given codeword of duration <cite>T</cite>, the path metric is\ncomputed from time <cite>0</cite> to <cite>T</cite> and the path with optimal metric at time\n<cite>T</cite> is selected. The optimal path is then traced back from <cite>T</cite> to <cite>0</cite>\nto output the estimate of the information bit vector used to encode.\nFor larger codewords, note that the current method is sub-optimal\nin terms of memory utilization and latency.\n\n`property` `coderate`\n\nRate of the code used in the encoder\n\n\n`property` `gen_poly`\n\nGenerator polynomial used by the encoder\n\n\n`property` `k`\n\nNumber of information bits per codeword\n\n\n`property` `n`\n\nNumber of codeword bits\n\n\n`property` `terminate`\n\nIndicates if the encoder is terminated during codeword generation\n\n\n`property` `trellis`\n\nTrellis object used during encoding"
"## BCJR Decoding\n\n`class` `sionna.fec.conv.``BCJRDecoder`(*`encoder``=``None`*, *`gen_poly``=``None`*, *`rate``=``1` `/` `2`*, *`constraint_length``=``3`*, *`rsc``=``False`*, *`terminate``=``False`*, *`hard_out``=``True`*, *`algorithm``=``'map'`*, *`output_dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/fec/conv/decoding.html#BCJRDecoder)\n\nImplements the BCJR decoding algorithm [[BCJR]](https://nvlabs.github.io/sionna/api/fec.conv.html#bcjr) that returns an\nestimate of the information bits for a noisy convolutional codeword.\nTakes as input either channel LLRs or a tuple\n(channel LLRs, apriori LLRs). Returns an estimate of the information\nbits, either output LLRs ( `hard_out` = <cite>False</cite>) or hard decoded\nbits ( `hard_out` = <cite>True</cite>), respectively.\n\nThe class inherits from the Keras layer class and can be used as layer in\na Keras model.\nParameters\n\n- **encoder** ([`ConvEncoder`](https://nvlabs.github.io/sionna/api/fec.conv.html#sionna.fec.conv.ConvEncoder))  If `encoder` is provided as input, the following input parameters\nare not required and will be ignored: `gen_poly`, `rate`,\n`constraint_length`, `rsc`, `terminate`. They will be inferred\nfrom the `encoder`  object itself. If `encoder` is <cite>None</cite>, the\nabove parameters must be provided explicitly.\n- **gen_poly** (*tuple*)  tuple of strings with each string being a 0, 1 sequence. If <cite>None</cite>,\n`rate` and `constraint_length` must be provided.\n- **rate** (*float*)  Valid values are 1/3 and 1/2. Only required if `gen_poly` is <cite>None</cite>.\n- **constraint_length** (*int*)  Valid values are between 3 and 8 inclusive. Only required if\n`gen_poly` is <cite>None</cite>.\n- **rsc** (*boolean*)  Boolean flag indicating whether the encoder is recursive-systematic for\ngiven generator polynomials. <cite>True</cite> indicates encoder is\nrecursive-systematic. <cite>False</cite> indicates encoder is feed-forward non-systematic.\n- **terminate** (*boolean*)  Boolean flag indicating whether the codeword is terminated.\n<cite>True</cite> indicates codeword is terminated to all-zero state.\n<cite>False</cite> indicates codeword is not terminated.\n- **hard_out** (*boolean*)  Boolean flag indicating whether to output hard or soft decisions on\nthe decoded information vector.\n<cite>True</cite> implies a hard-decoded information vector of 0/1s as output.\n<cite>False</cite> implies output is decoded LLRs of the information.\n- **algorithm** (*str*)  Defaults to <cite>map</cite>. Indicates the implemented BCJR algorithm,\nwhere <cite>map</cite> denotes the exact MAP algorithm, <cite>log</cite> indicates the\nexact MAP implementation, but in log-domain, and\n<cite>maxlog</cite> indicates the approximated MAP implementation in log-domain,\nwhere $\\log(e^{a}+e^{b}) \\sim \\max(a,b)$.\n- **output_dtype** (*tf.DType*)  Defaults to tf.float32. Defines the output datatype of the layer.\n\n\nInput\n\n- **llr_ch or (llr_ch, llr_a)**  Tensor or Tuple:\n- **llr_ch** (*[,n], tf.float32*)  2+D tensor containing the (noisy) channel\nLLRs, where <cite>n</cite> denotes the codeword length\n- **llr_a** (*[,k], tf.float32*)  2+D tensor containing the a priori information of each information bit.\nImplicitly assumed to be 0 if only `llr_ch` is provided.\n\n\nOutput\n\n*tf.float32*  2+D tensor of shape <cite>[,coderate*n]</cite> containing the estimates of the\ninformation bit tensor\n\n\n`property` `coderate`\n\nRate of the code used in the encoder\n\n\n`property` `gen_poly`\n\nGenerator polynomial used by the encoder\n\n\n`property` `k`\n\nNumber of information bits per codeword\n\n\n`property` `n`\n\nNumber of codeword bits\n\n\n`property` `terminate`\n\nIndicates if the encoder is terminated during codeword generation\n\n\n`property` `trellis`\n\nTrellis object used during encoding"
"### Trellis\n\n`sionna.fec.conv.utils.``Trellis`(*`gen_poly`*, *`rsc``=``True`*)[`[source]`](../_modules/sionna/fec/conv/utils.html#Trellis)\n\nTrellis structure for a given generator polynomial. Defines\nstate transitions and output symbols (and bits) for each current\nstate and input.\nParameters\n\n- **gen_poly** (*tuple*)  Sequence of strings with each string being a 0,1 sequence.\nIf <cite>None</cite>, `rate` and `constraint_length` must be provided. If\n<cite>rsc</cite> is True, then first polynomial will act as denominator for\nthe remaining generator polynomials. For e.g., `rsc` = <cite>True</cite> and\n`gen_poly` = (<cite>111</cite>, <cite>101</cite>, <cite>011</cite>) implies generator matrix equals\n$G(D)=[\\frac{1+D^2}{1+D+D^2}, \\frac{D+D^2}{1+D+D^2}]$.\nCurrently Trellis is only implemented for generator matrices of\nsize $\\frac{1}{n}$.\n- **rsc** (*boolean*)  Boolean flag indicating whether the Trellis is recursive systematic\nor not. If <cite>True</cite>, the encoder is recursive systematic in which\ncase first polynomial in `gen_poly` is used as the feedback\npolynomial. Default is <cite>True</cite>."
"### polynomial_selector\n\n`sionna.fec.conv.utils.``polynomial_selector`(*`rate`*, *`constraint_length`*)[`[source]`](../_modules/sionna/fec/conv/utils.html#polynomial_selector)\n\nReturns generator polynomials for given code parameters. The\npolynomials are chosen from [[Moon]](https://nvlabs.github.io/sionna/api/fec.conv.html#moon) which are tabulated by searching\nfor polynomials with best free distances for a given rate and\nconstraint length.\nInput\n\n- **rate** (*float*)  Desired rate of the code.\nCurrently, only r=1/3 and r=1/2 are supported.\n- **constraint_length** (*int*)  Desired constraint length of the encoder\n\n\nOutput\n\n*tuple*  Tuple of strings with each string being a 0,1 sequence where\neach polynomial is represented in binary form.\n\n\nReferences:\nViterbi([1](https://nvlabs.github.io/sionna/api/fec.conv.html#id1),[2](https://nvlabs.github.io/sionna/api/fec.conv.html#id5))\n\nA. Viterbi, Error bounds for convolutional codes and an\nasymptotically optimum decoding algorithm, IEEE Trans. Inf. Theory, 1967.\n\nBCJR([1](https://nvlabs.github.io/sionna/api/fec.conv.html#id2),[2](https://nvlabs.github.io/sionna/api/fec.conv.html#id6))\n\nL. Bahl, J. Cocke, F. Jelinek, und J. Raviv, Optimal Decoding\nof Linear Codes for Minimizing Symbol Error Rate, IEEE Trans. Inf.\nTheory, March 1974.\n\nMoon([1](https://nvlabs.github.io/sionna/api/fec.conv.html#id3),[2](https://nvlabs.github.io/sionna/api/fec.conv.html#id4),[3](https://nvlabs.github.io/sionna/api/fec.conv.html#id7))\n\nTodd. K. Moon, Error Correction Coding: Mathematical\nMethods and Algorithms, John Wiley & Sons, 2020."

"# Basic MIMO Simulations\n\nIn this notebook, you will learn how to setup simulations of MIMO transmissions over a flat-fading channel.\n\nHere is a schematic diagram of the system model with all required components:\n\n\nYou will learn how to:\n\n- Use the FastFadingChannel class\n- Apply spatial antenna correlation\n- Implement LMMSE detection with perfect channel knowledge\n- Run BER/SER simulations\n\n\nWe will first walk through the configuration of all components of the system model, before building a general Keras model which will allow you to run efficiently simulations with different parameter settings."
"## GPU Configuration and Imports\n\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Import Sionna\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n# For more details, see https://www.tensorflow.org/guide/gpu\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\n```\n\n```python\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nfrom sionna.utils import BinarySource, QAMSource, ebnodb2no, compute_ser, compute_ber, PlotBER\nfrom sionna.channel import FlatFadingChannel, KroneckerModel\nfrom sionna.channel.utils import exp_corr_mat\nfrom sionna.mimo import lmmse_equalizer\nfrom sionna.mapping import SymbolDemapper, Mapper, Demapper\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\n```"
"## Simple uncoded transmission\n\nWe will consider point-to-point transmissions from a transmitter with `num_tx_ant` antennas to a receiver with `num_rx_ant` antennas. The transmitter applies no precoding and sends independent data stream from each antenna.\n\nLet us now generate a batch of random transmit vectors of random 16QAM symbols:\n\n\n```python\nnum_tx_ant = 4\nnum_rx_ant = 16\nnum_bits_per_symbol = 4\nbatch_size = 1024\nqam_source = QAMSource(num_bits_per_symbol)\nx = qam_source([batch_size, num_tx_ant])\nprint(x.shape)\n```\n\n\n```python\n(1024, 4)\n```\n\n\nNext, we will create an instance of the `FlatFadingChannel` class to simulate transmissions over an i.i.d. Rayleigh fading channel. The channel will also add AWGN with variance `no`. As we will need knowledge of the channel realizations for detection, we activate the `return_channel` flag.\n\n\n```python\nchannel = FlatFadingChannel(num_tx_ant, num_rx_ant, add_awgn=True, return_channel=True)\nno = 0.2 # Noise variance of the channel\n# y and h are the channel output and channel realizations, respectively.\ny, h = channel([x, no])\nprint(y.shape)\nprint(h.shape)\n```\n\n\n```python\n(1024, 16)\n(1024, 16, 4)\n```\n\n\nUsing the perfect channel knowledge, we can now implement an LMMSE equalizer to compute soft-symbols. The noise covariance matrix in this example is just a scaled identity matrix which we need to provide to the `lmmse_equalizer`.\n\n\n```python\ns = tf.cast(no*tf.eye(num_rx_ant, num_rx_ant), y.dtype)\nx_hat, no_eff = lmmse_equalizer(y, h, s)\n```\n\n\nLet us know have a look at the transmitted and received constellations:\n\n\n```python\nplt.axes().set_aspect(1.0)\nplt.scatter(np.real(x_hat), np.imag(x_hat));\nplt.scatter(np.real(x), np.imag(x));\n```\n\n\nAs expected, the soft symbols `x_hat` are scattered around the 16QAM constellation points. The equalizer output `no_eff` provides an estimate of the effective noise variance for each soft-symbol.\n\n\n```python\nprint(no_eff.shape)\n```\n\n\n```python\n(1024, 4)\n```"
"One can confirm that this estimate is correct by comparing the MSE between the transmitted and equalized symbols against the average estimated effective noise variance:\n\n\n```python\nnoise_var_eff = np.var(x-x_hat)\nnoise_var_est = np.mean(no_eff)\nprint(noise_var_eff)\nprint(noise_var_est)\n```\n\n\n```python\n0.016722694\n0.016684469\n```\n\n\nThe last step is to make hard decisions on the symbols and compute the SER:\n\n\n```python\nsymbol_demapper = SymbolDemapper(\"qam\", num_bits_per_symbol, hard_out=True)\n# Get symbol indices for the transmitted symbols\nx_ind = symbol_demapper([x, no])\n# Get symbol indices for the received soft-symbols\nx_ind_hat = symbol_demapper([x_hat, no])\ncompute_ser(x_ind, x_ind_hat)\n```\n\n```python\n<tf.Tensor: shape=(), dtype=float64, numpy=0.002197265625>\n```"
"### Adding spatial correlation\n\nIt is very easy add spatial correlation to the `FlatFadingChannel` using the `SpatialCorrelation` class. We can, e.g., easily setup a Kronecker (`KroneckerModel`) (or two-sided) correlation model using exponetial correlation matrices (`exp_corr_mat`).\n\n\n```python\n# Create transmit and receive correlation matrices\nr_tx = exp_corr_mat(0.4, num_tx_ant)\nr_rx = exp_corr_mat(0.9, num_rx_ant)\n# Add the spatial correlation model to the channel\nchannel.spatial_corr = KroneckerModel(r_tx, r_rx)\n```\n\n\nNext, we can validate that the channel model applies the desired spatial correlation by creating a large batch of channel realizations from which we compute the empirical transmit and receiver covariance matrices:\n\n\n```python\nh = channel.generate(1000000)\n# Compute empirical covariance matrices\nr_tx_hat = tf.reduce_mean(tf.matmul(h, h, adjoint_a=True), 0)/num_rx_ant\nr_rx_hat = tf.reduce_mean(tf.matmul(h, h, adjoint_b=True), 0)/num_tx_ant\n# Test that the empirical results match the theory\nassert(np.allclose(r_tx, r_tx_hat, atol=1e-2))\nassert(np.allclose(r_rx, r_rx_hat, atol=1e-2))\n```\n\n\nNow, we can transmit the same symbols `x` over the channel with spatial correlation and compute the SER:\n\n\n```python\ny, h = channel([x, no])\nx_hat, no_eff = lmmse_equalizer(y, h, s)\nx_ind_hat = symbol_demapper([x_hat, no])\ncompute_ser(x_ind, x_ind_hat)\n```\n```python\n<tf.Tensor: shape=(), dtype=float64, numpy=0.115234375>\n```\n\n\nThe result cleary show the negative effect of spatial correlation in this setting. You can play around with the `a` parameter defining the exponential correlation matrices and see its impact on the SER."
"## Extension to channel coding\n\nSo far, we have simulated uncoded symbol transmissions. With a few lines of additional code, we can extend what we have done to coded BER simulations. We need the following additional components:\n\n\n```python\nn = 1024 # codeword length\nk = 512  # number of information bits per codeword\ncoderate = k/n # coderate\nbatch_size = 32\nbinary_source = BinarySource()\nencoder = LDPC5GEncoder(k, n)\ndecoder = LDPC5GDecoder(encoder, hard_out=True)\nmapper = Mapper(\"qam\", num_bits_per_symbol)\ndemapper = Demapper(\"app\", \"qam\", num_bits_per_symbol)\n```\n\n\nNext we need to generate random QAM symbols through mapping of coded bits. Reshaping is required to bring `x` into the needed shape.\n\n\n```python\nb = binary_source([batch_size, num_tx_ant, k])\nc = encoder(b)\nx = mapper(c)\nx_ind = symbol_demapper([x, no]) # Get symbol indices for SER computation later on\nshape = tf.shape(x)\nx = tf.reshape(x, [-1, num_tx_ant])\nprint(x.shape)\n```\n\n\n```python\n(8192, 4)\n```\n\n\nWe will now transmit the symbols over the channel:\n\n\n```python\ny, h = channel([x, no])\nx_hat, no_eff = lmmse_equalizer(y, h, s)\n```\n\n\nAnd then demap the symbols to LLRs prior to decoding them. Note that we need to bring `x_hat` and `no_eff` back to the desired shape for decoding.\n\n\n```python\nx_ind_hat.shape\n```\n\n```python\nTensorShape([1024, 4])\n```\n\n```python\nx_hat = tf.reshape(x_hat, shape)\nno_eff = tf.reshape(no_eff, shape)\nllr = demapper([x_hat, no_eff])\nb_hat = decoder(llr)\nx_ind_hat = symbol_demapper([x_hat, no])\nber = compute_ber(b, b_hat).numpy()\nprint(\"Uncoded SER : {}\".format(compute_ser(x_ind, x_ind_hat)))\nprint(\"Coded BER : {}\".format(compute_ber(b, b_hat)))\n```\n\n\n```python\nUncoded SER : 0.1219482421875\nCoded BER : 0.0\n```\n\n\nDespite the fairly high SER, the BER is very low, thanks to the channel code."
"### BER simulations using a Keras model\n\nNext, we will wrap everything that we have done so far in a Keras model for convenient BER simulations and comparison of model parameters. Note that we use the `@tf.function(jit_compile=True)` decorator which will speed-up the simulations tremendously. See [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function) for further information. You need to enable the [sionna.config.xla_compat](https://nvlabs.github.io/sionna/api/config.html#sionna.Config.xla_compat) feature prior to executing the model.\n\n\n```python\nsionna.config.xla_compat=True\nclass Model(tf.keras.Model):\n    def __init__(self, spatial_corr=None):\n        super().__init__()\n        self.n = 1024\n        self.k = 512\n        self.coderate = self.k/self.n\n        self.num_bits_per_symbol = 4\n        self.num_tx_ant = 4\n        self.num_rx_ant = 16\n        self.binary_source = BinarySource()\n        self.encoder = LDPC5GEncoder(self.k, self.n)\n        self.mapper = Mapper(\"qam\", self.num_bits_per_symbol)\n        self.demapper = Demapper(\"app\", \"qam\", self.num_bits_per_symbol)\n        self.decoder = LDPC5GDecoder(self.encoder, hard_out=True)\n        self.channel = FlatFadingChannel(self.num_tx_ant,\n                                         self.num_rx_ant,\n                                         spatial_corr=spatial_corr,\n                                         add_awgn=True,\n                                         return_channel=True)\n    @tf.function(jit_compile=True)\n    def call(self, batch_size, ebno_db):\n        b = self.binary_source([batch_size, self.num_tx_ant, self.k])\n        c = self.encoder(b)\n        x = self.mapper(c)\n        shape = tf.shape(x)\n        x = tf.reshape(x, [-1, self.num_tx_ant])\n        no = ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate)\n        no *= np.sqrt(self.num_rx_ant)\n        y, h = self.channel([x, no])\n        s = tf.complex(no*tf.eye(self.num_rx_ant, self.num_rx_ant), 0.0)\n        x_hat, no_eff = lmmse_equalizer(y, h, s)\n        x_hat = tf.reshape(x_hat, shape)\n        no_eff = tf.reshape(no_eff, shape)\n        llr = self.demapper([x_hat, no_eff])\n        b_hat = self.decoder(llr)\n        return b,  b_hat\n```"
"We can now instantiate different version of this model and use the `PlotBer` class for easy Monte-Carlo simulations.\n\n\n```python\nber_plot = PlotBER()\n```\n\n```python\nmodel1 = Model()\nber_plot.simulate(model1,\n        np.arange(-2.5, 0.25, 0.25),\n        batch_size=4096,\n        max_mc_iter=1000,\n        num_target_block_errors=100,\n        legend=\"Uncorrelated\",\n        show_fig=False);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n     -2.5 | 1.1113e-01 | 9.1742e-01 |      932264 |     8388608 |        15031 |       16384 |         6.7 |reached target block errors\n    -2.25 | 7.9952e-02 | 7.8546e-01 |      670690 |     8388608 |        12869 |       16384 |         0.2 |reached target block errors\n     -2.0 | 5.0242e-02 | 5.9320e-01 |      421458 |     8388608 |         9719 |       16384 |         0.2 |reached target block errors\n    -1.75 | 2.6710e-02 | 3.7158e-01 |      224061 |     8388608 |         6088 |       16384 |         0.2 |reached target block errors\n     -1.5 | 1.1970e-02 | 1.9104e-01 |      100415 |     8388608 |         3130 |       16384 |         0.2 |reached target block errors\n    -1.25 | 4.6068e-03 | 7.8186e-02 |       38645 |     8388608 |         1281 |       16384 |         0.2 |reached target block errors\n     -1.0 | 1.2861e-03 | 2.5818e-02 |       10789 |     8388608 |          423 |       16384 |         0.2 |reached target block errors\n    -0.75 | 2.7883e-04 | 7.8735e-03 |        2339 |     8388608 |          129 |       16384 |         0.2 |reached target block errors\n     -0.5 | 8.5314e-05 | 2.2990e-03 |        2147 |    25165824 |          113 |       49152 |         0.7 |reached target block errors\n    -0.25 | 1.2082e-05 | 3.1738e-04 |        2027 |   167772160 |          104 |      327680 |         4.4 |reached target block errors\n      0.0 | 1.9351e-06 | 7.1681e-05 |        1396 |   721420288 |          101 |     1409024 |        19.0 |reached target block errors\n```"
"```python\nr_tx = exp_corr_mat(0.4, num_tx_ant)\nr_rx = exp_corr_mat(0.7, num_rx_ant)\nmodel2 = Model(KroneckerModel(r_tx, r_rx))\nber_plot.simulate(model2,\n        np.arange(0,2.6,0.25),\n        batch_size=4096,\n        max_mc_iter=1000,\n        num_target_block_errors=200,\n        legend=\"Kronecker model\");\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      0.0 | 7.2249e-02 | 7.1844e-01 |      606070 |     8388608 |        11771 |       16384 |         4.9 |reached target block errors\n     0.25 | 4.7019e-02 | 5.4126e-01 |      394421 |     8388608 |         8868 |       16384 |         0.2 |reached target block errors\n      0.5 | 2.7080e-02 | 3.5498e-01 |      227161 |     8388608 |         5816 |       16384 |         0.2 |reached target block errors\n     0.75 | 1.3981e-02 | 2.0380e-01 |      117280 |     8388608 |         3339 |       16384 |         0.2 |reached target block errors\n      1.0 | 6.1550e-03 | 9.6802e-02 |       51632 |     8388608 |         1586 |       16384 |         0.2 |reached target block errors\n     1.25 | 2.2970e-03 | 4.0100e-02 |       19269 |     8388608 |          657 |       16384 |         0.2 |reached target block errors\n      1.5 | 7.9966e-04 | 1.4282e-02 |        6708 |     8388608 |          234 |       16384 |         0.2 |reached target block errors\n     1.75 | 2.4907e-04 | 4.5166e-03 |        6268 |    25165824 |          222 |       49152 |         0.7 |reached target block errors\n      2.0 | 7.0466e-05 | 1.3767e-03 |        5320 |    75497472 |          203 |      147456 |         2.0 |reached target block errors\n     2.25 | 1.4114e-05 | 3.0670e-04 |        4736 |   335544320 |          201 |      655360 |         9.0 |reached target block errors\n      2.5 | 2.8881e-06 | 7.0971e-05 |        4167 |  1442840576 |          200 |     2818048 |        38.7 |reached target block errors\n```"

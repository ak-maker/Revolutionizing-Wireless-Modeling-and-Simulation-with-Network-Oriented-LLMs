"# Mapping\n\nThis module contains classes and functions related to mapping\nof bits to constellation symbols and demapping of soft-symbols\nto log-likelihood ratios (LLRs). The key components are the\n[`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation), [`Mapper`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Mapper),\nand [`Demapper`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Demapper). A [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\ncan be made trainable to enable learning of geometric shaping."
"### Constellation\n\n`class` `sionna.mapping.``Constellation`(*`constellation_type`*, *`num_bits_per_symbol`*, *`initial_value``=``None`*, *`normalize``=``True`*, *`center``=``False`*, *`trainable``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#Constellation)\n\nConstellation that can be used by a (de)mapper.\n\nThis class defines a constellation, i.e., a complex-valued vector of\nconstellation points. A constellation can be trainable. The binary\nrepresentation of the index of an element of this vector corresponds\nto the bit label of the constellation point. This implicit bit\nlabeling is used by the `Mapper` and `Demapper` classes.\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, the constellation points are randomly initialized\nif no `initial_value` is provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **initial_value** ($[2^\\text{num_bits_per_symbol}]$, NumPy array or Tensor)  Initial values of the constellation points. If `normalize` or\n`center` are <cite>True</cite>, the initial constellation might be changed.\n- **normalize** (*bool*)  If <cite>True</cite>, the constellation is normalized to have unit power.\nDefaults to <cite>True</cite>.\n- **center** (*bool*)  If <cite>True</cite>, the constellation is ensured to have zero mean.\nDefaults to <cite>False</cite>.\n- **trainable** (*bool*)  If <cite>True</cite>, the constellation points are trainable variables.\nDefaults to <cite>False</cite>.\n- **dtype** (*[**tf.complex64**, **tf.complex128**]**, **tf.DType*)  The dtype of the constellation.\n\n\nOutput\n\n$[2^\\text{num_bits_per_symbol}]$, `dtype`  The constellation.\n\n\n**Note**\n\nOne can create a trainable PAM/QAM constellation. This is\nequivalent to creating a custom trainable constellation which is\ninitialized with PAM/QAM constellation points.\n\n`property` `center`\n\nIndicates if the constellation is centered.\n\n\n`create_or_check_constellation`(*`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`dtype``=``tf.complex64`*)[`[source]`](../_modules/sionna/mapping.html#Constellation.create_or_check_constellation)\n\nStatic method for conviently creating a constellation object or checking that an existing one\nis consistent with requested settings.\n\nIf `constellation` is <cite>None</cite>, then this method creates a [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nobject of type `constellation_type` and with `num_bits_per_symbol` bits per symbol.\nOtherwise, this method checks that <cite>constellation</cite> is consistent with `constellation_type` and\n`num_bits_per_symbol`. If it is, `constellation` is returned. Otherwise, an assertion is raised.\nInput\n\n- **constellation_type** (*One of [qam, pam, custom], str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** (*Constellation*)  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or\n<cite>None</cite>. In the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n\n\nOutput\n\n[`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)  A constellation object.\n\n\n`property` `normalize`\n\nIndicates if the constellation is normalized or not.\n\n\n`property` `num_bits_per_symbol`\n\nThe number of bits per constellation symbol.\n\n\n`property` `points`\n\nThe (possibly) centered and normalized constellation points.\n\n\n`show`(*`labels``=``True`*, *`figsize``=``(7,` `7)`*)[`[source]`](../_modules/sionna/mapping.html#Constellation.show)\n\nGenerate a scatter-plot of the constellation.\nInput\n\n- **labels** (*bool*)  If <cite>True</cite>, the bit labels will be drawn next to each constellation\npoint. Defaults to <cite>True</cite>.\n- **figsize** (*Two-element Tuple, float*)  Width and height in inches. Defaults to <cite>(7,7)</cite>.\n\n\nOutput\n\n*matplotlib.figure.Figure*  A handle to a matplot figure object."
"### qam\n\n`sionna.mapping.``qam`(*`num_bits_per_symbol`*, *`normalize``=``True`*)[`[source]`](../_modules/sionna/mapping.html#qam)\n\nGenerates a QAM constellation.\n\nThis function generates a complex-valued vector, where each element is\na constellation point of an M-ary QAM constellation. The bit\nlabel of the `n` th point is given by the length-`num_bits_per_symbol`\nbinary represenation of `n`.\nInput\n\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation point.\nMust be a multiple of two, e.g., 2, 4, 6, 8, etc.\n- **normalize** (*bool*)  If <cite>True</cite>, the constellation is normalized to have unit power.\nDefaults to <cite>True</cite>.\n\n\nOutput\n\n$[2^{\\text{num_bits_per_symbol}}]$, np.complex64  The QAM constellation.\n\n\n**Note**\n\nThe bit label of the nth constellation point is given by the binary\nrepresentation of its position within the array and can be obtained\nthrough `np.binary_repr(n,` `num_bits_per_symbol)`.\n\nThe normalization factor of a QAM constellation is given in\nclosed-form as:\n\n$$\n\\sqrt{\\frac{1}{2^{n-2}}\\sum_{i=1}^{2^{n-1}}(2i-1)^2}\n$$\n\nwhere $n= \\text{num_bits_per_symbol}/2$ is the number of bits\nper dimension.\n\nThis algorithm is a recursive implementation of the expressions found in\nSection 5.1 of [[3GPPTS38211]](https://nvlabs.github.io/sionna/api/mapping.html#gppts38211). It is used in the 5G standard."
"### pam\n\n`sionna.mapping.``pam`(*`num_bits_per_symbol`*, *`normalize``=``True`*)[`[source]`](../_modules/sionna/mapping.html#pam)\n\nGenerates a PAM constellation.\n\nThis function generates a real-valued vector, where each element is\na constellation point of an M-ary PAM constellation. The bit\nlabel of the `n` th point is given by the length-`num_bits_per_symbol`\nbinary represenation of `n`.\nInput\n\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation point.\nMust be positive.\n- **normalize** (*bool*)  If <cite>True</cite>, the constellation is normalized to have unit power.\nDefaults to <cite>True</cite>.\n\n\nOutput\n\n$[2^{\\text{num_bits_per_symbol}}]$, np.float32  The PAM constellation.\n\n\n**Note**\n\nThe bit label of the nth constellation point is given by the binary\nrepresentation of its position within the array and can be obtained\nthrough `np.binary_repr(n,` `num_bits_per_symbol)`.\n\nThe normalization factor of a PAM constellation is given in\nclosed-form as:\n\n$$\n\\sqrt{\\frac{1}{2^{n-1}}\\sum_{i=1}^{2^{n-1}}(2i-1)^2}\n$$\n\nwhere $n= \\text{num_bits_per_symbol}$ is the number of bits\nper symbol.\n\nThis algorithm is a recursive implementation of the expressions found in\nSection 5.1 of [[3GPPTS38211]](https://nvlabs.github.io/sionna/api/mapping.html#gppts38211). It is used in the 5G standard."
"### pam_gray\n\n`sionna.mapping.``pam_gray`(*`b`*)[`[source]`](../_modules/sionna/mapping.html#pam_gray)\n\nMaps a vector of bits to a PAM constellation points with Gray labeling.\n\nThis recursive function maps a binary vector to Gray-labelled PAM\nconstellation points. It can be used to generated QAM constellations.\nThe constellation is not normalized.\nInput\n\n**b** (*[n], NumPy array*)  Tensor with with binary entries.\n\nOutput\n\n*signed int*  The PAM constellation point taking values in\n$\\{\\pm 1,\\pm 3,\\dots,\\pm (2^n-1)\\}$.\n\n\n**Note**\n\nThis algorithm is a recursive implementation of the expressions found in\nSection 5.1 of [[3GPPTS38211]](https://nvlabs.github.io/sionna/api/mapping.html#gppts38211). It is used in the 5G standard."
"## Mapper\n\n`class` `sionna.mapping.``Mapper`(*`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`return_indices``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#Mapper)\n\nMaps binary tensors to points of a constellation.\n\nThis class defines a layer that maps a tensor of binary values\nto a tensor of points from a provided constellation.\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or\n<cite>None</cite>. In the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **return_indices** (*bool*)  If enabled, symbol indices are additionally returned.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**]**, **tf.DType*)  The output dtype. Defaults to tf.complex64.\n\n\nInput\n\n*[, n], tf.float or tf.int*  Tensor with with binary entries.\n\nOutput\n\n- *[,n/Constellation.num_bits_per_symbol], tf.complex*  The mapped constellation symbols.\n- *[,n/Constellation.num_bits_per_symbol], tf.int32*  The symbol indices corresponding to the constellation symbols.\nOnly returned if `return_indices` is set to True.\n\n\n**Note**\n\nThe last input dimension must be an integer multiple of the\nnumber of bits per constellation symbol.\n\n`property` `constellation`\n\nThe Constellation used by the Mapper."
"### Demapper\n\n`class` `sionna.mapping.``Demapper`(*`demapping_method`*, *`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`hard_out``=``False`*, *`with_prior``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#Demapper)\n\nComputes log-likelihood ratios (LLRs) or hard-decisions on bits\nfor a tensor of received symbols.\nIf the flag `with_prior` is set, prior knowledge on the bits is assumed to be available.\n\nThis class defines a layer implementing different demapping\nfunctions. All demapping functions are fully differentiable when soft-decisions\nare computed.\nParameters\n\n- **demapping_method** (*One of** [**\"app\"**, **\"maxlog\"**]**, **str*)  The demapping method used.\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or <cite>None</cite>.\nIn the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **hard_out** (*bool*)  If <cite>True</cite>, the demapper provides hard-decided bits instead of soft-values.\nDefaults to <cite>False</cite>.\n- **with_prior** (*bool*)  If <cite>True</cite>, it is assumed that prior knowledge on the bits is available.\nThis prior information is given as LLRs as an additional input to the layer.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**] **tf.DType** (**dtype**)*)  The dtype of <cite>y</cite>. Defaults to tf.complex64.\nThe output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n\nInput\n\n- **(y,no) or (y, prior, no)**  Tuple:\n- **y** (*[,n], tf.complex*)  The received symbols.\n- **prior** (*[num_bits_per_symbol] or [,num_bits_per_symbol], tf.float*)  Prior for every bit as LLRs.\nIt can be provided either as a tensor of shape <cite>[num_bits_per_symbol]</cite> for the\nentire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_bits_per_symbol]</cite>.\nOnly required if the `with_prior` flag is set.\n- **no** (*Scalar or [,n], tf.float*)  The noise variance estimate. It can be provided either as scalar\nfor the entire input batch or as a tensor that is broadcastable to\n`y`.\n\n\nOutput\n\n*[,n*num_bits_per_symbol], tf.float*  LLRs or hard-decisions for every bit.\n\n\n**Note**\n\nWith the app demapping method, the LLR for the $i\\text{th}$ bit\nis computed according to\n\n$$\nLLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert y,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert y,\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)\n$$\n\nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the\nsets of constellation points for which the $i\\text{th}$ bit is\nequal to 1 and 0, respectively. $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$\nis the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to\na constellation point and is set to $\\mathbf{0}$ if no prior knowledge is assumed to be available,\nand $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:\n\n$$\n\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n$$\n\nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is\nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.\n\nWith the maxlog demapping method, LLRs for the $i\\text{th}$ bit\nare approximated like\n\n$$\n\\begin{split}\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)\\\\\n        &= \\max_{c\\in\\mathcal{C}_{i,0}}\n            \\left(\\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right)-\\frac{|y-c|^2}{N_o}\\right) -\n         \\max_{c\\in\\mathcal{C}_{i,1}}\\left( \\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right) - \\frac{|y-c|^2}{N_o}\\right)\n        .\n\\end{align}\\end{split}\n$$"
"### DemapperWithPrior\n\n`class` `sionna.mapping.``DemapperWithPrior`(*`demapping_method`*, *`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`hard_out``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#DemapperWithPrior)\n\nComputes log-likelihood ratios (LLRs) or hard-decisions on bits\nfor a tensor of received symbols, assuming that prior knowledge on the bits is available.\n\nThis class defines a layer implementing different demapping\nfunctions. All demapping functions are fully differentiable when soft-decisions\nare computed.\n\nThis class is deprecated as the functionality has been integrated\ninto [`Demapper`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Demapper).\nParameters\n\n- **demapping_method** (*One of** [**\"app\"**, **\"maxlog\"**]**, **str*)  The demapping method used.\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or <cite>None</cite>.\nIn the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **hard_out** (*bool*)  If <cite>True</cite>, the demapper provides hard-decided bits instead of soft-values.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**] **tf.DType** (**dtype**)*)  The dtype of <cite>y</cite>. Defaults to tf.complex64.\nThe output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n\nInput\n\n- **(y, prior, no)**  Tuple:\n- **y** (*[,n], tf.complex*)  The received symbols.\n- **prior** (*[num_bits_per_symbol] or [,num_bits_per_symbol], tf.float*)  Prior for every bit as LLRs.\nIt can be provided either as a tensor of shape <cite>[num_bits_per_symbol]</cite> for the\nentire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_bits_per_symbol]</cite>.\n- **no** (*Scalar or [,n], tf.float*)  The noise variance estimate. It can be provided either as scalar\nfor the entire input batch or as a tensor that is broadcastable to\n`y`.\n\n\nOutput\n\n*[,n*num_bits_per_symbol], tf.float*  LLRs or hard-decisions for every bit.\n\n\n**Note**\n\nWith the app demapping method, the LLR for the $i\\text{th}$ bit\nis computed according to\n\n$$\nLLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert y,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert y,\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)\n$$\n\nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the\nsets of constellation points for which the $i\\text{th}$ bit is\nequal to 1 and 0, respectively. $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$\nis the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to\na constellation point,\nand $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:\n\n$$\n\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n$$\n\nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is\nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.\n\nWith the maxlog demapping method, LLRs for the $i\\text{th}$ bit\nare approximated like\n\n$$\n\\begin{split}\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)\\\\\n        &= \\max_{c\\in\\mathcal{C}_{i,0}}\n            \\left(\\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right)-\\frac{|y-c|^2}{N_o}\\right) -\n         \\max_{c\\in\\mathcal{C}_{i,1}}\\left( \\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right) - \\frac{|y-c|^2}{N_o}\\right)\n        .\n\\end{align}\\end{split}\n$$"
"### SymbolDemapper\n\n`class` `sionna.mapping.``SymbolDemapper`(*`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`hard_out``=``False`*, *`with_prior``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolDemapper)\n\nComputes normalized log-probabilities (logits) or hard-decisions on symbols\nfor a tensor of received symbols.\nIf the `with_prior` flag is set, prior knowldge on the transmitted constellation points is assumed to be available.\nThe demapping function is fully differentiable when soft-values are\ncomputed.\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or <cite>None</cite>.\nIn the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **hard_out** (*bool*)  If <cite>True</cite>, the demapper provides hard-decided symbols instead of soft-values.\nDefaults to <cite>False</cite>.\n- **with_prior** (*bool*)  If <cite>True</cite>, it is assumed that prior knowledge on the constellation points is available.\nThis prior information is given as log-probabilities (logits) as an additional input to the layer.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**] **tf.DType** (**dtype**)*)  The dtype of <cite>y</cite>. Defaults to tf.complex64.\nThe output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n\nInput\n\n- **(y, no) or (y, prior, no)**  Tuple:\n- **y** (*[,n], tf.complex*)  The received symbols.\n- **prior** (*[num_points] or [,num_points], tf.float*)  Prior for every symbol as log-probabilities (logits).\nIt can be provided either as a tensor of shape <cite>[num_points]</cite> for the\nentire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_points]</cite>.\nOnly required if the `with_prior` flag is set.\n- **no** (*Scalar or [,n], tf.float*)  The noise variance estimate. It can be provided either as scalar\nfor the entire input batch or as a tensor that is broadcastable to\n`y`.\n\n\nOutput\n\n*[,n, num_points] or [,n], tf.float*  A tensor of shape <cite>[,n, num_points]</cite> of logits for every constellation\npoint if <cite>hard_out</cite> is set to <cite>False</cite>.\nOtherwise, a tensor of shape <cite>[,n]</cite> of hard-decisions on the symbols.\n\n\n**Note**\n\nThe normalized log-probability for the constellation point $c$ is computed according to\n\n$$\n\\ln\\left(\\Pr\\left(c \\lvert y,\\mathbf{p}\\right)\\right) = \\ln\\left( \\frac{\\exp\\left(-\\frac{|y-c|^2}{N_0} + p_c \\right)}{\\sum_{c'\\in\\mathcal{C}} \\exp\\left(-\\frac{|y-c'|^2}{N_0} + p_{c'} \\right)} \\right)\n$$\n\nwhere $\\mathcal{C}$ is the set of constellation points used for modulation,\nand $\\mathbf{p} = \\left\\{p_c \\lvert c \\in \\mathcal{C}\\right\\}$ the prior information on constellation points given as log-probabilities\nand which is set to $\\mathbf{0}$ if no prior information on the constellation points is assumed to be available."
"### SymbolDemapperWithPrior\n\n`class` `sionna.mapping.``SymbolDemapperWithPrior`(*`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`hard_out``=``False`*, *`dtype``=``tf.complex64`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolDemapperWithPrior)\n\nComputes normalized log-probabilities (logits) or hard-decisions on symbols\nfor a tensor of received symbols, assuming that prior knowledge on the constellation points is available.\nThe demapping function is fully differentiable when soft-values are\ncomputed.\n\nThis class is deprecated as the functionality has been integrated\ninto [`SymbolDemapper`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.SymbolDemapper).\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or <cite>None</cite>.\nIn the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **hard_out** (*bool*)  If <cite>True</cite>, the demapper provides hard-decided symbols instead of soft-values.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.complex64**, **tf.complex128**] **tf.DType** (**dtype**)*)  The dtype of <cite>y</cite>. Defaults to tf.complex64.\nThe output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n\nInput\n\n- **(y, prior, no)**  Tuple:\n- **y** (*[,n], tf.complex*)  The received symbols.\n- **prior** (*[num_points] or [,num_points], tf.float*)  Prior for every symbol as log-probabilities (logits).\nIt can be provided either as a tensor of shape <cite>[num_points]</cite> for the\nentire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_points]</cite>.\n- **no** (*Scalar or [,n], tf.float*)  The noise variance estimate. It can be provided either as scalar\nfor the entire input batch or as a tensor that is broadcastable to\n`y`.\n\n\nOutput\n\n*[,n, num_points] or [,n], tf.float*  A tensor of shape <cite>[,n, num_points]</cite> of logits for every constellation\npoint if <cite>hard_out</cite> is set to <cite>False</cite>.\nOtherwise, a tensor of shape <cite>[,n]</cite> of hard-decisions on the symbols.\n\n\n**Note**\n\nThe normalized log-probability for the constellation point $c$ is computed according to\n\n$$\n\\ln\\left(\\Pr\\left(c \\lvert y,\\mathbf{p}\\right)\\right) = \\ln\\left( \\frac{\\exp\\left(-\\frac{|y-c|^2}{N_0} + p_c \\right)}{\\sum_{c'\\in\\mathcal{C}} \\exp\\left(-\\frac{|y-c'|^2}{N_0} + p_{c'} \\right)} \\right)\n$$\n\nwhere $\\mathcal{C}$ is the set of constellation points used for modulation,\nand $\\mathbf{p} = \\left\\{p_c \\lvert c \\in \\mathcal{C}\\right\\}$ the prior information on constellation points given as log-probabilities."
"### SymbolLogits2LLRs\n\n`class` `sionna.mapping.``SymbolLogits2LLRs`(*`method`*, *`num_bits_per_symbol`*, *`hard_out``=``False`*, *`with_prior``=``False`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolLogits2LLRs)\n\nComputes log-likelihood ratios (LLRs) or hard-decisions on bits\nfrom a tensor of logits (i.e., unnormalized log-probabilities) on constellation points.\nIf the flag `with_prior` is set, prior knowledge on the bits is assumed to be available.\nParameters\n\n- **method** (*One of** [**\"app\"**, **\"maxlog\"**]**, **str*)  The method used for computing the LLRs.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **hard_out** (*bool*)  If <cite>True</cite>, the layer provides hard-decided bits instead of soft-values.\nDefaults to <cite>False</cite>.\n- **with_prior** (*bool*)  If <cite>True</cite>, it is assumed that prior knowledge on the bits is available.\nThis prior information is given as LLRs as an additional input to the layer.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.float32**, **tf.float64**] **tf.DType** (**dtype**)*)  The dtype for the input and output.\nDefaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **logits or (logits, prior)**  Tuple:\n- **logits** (*[,n, num_points], tf.float*)  Logits on constellation points.\n- **prior** (*[num_bits_per_symbol] or [n, num_bits_per_symbol], tf.float*)  Prior for every bit as LLRs.\nIt can be provided either as a tensor of shape <cite>[num_bits_per_symbol]</cite>\nfor the entire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_bits_per_symbol]</cite>.\nOnly required if the `with_prior` flag is set.\n\n\nOutput\n\n*[,n, num_bits_per_symbol], tf.float*  LLRs or hard-decisions for every bit.\n\n\n**Note**\n\nWith the app method, the LLR for the $i\\text{th}$ bit\nis computed according to\n\n$$\nLLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert \\mathbf{z},\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert \\mathbf{z},\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }\\right)\n$$\n\nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the\nsets of $2^K$ constellation points for which the $i\\text{th}$ bit is\nequal to 1 and 0, respectively. $\\mathbf{z} = \\left[z_{c_0},\\dots,z_{c_{2^K-1}}\\right]$ is the vector of logits on the constellation points, $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$\nis the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to\na constellation point and is set to $\\mathbf{0}$ if no prior knowledge is assumed to be available,\nand $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:\n\n$$\n\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert\\mathbf{p} \\right)\n= \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n$$\n\nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is\nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.\n\nWith the maxlog method, LLRs for the $i\\text{th}$ bit\nare approximated like\n\n$$\n\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }\\right)\n        .\n\\end{align}\n$$"
"### LLRs2SymbolLogits\n\n`class` `sionna.mapping.``LLRs2SymbolLogits`(*`num_bits_per_symbol`*, *`hard_out``=``False`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#LLRs2SymbolLogits)\n\nComputes logits (i.e., unnormalized log-probabilities) or hard decisions\non constellation points from a tensor of log-likelihood ratios (LLRs) on bits.\nParameters\n\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **hard_out** (*bool*)  If <cite>True</cite>, the layer provides hard-decided constellation points instead of soft-values.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.float32**, **tf.float64**] **tf.DType** (**dtype**)*)  The dtype for the input and output.\nDefaults to <cite>tf.float32</cite>.\n\n\nInput\n\n**llrs** (*[, n, num_bits_per_symbol], tf.float*)  LLRs for every bit.\n\nOutput\n\n*[,n, num_points], tf.float or [, n], tf.int32*  Logits or hard-decisions on constellation points.\n\n\n**Note**\n\nThe logit for the constellation $c$ point\nis computed according to\n\n$$\n\\begin{split}\\begin{align}\n    \\log{\\left(\\Pr\\left(c\\lvert LLRs \\right)\\right)}\n        &= \\log{\\left(\\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert LLRs \\right)\\right)}\\\\\n        &= \\log{\\left(\\prod_{k=0}^{K-1} \\text{sigmoid}\\left(LLR(k) \\ell(c)_k\\right)\\right)}\\\\\n        &= \\sum_{k=0}^{K-1} \\log{\\left(\\text{sigmoid}\\left(LLR(k) \\ell(c)_k\\right)\\right)}\n\\end{align}\\end{split}\n$$\n\nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is\nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$."
"### SymbolLogits2LLRsWithPrior\n\n`class` `sionna.mapping.``SymbolLogits2LLRsWithPrior`(*`method`*, *`num_bits_per_symbol`*, *`hard_out``=``False`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolLogits2LLRsWithPrior)\n\nComputes log-likelihood ratios (LLRs) or hard-decisions on bits\nfrom a tensor of logits (i.e., unnormalized log-probabilities) on constellation points,\nassuming that prior knowledge on the bits is available.\n\nThis class is deprecated as the functionality has been integrated\ninto [`SymbolLogits2LLRs`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.SymbolLogits2LLRs).\nParameters\n\n- **method** (*One of** [**\"app\"**, **\"maxlog\"**]**, **str*)  The method used for computing the LLRs.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **hard_out** (*bool*)  If <cite>True</cite>, the layer provides hard-decided bits instead of soft-values.\nDefaults to <cite>False</cite>.\n- **dtype** (*One of** [**tf.float32**, **tf.float64**] **tf.DType** (**dtype**)*)  The dtype for the input and output.\nDefaults to <cite>tf.float32</cite>.\n\n\nInput\n\n- **(logits, prior)**  Tuple:\n- **logits** (*[,n, num_points], tf.float*)  Logits on constellation points.\n- **prior** (*[num_bits_per_symbol] or [n, num_bits_per_symbol], tf.float*)  Prior for every bit as LLRs.\nIt can be provided either as a tensor of shape <cite>[num_bits_per_symbol]</cite> for the\nentire input batch, or as a tensor that is broadcastable\nto <cite>[, n, num_bits_per_symbol]</cite>.\n\n\nOutput\n\n*[,n, num_bits_per_symbol], tf.float*  LLRs or hard-decisions for every bit.\n\n\n**Note**\n\nWith the app method, the LLR for the $i\\text{th}$ bit\nis computed according to\n\n$$\nLLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert \\mathbf{z},\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert \\mathbf{z},\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }\\right)\n$$\n\nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the\nsets of $2^K$ constellation points for which the $i\\text{th}$ bit is\nequal to 1 and 0, respectively. $\\mathbf{z} = \\left[z_{c_0},\\dots,z_{c_{2^K-1}}\\right]$ is the vector of logits on the constellation points, $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$\nis the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to\na constellation point,\nand $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:\n\n$$\n\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert\\mathbf{p} \\right)\n= \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n$$\n\nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is\nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.\n\nWith the maxlog method, LLRs for the $i\\text{th}$ bit\nare approximated like\n\n$$\n\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }\\right)\n        .\n\\end{align}\n$$"
"### SymbolLogits2Moments\n\n`class` `sionna.mapping.``SymbolLogits2Moments`(*`constellation_type``=``None`*, *`num_bits_per_symbol``=``None`*, *`constellation``=``None`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolLogits2Moments)\n\nComputes the mean and variance of a constellation from logits (unnormalized log-probabilities) on the\nconstellation points.\n\nMore precisely, given a constellation $\\mathcal{C} = \\left[ c_0,\\dots,c_{N-1} \\right]$ of size $N$, this layer computes the mean and variance\naccording to\n\n$$\n\\begin{split}\\begin{align}\n    \\mu &= \\sum_{n = 0}^{N-1} c_n \\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right)\\\\\n    \\nu &= \\sum_{n = 0}^{N-1} \\left( c_n - \\mu \\right)^2 \\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right)\n\\end{align}\\end{split}\n$$\n\nwhere $\\mathbf{\\ell} = \\left[ \\ell_0, \\dots, \\ell_{N-1} \\right]$ are the logits, and\n\n$$\n\\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right) = \\frac{\\exp \\left( \\ell_n \\right)}{\\sum_{i=0}^{N-1} \\exp \\left( \\ell_i \\right) }.\n$$\n\nParameters\n\n- **constellation_type** (*One of** [**\"qam\"**, **\"pam\"**, **\"custom\"**]**, **str*)  For custom, an instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation)\nmust be provided.\n- **num_bits_per_symbol** (*int*)  The number of bits per constellation symbol, e.g., 4 for QAM16.\nOnly required for `constellation_type` in [qam, pam].\n- **constellation** ()  An instance of [`Constellation`](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.Constellation) or <cite>None</cite>.\nIn the latter case, `constellation_type`\nand `num_bits_per_symbol` must be provided.\n- **dtype** (*One of** [**tf.float32**, **tf.float64**] **tf.DType** (**dtype**)*)  The dtype for the input and output.\nDefaults to <cite>tf.float32</cite>.\n\n\nInput\n\n**logits** (*[,n, num_points], tf.float*)  Logits on constellation points.\n\nOutput\n\n- **mean** (*[,n], tf.float*)  Mean of the constellation.\n- **var** (*[,n], tf.float*)  Variance of the constellation"
"### SymbolInds2Bits\n\n`class` `sionna.mapping.``SymbolInds2Bits`(*`num_bits_per_symbol`*, *`dtype``=``tf.float32`*, *`**``kwargs`*)[`[source]`](../_modules/sionna/mapping.html#SymbolInds2Bits)\n\nTransforms symbol indices to their binary representations.\nParameters\n\n- **num_bits_per_symbol** (*int*)  Number of bits per constellation symbol\n- **dtype** (*tf.DType*)  Output dtype. Defaults to <cite>tf.float32</cite>.\n\n\nInput\n\n*Tensor, tf.int*  Symbol indices\n\nOutput\n\n*input.shape + [num_bits_per_symbol], dtype*  Binary representation of symbol indices"
"### PAM2QAM\n\n`class` `sionna.mapping.``PAM2QAM`(*`num_bits_per_symbol`*, *`hard_in_out``=``True`*)[`[source]`](../_modules/sionna/mapping.html#PAM2QAM)\n\nTransforms PAM symbol indices/logits to QAM symbol indices/logits.\n\nFor two PAM constellation symbol indices or logits, corresponding to\nthe real and imaginary components of a QAM constellation,\ncompute the QAM symbol index or logits.\nParameters\n\n- **num_bits_per_symbol** (*int*)  Number of bits per QAM constellation symbol, e.g., 4 for QAM16\n- **hard_in_out** (*bool*)  Determines if inputs and outputs are indices or logits over\nconstellation symbols.\nDefaults to <cite>True</cite>.\n\n\nInput\n\n- **pam1** (*Tensor, tf.int, or [,2**(num_bits_per_symbol/2)], tf.float*)  Indices or logits for the first PAM constellation\n- **pam2** (*Tensor, tf.int, or [,2**(num_bits_per_symbol/2)], tf.float*)  Indices or logits for the second PAM constellation\n\n\nOutput\n\n**qam** (*Tensor, tf.int, or [,2**num_bits_per_symbol], tf.float*)  Indices or logits for the corresponding QAM constellation"
"### QAM2PAM\n\n`class` `sionna.mapping.``QAM2PAM`(*`num_bits_per_symbol`*)[`[source]`](../_modules/sionna/mapping.html#QAM2PAM)\n\nTransforms QAM symbol indices to PAM symbol indices.\n\nFor indices in a QAM constellation, computes the corresponding indices\nfor the two PAM constellations corresponding the real and imaginary\ncomponents of the QAM constellation.\nParameters\n\n**num_bits_per_symbol** (*int*)  The number of bits per QAM constellation symbol, e.g., 4 for QAM16.\n\nInput\n\n**ind_qam** (*Tensor, tf.int*)  Indices in the QAM constellation\n\nOutput\n\n- **ind_pam1** (*Tensor, tf.int*)  Indices for the first component of the corresponding PAM modulation\n- **ind_pam2** (*Tensor, tf.int*)  Indices for the first component of the corresponding PAM modulation\n\n\nReferences:\n3GPPTS38211([1](https://nvlabs.github.io/sionna/api/mapping.html#id1),[2](https://nvlabs.github.io/sionna/api/mapping.html#id2),[3](https://nvlabs.github.io/sionna/api/mapping.html#id3))\n\nETSI TS 38.211 5G NR Physical channels and modulation, V16.2.0, Jul. 2020\n[https://www.3gpp.org/ftp/Specs/archive/38_series/38.211/38211-h00.zip](https://www.3gpp.org/ftp/Specs/archive/38_series/38.211/38211-h00.zip)"

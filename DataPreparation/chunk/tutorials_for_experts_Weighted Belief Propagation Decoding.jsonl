"# Weighted Belief Propagation Decoding\n\nThis notebooks implements the *Weighted Belief Propagation* (BP) algorithm as proposed by Nachmani *et al.* in [1]. The main idea is to leverage BP decoding by additional trainable weights that scale each outgoing variable node (VN) and check node (CN) message. These weights provide additional degrees of freedom and can be trained by stochastic gradient descent (SGD) to improve the BP performance for the given code. If all weights are initialized with *1*, the algorithm equals the *classical* BP\nalgorithm and, thus, the concept can be seen as a generalized BP decoder.\n\nOur main focus is to show how Sionna can lower the barrier-to-entry for state-of-the-art research. For this, you will investigate:\n\n- How to implement the multi-loss BP decoding with Sionna\n- How a single scaling factor can lead to similar results\n- What happens for training of the 5G LDPC code\n\n\nThe setup includes the following components:\n\n- LDPC BP Decoder\n- Gaussian LLR source\n\n\nPlease note that we implement a simplified version of the original algorithm consisting of two major simplifications:\n<ol class=\"arabic simple\">\n- ) Only outgoing variable node (VN) messages are weighted. This is possible as the VN operation is linear and it would only increase the memory complexity without increasing the *expressive* power of the neural network.\n- ) We use the same shared weights for all iterations. This can potentially influence the final performance, however, simplifies the implementation and allows to run the decoder with different number of iterations.\n</ol>\n\n**Note**: If you are not familiar with all-zero codeword-based simulations please have a look into the [Bit-Interleaved Coded Modulation](https://nvlabs.github.io/sionna/examples/Bit_Interleaved_Coded_Modulation.html) example notebook first."
"## GPU Configuration and Imports\n\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Import Sionna\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n# Import required Sionna components\nfrom sionna.fec.ldpc import LDPCBPDecoder, LDPC5GEncoder, LDPC5GDecoder\nfrom sionna.utils.metrics import BitwiseMutualInformation\nfrom sionna.fec.utils import GaussianPriorSource, load_parity_check_examples\nfrom sionna.utils import ebnodb2no, hard_decisions\nfrom sionna.utils.metrics import compute_ber\nfrom sionna.utils.plotting import PlotBER\nfrom tensorflow.keras.losses import BinaryCrossentropy\n```\n\n```python\nimport tensorflow as tf\n# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n# For more details, see https://www.tensorflow.org/guide/gpu\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\n```\n\n```python\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n```"
"## Weighted BP for BCH Codes\n\nFirst, we define the trainable model consisting of:\n\n- LDPC BP decoder\n- Gaussian LLR source\n\n\nThe idea of the multi-loss function in [1]is to average the loss overall iterations, i.e., not just the final estimate is evaluated. This requires to call the BP decoder *iteration-wise* by setting `num_iter=1` and `stateful=True` such that the decoder will perform a single iteration and returns its current estimate while also providing the internal messages for the next iteration.\n\nA few comments:\n\n- We assume the transmission of the all-zero codeword. This allows to train and analyze the decoder without the need of an encoder. Remark: The final decoder can be used for arbitrary codewords.\n- We directly generate the channel LLRs with `GaussianPriorSource`. The equivalent LLR distribution could be achieved by transmitting the all-zero codeword over an AWGN channel with BPSK modulation.\n- For the proposed *multi-loss* [1] (i.e., the loss is averaged over all iterations), we need to access the decoders intermediate output after each iteration. This is done by calling the decoding function multiple times while setting `stateful` to True, i.e., the decoder continuous the decoding process at the last message state.\n\n```python\nclass WeightedBP(tf.keras.Model):\n    \"\"\"System model for BER simulations of weighted BP decoding.\n    This model uses `GaussianPriorSource` to mimic the LLRs after demapping of\n    QPSK symbols transmitted over an AWGN channel.\n    Parameters\n    ----------\n        pcm: ndarray\n            The parity-check matrix of the code under investigation.\n        num_iter: int\n            Number of BP decoding iterations.\n\n    Input\n    -----\n        batch_size: int or tf.int\n            The batch_size used for the simulation.\n        ebno_db: float or tf.float\n            A float defining the simulation SNR.\n    Output\n    ------\n        (u, u_hat, loss):\n            Tuple:\n        u: tf.float32\n            A tensor of shape `[batch_size, k] of 0s and 1s containing the transmitted information bits.\n        u_hat: tf.float32\n            A tensor of shape `[batch_size, k] of 0s and 1s containing the estimated information bits.\n        loss: tf.float32\n            Binary cross-entropy loss between `u` and `u_hat`.\n    \"\"\"\n    def __init__(self, pcm, num_iter=5):\n        super().__init__()\n        # init components\n        self.decoder = LDPCBPDecoder(pcm,\n                                     num_iter=1, # iterations are done via outer loop (to access intermediate results for multi-loss)\n                                     stateful=True, # decoder stores internal messages after call\n                                     hard_out=False, # we need to access soft-information\n                                     cn_type=\"boxplus\",\n                                     trainable=True) # the decoder must be trainable, otherwise no weights are generated\n        # used to generate llrs during training (see example notebook on all-zero codeword trick)\n        self.llr_source = GaussianPriorSource()\n        self._num_iter = num_iter\n        self._bce = BinaryCrossentropy(from_logits=True)\n    def call(self, batch_size, ebno_db):\n        noise_var = ebnodb2no(ebno_db,\n                              num_bits_per_symbol=2, # QPSK\n                              coderate=coderate)\n        # all-zero CW to calculate loss / BER\n        c = tf.zeros([batch_size, n])\n        # Gaussian LLR source\n        llr = self.llr_source([[batch_size, n], noise_var])\n        # --- implement multi-loss as proposed by Nachmani et al. [1]---\n        loss = 0\n        msg_vn = None # internal state of decoder\n        for i in range(self._num_iter):\n            c_hat, msg_vn = self.decoder((llr, msg_vn)) # perform one decoding iteration; decoder returns soft-values\n            loss += self._bce(c, c_hat)  # add loss after each iteration\n        loss /= self._num_iter # scale loss by number of iterations\n        return c, c_hat, loss\n```"
"Load a parity-check matrix used for the experiment. We use the same BCH(63,45) code as in [1]. The code can be replaced by any parity-check matrix of your choice.\n\n\n```python\npcm_id = 1 # (63,45) BCH code parity check matrix\npcm, k , n, coderate = load_parity_check_examples(pcm_id=pcm_id, verbose=True)\nnum_iter = 10 # set number of decoding iterations\n# and initialize the model\nmodel = WeightedBP(pcm=pcm, num_iter=num_iter)\n```\n\n\n```python\n\nn: 63, k: 45, coderate: 0.714\n```\n\n\n**Note**: weighted BP tends to work better for small number of iterations. The effective gains (compared to the baseline with same number of iterations) vanish with more iterations."
"### Weights before Training and Simulation of BER\n\nLet us plot the weights after initialization of the decoder to verify that everything is properly initialized. This is equivalent the *classical* BP decoder.\n\n\n```python\n# count number of weights/edges\nprint(\"Total number of weights: \", np.size(model.decoder.get_weights()))\n# and show the weight distribution\nmodel.decoder.show_weights()\n```\n\n\n```python\nTotal number of weights:  432\n```\n\n\nWe first simulate (and store) the BER performance *before* training. For this, we use the `PlotBER` class, which provides a convenient way to store the results for later comparison.\n\n\n```python\n# SNR to simulate the results\nebno_dbs = np.array(np.arange(1, 7, 0.5))\nmc_iters = 100 # number of Monte Carlo iterations\n# we generate a new PlotBER() object to simulate, store and plot the BER results\nber_plot = PlotBER(\"Weighted BP\")\n# simulate and plot the BER curve of the untrained decoder\nber_plot.simulate(model,\n                  ebno_dbs=ebno_dbs,\n                  batch_size=1000,\n                  num_target_bit_errors=2000, # stop sim after 2000 bit errors\n                  legend=\"Untrained\",\n                  soft_estimates=True,\n                  max_mc_iter=mc_iters,\n                  forward_keyboard_interrupt=False);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      1.0 | 8.9492e-02 | 9.7600e-01 |        5638 |       63000 |          976 |        1000 |         0.2 |reached target bit errors\n      1.5 | 7.4079e-02 | 9.0800e-01 |        4667 |       63000 |          908 |        1000 |         0.2 |reached target bit errors\n      2.0 | 5.9444e-02 | 8.1300e-01 |        3745 |       63000 |          813 |        1000 |         0.2 |reached target bit errors\n      2.5 | 4.4667e-02 | 6.6400e-01 |        2814 |       63000 |          664 |        1000 |         0.2 |reached target bit errors\n      3.0 | 3.4365e-02 | 5.1700e-01 |        2165 |       63000 |          517 |        1000 |         0.2 |reached target bit errors\n      3.5 | 2.1563e-02 | 3.4950e-01 |        2717 |      126000 |          699 |        2000 |         0.3 |reached target bit errors\n      4.0 | 1.3460e-02 | 2.3200e-01 |        2544 |      189000 |          696 |        3000 |         0.5 |reached target bit errors\n      4.5 | 7.1778e-03 | 1.2880e-01 |        2261 |      315000 |          644 |        5000 |         0.8 |reached target bit errors\n      5.0 | 3.9877e-03 | 7.5889e-02 |        2261 |      567000 |          683 |        9000 |         1.4 |reached target bit errors\n      5.5 | 2.1240e-03 | 3.9188e-02 |        2141 |     1008000 |          627 |       16000 |         2.5 |reached target bit errors\n      6.0 | 1.0169e-03 | 2.0406e-02 |        2050 |     2016000 |          653 |       32000 |         4.9 |reached target bit errors\n      6.5 | 4.4312e-04 | 8.5417e-03 |        2010 |     4536000 |          615 |       72000 |        11.1 |reached target bit errors\n```"
"### Training\n\nWe now train the model for a fixed number of SGD training iterations.\n\n**Note**: this is a very basic implementation of the training loop. You can also try more sophisticated training loops with early stopping, different hyper-parameters or optimizers etc.\n\n\n```python\n# training parameters\nbatch_size = 1000\ntrain_iter = 200\nebno_db = 4.0\nclip_value_grad = 10 # gradient clipping for stable training convergence\n# bmi is used as metric to evaluate the intermediate results\nbmi = BitwiseMutualInformation()\n# try also different optimizers or different hyperparameters\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\nfor it in range(0, train_iter):\n    with tf.GradientTape() as tape:\n        b, llr, loss = model(batch_size, ebno_db)\n    grads = tape.gradient(loss, model.trainable_variables)\n    grads = tf.clip_by_value(grads, -clip_value_grad, clip_value_grad, name=None)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    # calculate and print intermediate metrics\n    # only for information\n    # this has no impact on the training\n    if it%10==0: # evaluate every 10 iterations\n        # calculate ber from received LLRs\n        b_hat = hard_decisions(llr) # hard decided LLRs first\n        ber = compute_ber(b, b_hat)\n        # and print results\n        mi = bmi(b, llr).numpy() # calculate bit-wise mutual information\n        l = loss.numpy() # copy loss to numpy for printing\n        print(f\"Current loss: {l:3f} ber: {ber:.4f} bmi: {mi:.3f}\".format())\n        bmi.reset_states() # reset the BMI metric\n```\n\n\n```python\nCurrent loss: 0.048708 ber: 0.0120 bmi: 0.934\nCurrent loss: 0.058506 ber: 0.0139 bmi: 0.923\nCurrent loss: 0.052293 ber: 0.0125 bmi: 0.934\nCurrent loss: 0.054314 ber: 0.0134 bmi: 0.928\nCurrent loss: 0.051650 ber: 0.0125 bmi: 0.924\nCurrent loss: 0.047477 ber: 0.0133 bmi: 0.931\nCurrent loss: 0.045135 ber: 0.0122 bmi: 0.935\nCurrent loss: 0.050638 ber: 0.0125 bmi: 0.938\nCurrent loss: 0.045256 ber: 0.0119 bmi: 0.949\nCurrent loss: 0.041335 ber: 0.0124 bmi: 0.952\nCurrent loss: 0.040905 ber: 0.0107 bmi: 0.937\nCurrent loss: 0.043627 ber: 0.0125 bmi: 0.949\nCurrent loss: 0.044397 ber: 0.0126 bmi: 0.942\nCurrent loss: 0.043392 ber: 0.0126 bmi: 0.938\nCurrent loss: 0.043059 ber: 0.0133 bmi: 0.947\nCurrent loss: 0.047521 ber: 0.0130 bmi: 0.937\nCurrent loss: 0.040529 ber: 0.0116 bmi: 0.944\nCurrent loss: 0.041838 ber: 0.0128 bmi: 0.942\nCurrent loss: 0.041801 ber: 0.0130 bmi: 0.940\nCurrent loss: 0.042754 ber: 0.0142 bmi: 0.946\n```"
"### Results\n\nAfter training, the weights of the decoder have changed. In average, the weights are smaller after training.\n\n\n```python\nmodel.decoder.show_weights() # show weights AFTER training\n```\n\n\nAnd let us compare the new BER performance. For this, we can simply call the ber_plot.simulate() function again as it internally stores all previous results (if `add_results` is True).\n\n\n```python\nebno_dbs = np.array(np.arange(1, 7, 0.5))\nbatch_size = 10000\nmc_ites = 100\nber_plot.simulate(model,\n                  ebno_dbs=ebno_dbs,\n                  batch_size=1000,\n                  num_target_bit_errors=2000, # stop sim after 2000 bit errors\n                  legend=\"Trained\",\n                  max_mc_iter=mc_iters,\n                  soft_estimates=True);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      1.0 | 9.0730e-02 | 9.9600e-01 |        5716 |       63000 |          996 |        1000 |         0.2 |reached target bit errors\n      1.5 | 7.8889e-02 | 9.8400e-01 |        4970 |       63000 |          984 |        1000 |         0.1 |reached target bit errors\n      2.0 | 6.6365e-02 | 9.2500e-01 |        4181 |       63000 |          925 |        1000 |         0.1 |reached target bit errors\n      2.5 | 4.9825e-02 | 8.2000e-01 |        3139 |       63000 |          820 |        1000 |         0.1 |reached target bit errors\n      3.0 | 3.6603e-02 | 6.4400e-01 |        2306 |       63000 |          644 |        1000 |         0.1 |reached target bit errors\n      3.5 | 2.2302e-02 | 4.2000e-01 |        2810 |      126000 |          840 |        2000 |         0.3 |reached target bit errors\n      4.0 | 1.2577e-02 | 2.4400e-01 |        2377 |      189000 |          732 |        3000 |         0.5 |reached target bit errors\n      4.5 | 6.5778e-03 | 1.3460e-01 |        2072 |      315000 |          673 |        5000 |         0.7 |reached target bit errors\n      5.0 | 2.9769e-03 | 6.2818e-02 |        2063 |      693000 |          691 |       11000 |         1.7 |reached target bit errors\n      5.5 | 1.3287e-03 | 2.9667e-02 |        2009 |     1512000 |          712 |       24000 |         3.6 |reached target bit errors\n      6.0 | 5.2511e-04 | 1.2967e-02 |        2018 |     3843000 |          791 |       61000 |         9.2 |reached target bit errors\n      6.5 | 2.0333e-04 | 5.6000e-03 |        1281 |     6300000 |          560 |      100000 |        15.0 |reached max iter\n```"
"## Further Experiments\n\nYou will now see that the memory footprint can be drastically reduced by using the same weight for all messages. In the second part we will apply the concept to the 5G LDPC codes."
"### Damped BP\n\nIt is well-known that scaling of LLRs / messages can help to improve the performance of BP decoding in some scenarios [3,4]. In particular, this works well for very short codes such as the code we are currently analyzing.\n\nWe now follow the basic idea of [2] and scale all weights with the same scalar.\n\n\n```python\n# get weights of trained model\nweights_bp = model.decoder.get_weights()\n# calc mean value of weights\ndamping_factor = tf.reduce_mean(weights_bp)\n# set all weights to the SAME constant scaling\nweights_damped = tf.ones_like(weights_bp) * damping_factor\n# and apply the new weights\nmodel.decoder.set_weights(weights_damped)\n# let us have look at the new weights again\nmodel.decoder.show_weights()\n# and simulate the BER again\nleg_str = f\"Damped BP (scaling factor {damping_factor.numpy():.3f})\"\nber_plot.simulate(model,\n                  ebno_dbs=ebno_dbs,\n                  batch_size=1000,\n                  num_target_bit_errors=2000, # stop sim after 2000 bit errors\n                  legend=leg_str,\n                  max_mc_iter=mc_iters,\n                  soft_estimates=True);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      1.0 | 9.0333e-02 | 9.9500e-01 |        5691 |       63000 |          995 |        1000 |         0.2 |reached target bit errors\n      1.5 | 7.6413e-02 | 9.7800e-01 |        4814 |       63000 |          978 |        1000 |         0.2 |reached target bit errors\n      2.0 | 6.1556e-02 | 8.9800e-01 |        3878 |       63000 |          898 |        1000 |         0.1 |reached target bit errors\n      2.5 | 4.8746e-02 | 7.9700e-01 |        3071 |       63000 |          797 |        1000 |         0.2 |reached target bit errors\n      3.0 | 3.5746e-02 | 6.0800e-01 |        2252 |       63000 |          608 |        1000 |         0.1 |reached target bit errors\n      3.5 | 2.0857e-02 | 3.7950e-01 |        2628 |      126000 |          759 |        2000 |         0.3 |reached target bit errors\n      4.0 | 1.2222e-02 | 2.3433e-01 |        2310 |      189000 |          703 |        3000 |         0.5 |reached target bit errors\n      4.5 | 6.4524e-03 | 1.2967e-01 |        2439 |      378000 |          778 |        6000 |         0.9 |reached target bit errors\n      5.0 | 2.7712e-03 | 5.8667e-02 |        2095 |      756000 |          704 |       12000 |         1.8 |reached target bit errors\n      5.5 | 1.2844e-03 | 2.8960e-02 |        2023 |     1575000 |          724 |       25000 |         3.7 |reached target bit errors\n      6.0 | 5.0743e-04 | 1.2032e-02 |        2014 |     3969000 |          758 |       63000 |         9.5 |reached target bit errors\n      6.5 | 2.1730e-04 | 5.5200e-03 |        1369 |     6300000 |          552 |      100000 |        15.2 |reached max iter\n```"
"When looking at the results, we observe almost the same performance although we only scale by a single scalar. This implies that the number of weights of our model is by far too large and the memory footprint could be reduced significantly. However, isnt it fascinating to see that this simple concept of weighted BP leads to the same results as the concept of *damped BP*?\n\n**Note**: for more iterations it could be beneficial to implement an individual damping per iteration."
"### Learning the 5G LDPC Code\n\nIn this Section, you will experience what happens if we apply the same concept to the 5G LDPC code (including rate matching).\n\nFor this, we need to define a new model.\n\n\n```python\nclass WeightedBP5G(tf.keras.Model):\n    \"\"\"System model for BER simulations of weighted BP decoding for 5G instruction_answer codes.\n    This model uses `GaussianPriorSource` to mimic the LLRs after demapping of\n    QPSK symbols transmitted over an AWGN channel.\n    Parameters\n    ----------\n        k: int\n            Number of information bits per codeword.\n        n: int\n            Codeword length.\n        num_iter: int\n            Number of BP decoding iterations.\n    Input\n    -----\n        batch_size: int or tf.int\n            The batch_size used for the simulation.\n        ebno_db: float or tf.float\n            A float defining the simulation SNR.\n    Output\n    ------\n        (u, u_hat, loss):\n            Tuple:\n        u: tf.float32\n            A tensor of shape `[batch_size, k] of 0s and 1s containing the transmitted information bits.\n        u_hat: tf.float32\n            A tensor of shape `[batch_size, k] of 0s and 1s containing the estimated information bits.\n        loss: tf.float32\n            Binary cross-entropy loss between `u` and `u_hat`.\n    \"\"\"\n    def __init__(self, k, n, num_iter=20):\n        super().__init__()\n        # we need to initialize an encoder for the 5G parameters\n        self.encoder = LDPC5GEncoder(k, n)\n        self.decoder = LDPC5GDecoder(self.encoder,\n                                     num_iter=1, # iterations are done via outer loop (to access intermediate results for multi-loss)\n                                     stateful=True,\n                                     hard_out=False,\n                                     cn_type=\"boxplus\",\n                                     trainable=True)\n        self.llr_source = GaussianPriorSource()\n        self._num_iter = num_iter\n        self._coderate = k/n\n        self._bce = BinaryCrossentropy(from_logits=True)\n    def call(self, batch_size, ebno_db):\n        noise_var = ebnodb2no(ebno_db,\n                              num_bits_per_symbol=2, # QPSK\n                              coderate=self._coderate)\n        # BPSK modulated all-zero CW\n        c = tf.zeros([batch_size, k]) # decoder only returns info bits\n        # use fake llrs from GA\n        # works as BP is symmetric\n        llr = self.llr_source([[batch_size, n], noise_var])\n        # --- implement multi-loss is proposed by Nachmani et al. ---\n        loss = 0\n        msg_vn = None\n        for i in range(self._num_iter):\n            c_hat, msg_vn = self.decoder((llr, msg_vn)) # perform one decoding iteration; decoder returns soft-values\n            loss += self._bce(c, c_hat)  # add loss after each iteration\n        return c, c_hat, loss\n```"
"```python\n# generate model\nnum_iter = 10\nk = 400\nn = 800\nmodel5G = WeightedBP5G(k, n, num_iter=num_iter)\n# generate baseline BER\nebno_dbs = np.array(np.arange(0, 4, 0.25))\nmc_iters = 100 # number of monte carlo iterations\nber_plot_5G = PlotBER(\"Weighted BP for 5G instruction_answer\")\n# simulate the untrained performance\nber_plot_5G.simulate(model5G,\n                     ebno_dbs=ebno_dbs,\n                     batch_size=1000,\n                     num_target_bit_errors=2000, # stop sim after 2000 bit errors\n                     legend=\"Untrained\",\n                     soft_estimates=True,\n                     max_mc_iter=mc_iters);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      0.0 | 1.6660e-01 | 1.0000e+00 |       66640 |      400000 |         1000 |        1000 |         0.2 |reached target bit errors\n     0.25 | 1.4864e-01 | 1.0000e+00 |       59455 |      400000 |         1000 |        1000 |         0.2 |reached target bit errors\n      0.5 | 1.2470e-01 | 9.9700e-01 |       49880 |      400000 |          997 |        1000 |         0.2 |reached target bit errors\n     0.75 | 9.4408e-02 | 9.8000e-01 |       37763 |      400000 |          980 |        1000 |         0.2 |reached target bit errors\n      1.0 | 6.6635e-02 | 9.3900e-01 |       26654 |      400000 |          939 |        1000 |         0.2 |reached target bit errors\n     1.25 | 4.1078e-02 | 8.1100e-01 |       16431 |      400000 |          811 |        1000 |         0.2 |reached target bit errors\n      1.5 | 2.1237e-02 | 6.1200e-01 |        8495 |      400000 |          612 |        1000 |         0.2 |reached target bit errors\n     1.75 | 9.2050e-03 | 3.7600e-01 |        3682 |      400000 |          376 |        1000 |         0.2 |reached target bit errors\n      2.0 | 2.7175e-03 | 1.7050e-01 |        2174 |      800000 |          341 |        2000 |         0.5 |reached target bit errors\n     2.25 | 8.8167e-04 | 6.3833e-02 |        2116 |     2400000 |          383 |        6000 |         1.5 |reached target bit errors\n      2.5 | 2.1781e-04 | 2.1875e-02 |        2091 |     9600000 |          525 |       24000 |         5.9 |reached target bit errors\n     2.75 | 4.2950e-05 | 4.9600e-03 |        1718 |    40000000 |          496 |      100000 |        24.5 |reached max iter\n      3.0 | 6.8000e-06 | 9.1000e-04 |         272 |    40000000 |           91 |      100000 |        24.5 |reached max iter\n     3.25 | 9.2500e-07 | 1.8000e-04 |          37 |    40000000 |           18 |      100000 |        24.5 |reached max iter\n      3.5 | 2.5000e-08 | 1.0000e-05 |           1 |    40000000 |            1 |      100000 |        24.5 |reached max iter\n     3.75 | 0.0000e+00 | 0.0000e+00 |           0 |    40000000 |            0 |      100000 |        24.1 |reached max iter\nSimulation stopped as no error occurred @ EbNo = 3.8 dB.\n\n```"
"And lets train this new model.\n\n\n```python\n# training parameters\nbatch_size = 1000\ntrain_iter = 200\nclip_value_grad = 10 # gradient clipping seems to be important\n# smaller training SNR as the new code is longer (=stronger) than before\nebno_db = 1.5 # rule of thumb: train at ber = 1e-2\n# only used as metric\nbmi = BitwiseMutualInformation()\n# try also different optimizers or different hyperparameters\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n# and let's go\nfor it in range(0, train_iter):\n    with tf.GradientTape() as tape:\n        b, llr, loss = model5G(batch_size, ebno_db)\n    grads = tape.gradient(loss, model5G.trainable_variables)\n    grads = tf.clip_by_value(grads, -clip_value_grad, clip_value_grad, name=None)\n    optimizer.apply_gradients(zip(grads, model5G.trainable_weights))\n    # calculate and print intermediate metrics\n    if it%10==0:\n        # calculate ber\n        b_hat = hard_decisions(llr)\n        ber = compute_ber(b, b_hat)\n        # and print results\n        mi = bmi(b, llr).numpy()\n        l = loss.numpy()\n        print(f\"Current loss: {l:3f} ber: {ber:.4f} bmi: {mi:.3f}\".format())\n        bmi.reset_states()\n```\n\n\n```python\nCurrent loss: 1.708751 ber: 0.0204 bmi: 0.925\nCurrent loss: 1.745474 ber: 0.0219 bmi: 0.918\nCurrent loss: 1.741312 ber: 0.0224 bmi: 0.917\nCurrent loss: 1.707712 ber: 0.0208 bmi: 0.923\nCurrent loss: 1.705274 ber: 0.0209 bmi: 0.923\nCurrent loss: 1.706761 ber: 0.0211 bmi: 0.922\nCurrent loss: 1.711995 ber: 0.0212 bmi: 0.921\nCurrent loss: 1.729707 ber: 0.0223 bmi: 0.917\nCurrent loss: 1.692947 ber: 0.0205 bmi: 0.924\nCurrent loss: 1.703924 ber: 0.0203 bmi: 0.924\nCurrent loss: 1.743640 ber: 0.0220 bmi: 0.919\nCurrent loss: 1.719159 ber: 0.0220 bmi: 0.919\nCurrent loss: 1.728399 ber: 0.0221 bmi: 0.920\nCurrent loss: 1.717423 ber: 0.0211 bmi: 0.922\nCurrent loss: 1.743661 ber: 0.0225 bmi: 0.918\nCurrent loss: 1.704675 ber: 0.0212 bmi: 0.923\nCurrent loss: 1.690425 ber: 0.0206 bmi: 0.924\nCurrent loss: 1.728023 ber: 0.0212 bmi: 0.922\nCurrent loss: 1.724549 ber: 0.0212 bmi: 0.922\nCurrent loss: 1.739966 ber: 0.0224 bmi: 0.917\n```"
"We now simulate the new results and compare it to the untrained results.\n\n\n```python\nebno_dbs = np.array(np.arange(0, 4, 0.25))\nbatch_size = 1000\nmc_iters = 100\nber_plot_5G.simulate(model5G,\n                     ebno_dbs=ebno_dbs,\n                     batch_size=batch_size,\n                     num_target_bit_errors=2000, # stop sim after 2000 bit errors\n                     legend=\"Trained\",\n                     max_mc_iter=mc_iters,\n                     soft_estimates=True);\n```\n\n\n```python\nEbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n---------------------------------------------------------------------------------------------------------------------------------------\n      0.0 | 1.6568e-01 | 1.0000e+00 |       66273 |      400000 |         1000 |        1000 |         0.2 |reached target bit errors\n     0.25 | 1.4965e-01 | 9.9900e-01 |       59858 |      400000 |          999 |        1000 |         0.2 |reached target bit errors\n      0.5 | 1.2336e-01 | 9.9900e-01 |       49342 |      400000 |          999 |        1000 |         0.2 |reached target bit errors\n     0.75 | 9.6135e-02 | 9.9100e-01 |       38454 |      400000 |          991 |        1000 |         0.3 |reached target bit errors\n      1.0 | 6.8543e-02 | 9.4500e-01 |       27417 |      400000 |          945 |        1000 |         0.2 |reached target bit errors\n     1.25 | 3.9152e-02 | 8.3300e-01 |       15661 |      400000 |          833 |        1000 |         0.2 |reached target bit errors\n      1.5 | 2.2040e-02 | 6.2400e-01 |        8816 |      400000 |          624 |        1000 |         0.2 |reached target bit errors\n     1.75 | 9.1300e-03 | 3.8400e-01 |        3652 |      400000 |          384 |        1000 |         0.2 |reached target bit errors\n      2.0 | 2.8075e-03 | 1.6600e-01 |        2246 |      800000 |          332 |        2000 |         0.5 |reached target bit errors\n     2.25 | 8.5500e-04 | 6.2000e-02 |        2052 |     2400000 |          372 |        6000 |         1.4 |reached target bit errors\n      2.5 | 1.9837e-04 | 2.1115e-02 |        2063 |    10400000 |          549 |       26000 |         6.3 |reached target bit errors\n     2.75 | 2.9600e-05 | 4.1000e-03 |        1184 |    40000000 |          410 |      100000 |        24.3 |reached max iter\n      3.0 | 6.5750e-06 | 9.1000e-04 |         263 |    40000000 |           91 |      100000 |        24.3 |reached max iter\n     3.25 | 5.5000e-07 | 1.4000e-04 |          22 |    40000000 |           14 |      100000 |        24.3 |reached max iter\n      3.5 | 7.5000e-08 | 3.0000e-05 |           3 |    40000000 |            3 |      100000 |        24.5 |reached max iter\n     3.75 | 2.5000e-08 | 1.0000e-05 |           1 |    40000000 |            1 |      100000 |        24.3 |reached max iter\n```"
"Unfortunately, we observe only very minor gains for the 5G LDPC code. We empirically observed that gain vanishes for more iterations and longer codewords, i.e., for most practical use-cases of the 5G LDPC code the gains are only minor.\n\nHowever, there may be other `codes` `on` `graphs` that benefit from the principle idea of weighted BP - or other channel setups? Feel free to adjust this notebook and train for your favorite code / channel.\n\nOther ideas for own experiments:\n\n- Implement weighted BP with unique weights per iteration.\n- Apply the concept to (scaled) min-sum decoding as in [5].\n- Can you replace the complete CN update by a neural network?\n- Verify the results from all-zero simulations for a *real* system simulation with explicit encoder and random data\n- What happens in combination with higher order modulation?"
"## References\n\n[1]E. Nachmani, Y. Beery and D. Burshtein, Learning to Decode Linear Codes Using Deep Learning, IEEE Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp.341-346., 2016. [https://arxiv.org/pdf/1607.04793.pdf](https://arxiv.org/pdf/1607.04793.pdf)\n\n[2] M. Lian, C. Hger, and H. Pfister, What can machine learning teach us about communications? IEEE Information Theory Workshop (ITW), pp.1-5. 2018.\n\n[3] ] M. Pretti, A message passing algorithm with damping, J. Statist. Mech.: Theory Practice, p.11008, Nov.2005.\n\n[4] J.S. Yedidia, W.T. Freeman and Y. Weiss, Constructing free energy approximations and Generalized Belief Propagation algorithms, IEEE Transactions on Information Theory, 2005.\n\n[5] E. Nachmani, E. Marciano, L. Lugosch, W. Gross, D. Burshtein and Y. Beery, Deep learning methods for improved decoding of linear codes, IEEE Journal of Selected Topics in Signal Processing, vol.12, no. 1, pp.119-131, 2018.\n[5] E. Nachmani, E. Marciano, L. Lugosch, W. Gross, D. Burshtein and Y. Beery, Deep learning methods for improved decoding of linear codes, IEEE Journal of Selected Topics in Signal Processing, vol.12, no. 1, pp.119-131, 2018.\n[5] E. Nachmani, E. Marciano, L. Lugosch, W. Gross, D. Burshtein and Y. Beery, Deep learning methods for improved decoding of linear codes, IEEE Journal of Selected Topics in Signal Processing, vol.12, no. 1, pp.119-131, 2018."
